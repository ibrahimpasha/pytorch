{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a8e85c",
   "metadata": {},
   "source": [
    "# PyTorch Learning Notebook\n",
    "\n",
    "Welcome to this comprehensive guide to PyTorch fundamentals! This notebook will take you from basic tensor operations through complete training loops, with each concept building upon previous ones.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- How to create and manipulate tensors\n",
    "- PyTorch's automatic differentiation system (autograd)\n",
    "- Building neural network components with nn.Module\n",
    "- Activation functions and their purposes\n",
    "- Complete training workflows with optimizers and loss functions\n",
    "\n",
    "Let's start by verifying your PyTorch installation and checking available compute devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d50b5859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.11.0.dev20260115+cu128\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db66e0f0",
   "metadata": {},
   "source": [
    "Great! Your environment is set up. Now let's dive into PyTorch fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff71b7d4",
   "metadata": {},
   "source": [
    "## torch.tensor - The Foundation of PyTorch\n",
    "\n",
    "**What is a tensor?**\n",
    "\n",
    "A tensor is PyTorch's fundamental data structure - a multi-dimensional array that can hold numbers. Think of it as a generalization of vectors and matrices:\n",
    "- A 0D tensor is a scalar (single number)\n",
    "- A 1D tensor is a vector (list of numbers)\n",
    "- A 2D tensor is a matrix (table of numbers)\n",
    "- A 3D+ tensor extends into higher dimensions\n",
    "\n",
    "**Why do tensors matter?**\n",
    "\n",
    "Tensors are the building blocks of deep learning. All data (images, text, audio) gets converted into tensors, and all neural network operations work on tensors. PyTorch tensors are similar to NumPy arrays but with two key advantages:\n",
    "1. They can run on GPUs for massive speedups\n",
    "2. They support automatic differentiation for training neural networks\n",
    "\n",
    "Let's explore different ways to create tensors and inspect their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e8c970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D tensor from list:\n",
      "tensor([1., 2., 3., 4.])\n",
      "Type: <class 'torch.Tensor'>\n",
      "\n",
      "2D tensor from nested list:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Creating tensors from Python lists\n",
    "# 1D tensor (vector)\n",
    "tensor_1d = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "print(\"1D tensor from list:\")\n",
    "print(tensor_1d)\n",
    "print(f\"Type: {type(tensor_1d)}\")\n",
    "print()\n",
    "\n",
    "# 2D tensor (matrix)\n",
    "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"2D tensor from nested list:\")\n",
    "print(tensor_2d)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a44eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from NumPy array:\n",
      "tensor([10, 20, 30, 40, 50])\n",
      "Original NumPy array: [10 20 30 40 50]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating tensors from NumPy arrays\n",
    "numpy_array = np.array([10, 20, 30, 40, 50])\n",
    "tensor_from_numpy = torch.tensor(numpy_array)\n",
    "print(\"Tensor from NumPy array:\")\n",
    "print(tensor_from_numpy)\n",
    "print(f\"Original NumPy array: {numpy_array}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fe9c12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer tensor: tensor([1, 2, 3], dtype=torch.int32)\n",
      "Data type: torch.int32\n",
      "\n",
      "Float tensor: tensor([1., 2., 3.])\n",
      "Data type: torch.float32\n",
      "\n",
      "Double tensor: tensor([1., 2., 3.], dtype=torch.float64)\n",
      "Data type: torch.float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating tensors with different data types (dtypes)\n",
    "# Integer tensor\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "print(f\"Integer tensor: {int_tensor}\")\n",
    "print(f\"Data type: {int_tensor.dtype}\")\n",
    "print()\n",
    "\n",
    "# Float tensor (default for decimals)\n",
    "float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "print(f\"Float tensor: {float_tensor}\")\n",
    "print(f\"Data type: {float_tensor.dtype}\")\n",
    "print()\n",
    "\n",
    "# Double precision tensor\n",
    "double_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)\n",
    "print(f\"Double tensor: {double_tensor}\")\n",
    "print(f\"Data type: {double_tensor.dtype}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de5eb336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor properties:\n",
      "Tensor: \n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], device='cuda:0')\n",
      "\n",
      "Shape: torch.Size([2, 3])\n",
      "Size: torch.Size([2, 3])\n",
      "Data type: torch.float32\n",
      "Device: cuda:0\n",
      "Number of dimensions: 2\n",
      "Total number of elements: 6\n"
     ]
    }
   ],
   "source": [
    "# Inspecting tensor properties\n",
    "sample_tensor = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "sample_tensor = sample_tensor.cuda()\n",
    "\n",
    "print(\"Tensor properties:\")\n",
    "print(f\"Tensor: \\n{sample_tensor}\")\n",
    "print()\n",
    "print(f\"Shape: {sample_tensor.shape}\")  # Dimensions of the tensor\n",
    "print(f\"Size: {sample_tensor.size()}\")  # Same as shape\n",
    "print(f\"Data type: {sample_tensor.dtype}\")  # Type of elements\n",
    "print(f\"Device: {sample_tensor.device}\")  # CPU or GPU\n",
    "print(f\"Number of dimensions: {sample_tensor.ndim}\")  # Rank of tensor\n",
    "print(f\"Total number of elements: {sample_tensor.numel()}\")  # Total elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117e9a76",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- Tensors can be created from Python lists, NumPy arrays, or other data sources\n",
    "- The `dtype` parameter controls the data type (int32, float32, float64, etc.)\n",
    "- Common properties include `.shape` (dimensions), `.dtype` (data type), and `.device` (CPU/GPU)\n",
    "- PyTorch defaults to float32 for decimal numbers, which is efficient for deep learning\n",
    "\n",
    "Now that we understand tensors, we can explore their shapes and operations in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d3058",
   "metadata": {},
   "source": [
    "## Shape - Understanding Tensor Dimensions\n",
    "\n",
    "**What is tensor shape?**\n",
    "\n",
    "The shape of a tensor describes its dimensions - how many elements it has along each axis. Understanding shape is crucial because:\n",
    "- It tells you the structure of your data\n",
    "- Operations require compatible shapes\n",
    "- Neural network layers expect specific input shapes\n",
    "\n",
    "**Why does shape matter?**\n",
    "\n",
    "In deep learning, you'll constantly work with different shaped tensors:\n",
    "- **1D tensors** (vectors): Single sequences like word embeddings [embedding_dim]\n",
    "- **2D tensors** (matrices): Batches of vectors like [batch_size, features]\n",
    "- **3D tensors**: Sequences of vectors like [batch_size, sequence_length, features]\n",
    "- **4D tensors**: Image batches like [batch_size, channels, height, width]\n",
    "\n",
    "Getting shapes right is essential - shape mismatches are one of the most common errors in deep learning!\n",
    "\n",
    "Let's explore tensor shapes and how to manipulate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4706b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D Tensor (vector):\n",
      "tensor([1, 2, 3, 4, 5])\n",
      "Shape: torch.Size([5])\n",
      "Number of dimensions: 1\n",
      "Size along dimension 0: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1D tensor (vector) - shape is (n,)\n",
    "tensor_1d = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(\"1D Tensor (vector):\")\n",
    "print(tensor_1d)\n",
    "print(f\"Shape: {tensor_1d.shape}\")\n",
    "print(f\"Number of dimensions: {tensor_1d.ndim}\")\n",
    "print(f\"Size along dimension 0: {tensor_1d.size(0)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "719fc93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Tensor (matrix):\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "Shape: torch.Size([4, 3])\n",
      "Number of dimensions: 2\n",
      "Rows (dimension 0): 4\n",
      "Columns (dimension 1): 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2D tensor (matrix) - shape is (rows, columns)\n",
    "tensor_2d = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "print(\"2D Tensor (matrix):\")\n",
    "print(tensor_2d)\n",
    "print(f\"Shape: {tensor_2d.shape}\")\n",
    "print(f\"Number of dimensions: {tensor_2d.ndim}\")\n",
    "print(f\"Rows (dimension 0): {tensor_2d.size(0)}\")\n",
    "print(f\"Columns (dimension 1): {tensor_2d.size(1)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5343dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D Tensor:\n",
      "tensor([[[ 1,  2],\n",
      "         [ 3,  4]],\n",
      "\n",
      "        [[ 5,  6],\n",
      "         [ 7,  8]],\n",
      "\n",
      "        [[ 9, 10],\n",
      "         [11, 12]]])\n",
      "Shape: torch.Size([3, 2, 2])\n",
      "Number of dimensions: 3\n",
      "Dimension 0 (depth): 3\n",
      "Dimension 1 (rows): 2\n",
      "Dimension 2 (columns): 2\n",
      "Total elements: 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3D tensor - shape is (depth, rows, columns)\n",
    "# Common for: batch of sequences, video frames, etc.\n",
    "tensor_3d = torch.tensor([\n",
    "    [[1, 2], [3, 4]],\n",
    "    [[5, 6], [7, 8]],\n",
    "    [[9, 10], [11, 12]]\n",
    "])\n",
    "print(\"3D Tensor:\")\n",
    "print(tensor_3d)\n",
    "print(f\"Shape: {tensor_3d.shape}\")\n",
    "print(f\"Number of dimensions: {tensor_3d.ndim}\")\n",
    "print(f\"Dimension 0 (depth): {tensor_3d.size(0)}\")\n",
    "print(f\"Dimension 1 (rows): {tensor_3d.size(1)}\")\n",
    "print(f\"Dimension 2 (columns): {tensor_3d.size(2)}\")\n",
    "print(f\"Total elements: {tensor_3d.numel()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aad8e8",
   "metadata": {},
   "source": [
    "**Understanding Shape Notation:**\n",
    "\n",
    "- `torch.Size([5])` - 1D tensor with 5 elements\n",
    "- `torch.Size([4, 3])` - 2D tensor with 4 rows and 3 columns (12 total elements)\n",
    "- `torch.Size([3, 2, 2])` - 3D tensor with 3 \"layers\", each containing a 2×2 matrix (12 total elements)\n",
    "\n",
    "The total number of elements is the product of all dimensions: 3 × 2 × 2 = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6c7ad",
   "metadata": {},
   "source": [
    "### Reshaping Tensors\n",
    "\n",
    "Often you need to change a tensor's shape without changing its data. PyTorch provides two main methods:\n",
    "\n",
    "1. **`.reshape()`** - Returns a tensor with the new shape (may copy data)\n",
    "2. **`.view()`** - Returns a tensor with the new shape (shares memory with original)\n",
    "\n",
    "Both require the total number of elements to remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b00cb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "Original shape: torch.Size([12])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start with a 1D tensor\n",
    "original = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
    "print(f\"Original tensor: {original}\")\n",
    "print(f\"Original shape: {original.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c837e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped to 3×4 matrix:\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "Shape: torch.Size([3, 4])\n",
      "\n",
      "Reshaped to 4×3 matrix:\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "Shape: torch.Size([4, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reshape to 2D (matrix)\n",
    "reshaped_2d = original.reshape(3, 4)  # 3 rows, 4 columns\n",
    "print(\"Reshaped to 3×4 matrix:\")\n",
    "print(reshaped_2d)\n",
    "print(f\"Shape: {reshaped_2d.shape}\")\n",
    "print()\n",
    "\n",
    "# Reshape to different 2D shape\n",
    "reshaped_2d_alt = original.reshape(4, 3)  # 4 rows, 3 columns\n",
    "print(\"Reshaped to 4×3 matrix:\")\n",
    "print(reshaped_2d_alt)\n",
    "print(f\"Shape: {reshaped_2d_alt.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0deaaa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 3, 4]' is invalid for input of size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43moriginal\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[2, 3, 4]' is invalid for input of size 12"
     ]
    }
   ],
   "source": [
    "original.reshape(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0999abe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped to 2×3×2 tensor:\n",
      "tensor([[[ 1,  2],\n",
      "         [ 3,  4],\n",
      "         [ 5,  6]],\n",
      "\n",
      "        [[ 7,  8],\n",
      "         [ 9, 10],\n",
      "         [11, 12]]])\n",
      "Shape: torch.Size([2, 3, 2])\n",
      "\n",
      "Reshaped with -1 (auto-infer dimension):\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "Shape: torch.Size([3, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reshape to 3D\n",
    "reshaped_3d = original.reshape(2, 3, 2)  # 2 layers of 3×2 matrices\n",
    "print(\"Reshaped to 2×3×2 tensor:\")\n",
    "print(reshaped_3d)\n",
    "print(f\"Shape: {reshaped_3d.shape}\")\n",
    "print()\n",
    "\n",
    "# Using -1 to infer one dimension\n",
    "# PyTorch automatically calculates the missing dimension\n",
    "auto_reshaped = original.reshape(3, -1)  # 3 rows, automatically 4 columns\n",
    "print(\"Reshaped with -1 (auto-infer dimension):\")\n",
    "print(auto_reshaped)\n",
    "print(f\"Shape: {auto_reshaped.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efccf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .view() - similar to reshape but requires contiguous memory\n",
    "viewed = original.view(2, 6)  # 2 rows, 6 columns\n",
    "print(\"Using .view() to reshape:\")\n",
    "print(viewed)\n",
    "print(f\"Shape: {viewed.shape}\")\n",
    "print()\n",
    "\n",
    "# Important: view() shares memory with the original tensor\n",
    "# Modifying the view affects the original\n",
    "viewed[0, 0] = 999\n",
    "print(f\"After modifying view, original tensor: {original}\")\n",
    "print(\"Notice the first element changed to 999!\")\n",
    "print()\n",
    "\n",
    "# Reset for clarity\n",
    "original[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common reshaping patterns in deep learning\n",
    "\n",
    "# Flattening: Convert any shape to 1D\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "flattened = matrix.reshape(-1)  # or matrix.flatten()\n",
    "print(f\"Original matrix shape: {matrix.shape}\")\n",
    "print(f\"Flattened shape: {flattened.shape}\")\n",
    "print(f\"Flattened: {flattened}\")\n",
    "print()\n",
    "\n",
    "# Adding a batch dimension\n",
    "single_sample = torch.tensor([1, 2, 3, 4])\n",
    "batched = single_sample.reshape(1, -1)  # Add batch dimension of size 1\n",
    "print(f\"Single sample shape: {single_sample.shape}\")\n",
    "print(f\"With batch dimension: {batched.shape}\")\n",
    "print(f\"Batched: {batched}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253a46f",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- **Shape** describes tensor dimensions and is accessed via `.shape` or `.size()`\n",
    "- The total number of elements must stay the same when reshaping\n",
    "- **`.reshape()`** is more flexible and may copy data if needed\n",
    "- **`.view()`** shares memory with the original tensor (faster but requires contiguous memory)\n",
    "- Use `-1` in reshape to automatically infer one dimension\n",
    "- Common patterns: flattening (`.reshape(-1)`), adding batch dimensions (`.reshape(1, -1)`)\n",
    "\n",
    "Understanding and manipulating tensor shapes is fundamental to building neural networks, where data flows through layers with specific shape requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe0d66",
   "metadata": {},
   "source": [
    "## Device Management - CPU vs GPU Computation\n",
    "\n",
    "**What is device management?**\n",
    "\n",
    "In PyTorch, a \"device\" refers to where your tensors and computations live - either on the CPU (your computer's processor) or GPU (graphics card). Device management is the practice of controlling where your data and models are placed.\n",
    "\n",
    "**Why does device management matter?**\n",
    "\n",
    "GPUs can perform tensor operations **10-100x faster** than CPUs for deep learning workloads because:\n",
    "- GPUs have thousands of cores optimized for parallel computation\n",
    "- Deep learning operations (matrix multiplications, convolutions) are highly parallelizable\n",
    "- Modern GPUs have specialized hardware for AI workloads (like NVIDIA's Tensor Cores)\n",
    "\n",
    "However, not all operations need GPU acceleration:\n",
    "- **Use GPU**: Training large models, processing big batches, production inference at scale\n",
    "- **Use CPU**: Small models, debugging, when GPU isn't available, data preprocessing\n",
    "\n",
    "**Key principle**: Tensors must be on the same device to operate together. You can't add a CPU tensor to a GPU tensor without moving one of them first!\n",
    "\n",
    "Let's explore how to detect devices and move tensors between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd2d4ac",
   "metadata": {},
   "source": [
    "### Detecting Available Devices\n",
    "\n",
    "First, let's check what devices are available on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (NVIDIA GPU) is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA (GPU) available: {cuda_available}\")\n",
    "print()\n",
    "\n",
    "if cuda_available:\n",
    "    # Get GPU information\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print()\n",
    "    \n",
    "    # Get GPU memory info\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"No GPU detected. Computations will run on CPU.\")\n",
    "    print(\"This is fine for learning and small experiments!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d21587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a device object for easy reference\n",
    "# This is a common pattern in PyTorch code\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print()\n",
    "\n",
    "# You can also specify a specific GPU if you have multiple\n",
    "# device = torch.device('cuda:0')  # First GPU\n",
    "# device = torch.device('cuda:1')  # Second GPU\n",
    "# device = torch.device('cpu')     # Force CPU\n",
    "\n",
    "print(f\"Device type: {device.type}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Device index: {device.index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970651e",
   "metadata": {},
   "source": [
    "### Creating Tensors on Specific Devices\n",
    "\n",
    "You can create tensors directly on a specific device, or create them on CPU and move them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Create tensor on CPU (default)\n",
    "cpu_tensor = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "print(\"Tensor created on CPU:\")\n",
    "print(f\"Tensor: {cpu_tensor}\")\n",
    "print(f\"Device: {cpu_tensor.device}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Create tensor directly on a specific device\n",
    "device_tensor = torch.tensor([1.0, 2.0, 3.0, 4.0], device=device)\n",
    "print(f\"Tensor created on {device}:\")\n",
    "print(f\"Tensor: {device_tensor}\")\n",
    "print(f\"Device: {device_tensor.device}\")\n",
    "print()\n",
    "\n",
    "# Method 3: Use device-specific constructors\n",
    "zeros_on_device = torch.zeros(3, 3, device=device)\n",
    "print(f\"Zeros tensor on {device}:\")\n",
    "print(zeros_on_device)\n",
    "print(f\"Device: {zeros_on_device.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3213e74f",
   "metadata": {},
   "source": [
    "### Moving Tensors Between Devices\n",
    "\n",
    "PyTorch provides several methods to move tensors between CPU and GPU. This is essential when you need to:\n",
    "- Move data to GPU for training\n",
    "- Move results back to CPU for visualization or saving\n",
    "- Ensure all tensors in an operation are on the same device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7817c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor on CPU\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "print(\"Original tensor (CPU):\")\n",
    "print(f\"Tensor: \\n{x}\")\n",
    "print(f\"Device: {x.device}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9915b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Using .to() method (most common)\n",
    "x_on_device = x.to(device)\n",
    "print(f\"Tensor moved to {device} using .to():\")\n",
    "print(f\"Tensor: \\n{x_on_device}\")\n",
    "print(f\"Device: {x_on_device.device}\")\n",
    "print()\n",
    "\n",
    "# .to() creates a copy if the device is different\n",
    "# If already on the target device, it returns the same tensor\n",
    "print(f\"Original tensor still on: {x.device}\")\n",
    "print(f\"New tensor on: {x_on_device.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0477857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using .cuda() and .cpu() methods\n",
    "# These are shortcuts for moving to GPU and CPU respectively\n",
    "\n",
    "# Move to CPU\n",
    "x_cpu = x_on_device.cpu()\n",
    "print(\"Moved to CPU using .cpu():\")\n",
    "print(f\"Device: {x_cpu.device}\")\n",
    "print()\n",
    "\n",
    "# Move to GPU (if available)\n",
    "if torch.cuda.is_available():\n",
    "    x_gpu = x.cuda()\n",
    "    print(\"Moved to GPU using .cuda():\")\n",
    "    print(f\"Device: {x_gpu.device}\")\n",
    "    print()\n",
    "    \n",
    "    # You can specify which GPU\n",
    "    # x_gpu = x.cuda(0)  # Move to first GPU\n",
    "else:\n",
    "    print(\"GPU not available, skipping .cuda() example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: In-place movement (less common)\n",
    "# Note: This is rarely used because .to() is more flexible\n",
    "y = torch.tensor([5.0, 6.0, 7.0])\n",
    "print(f\"Before: {y.device}\")\n",
    "\n",
    "# Using .to() with the same variable name\n",
    "y = y.to(device)\n",
    "print(f\"After: {y.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be49e6",
   "metadata": {},
   "source": [
    "### Device Compatibility - A Critical Rule\n",
    "\n",
    "**Important**: All tensors in an operation must be on the same device. Mixing CPU and GPU tensors will cause an error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors on different devices\n",
    "a = torch.tensor([1.0, 2.0, 3.0])  # CPU by default\n",
    "b = torch.tensor([4.0, 5.0, 6.0], device=device)  # On our target device\n",
    "\n",
    "print(f\"Tensor a device: {a.device}\")\n",
    "print(f\"Tensor b device: {b.device}\")\n",
    "print()\n",
    "\n",
    "# This will cause an error if devices are different!\n",
    "# Uncomment to see the error:\n",
    "# result = a + b  # RuntimeError: Expected all tensors to be on the same device\n",
    "\n",
    "# Solution: Move tensors to the same device\n",
    "a_on_device = a.to(device)\n",
    "result = a_on_device + b\n",
    "print(\"After moving to same device:\")\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Result device: {result.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07804647",
   "metadata": {},
   "source": [
    "### Practical Device Management Patterns\n",
    "\n",
    "Here are common patterns you'll use in real PyTorch code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caea547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Set device at the start of your script\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print()\n",
    "\n",
    "# Pattern 2: Move model and data to device\n",
    "# (We'll see this more with nn.Module later)\n",
    "data = torch.randn(10, 5)  # Some data\n",
    "data = data.to(device)\n",
    "print(f\"Data on device: {data.device}\")\n",
    "print()\n",
    "\n",
    "# Pattern 3: Move results back to CPU for numpy conversion\n",
    "# (GPU tensors can't be converted to numpy directly)\n",
    "result = data.sum()\n",
    "result_cpu = result.cpu()\n",
    "result_numpy = result_cpu.numpy()\n",
    "print(f\"Result as numpy: {result_numpy}\")\n",
    "print(f\"Type: {type(result_numpy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 4: Check device before operations\n",
    "def safe_add(tensor1, tensor2):\n",
    "    \"\"\"Add two tensors, ensuring they're on the same device.\"\"\"\n",
    "    if tensor1.device != tensor2.device:\n",
    "        tensor2 = tensor2.to(tensor1.device)\n",
    "    return tensor1 + tensor2\n",
    "\n",
    "x = torch.tensor([1.0, 2.0])\n",
    "y = torch.tensor([3.0, 4.0], device=device)\n",
    "\n",
    "result = safe_add(x, y)\n",
    "print(f\"Safe add result: {result}\")\n",
    "print(f\"Result device: {result.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84933069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 5: Timing CPU vs GPU operations (if GPU available)\n",
    "import time\n",
    "\n",
    "# Create large tensors for meaningful comparison\n",
    "size = 5000\n",
    "a_cpu = torch.randn(size, size)\n",
    "b_cpu = torch.randn(size, size)\n",
    "\n",
    "# Time CPU operation\n",
    "start = time.time()\n",
    "c_cpu = torch.matmul(a_cpu, b_cpu)\n",
    "cpu_time = time.time() - start\n",
    "print(f\"CPU matrix multiplication time: {cpu_time:.4f} seconds\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Move to GPU\n",
    "    a_gpu = a_cpu.to(device)\n",
    "    b_gpu = b_cpu.to(device)\n",
    "    \n",
    "    # Warm up GPU (first operation is slower)\n",
    "    _ = torch.matmul(a_gpu, b_gpu)\n",
    "    torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "    \n",
    "    # Time GPU operation\n",
    "    start = time.time()\n",
    "    c_gpu = torch.matmul(a_gpu, b_gpu)\n",
    "    torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "    gpu_time = time.time() - start\n",
    "    \n",
    "    print(f\"GPU matrix multiplication time: {gpu_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {cpu_time / gpu_time:.2f}x faster on GPU\")\n",
    "else:\n",
    "    print(\"GPU not available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea10577",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- **Device** refers to where tensors live: CPU or GPU (CUDA)\n",
    "- Use `torch.device('cuda' if torch.cuda.is_available() else 'cpu')` to automatically select the best device\n",
    "- Create tensors on a device with `device=` parameter: `torch.tensor([1, 2], device=device)`\n",
    "- Move tensors with `.to(device)`, `.cuda()`, or `.cpu()` methods\n",
    "- **Critical rule**: All tensors in an operation must be on the same device\n",
    "- GPUs provide massive speedups (10-100x) for large tensor operations\n",
    "- Move results to CPU with `.cpu()` before converting to NumPy\n",
    "\n",
    "**Best Practices:**\n",
    "1. Set your device once at the start: `device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')`\n",
    "2. Move models and data to device immediately after creation\n",
    "3. Keep tensors on GPU during training, only move to CPU when needed\n",
    "4. Use `.to(device)` instead of `.cuda()` for code that works on both CPU and GPU\n",
    "\n",
    "Device management becomes second nature with practice. The key is remembering that tensors must be on the same device to interact!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e3d3ff",
   "metadata": {},
   "source": [
    "## Dimension Arguments - Specifying Axes for Operations\n",
    "\n",
    "**What are dimension arguments?**\n",
    "\n",
    "Many PyTorch operations (like sum, mean, max) can work across specific dimensions (axes) of a tensor. The dimension argument (often called `dim` or `axis`) tells PyTorch which dimension to operate along. Think of it as choosing which direction to \"collapse\" the tensor.\n",
    "\n",
    "**Why do dimension arguments matter?**\n",
    "\n",
    "In deep learning, you constantly need to perform operations along specific dimensions:\n",
    "- **Batch processing**: Average loss across all samples in a batch (dim=0)\n",
    "- **Sequence operations**: Sum values across time steps (dim=1)\n",
    "- **Feature aggregation**: Find maximum activation across feature channels (dim=-1)\n",
    "- **Attention mechanisms**: Compute softmax over sequence length dimension\n",
    "\n",
    "Understanding dimensions is crucial because:\n",
    "- It controls how data is aggregated or transformed\n",
    "- Wrong dimension = wrong results (and often hard-to-debug errors!)\n",
    "- Many neural network operations depend on dimension-specific computations\n",
    "\n",
    "**Dimension indexing**:\n",
    "- Dimensions are **zero-indexed**: first dimension is 0, second is 1, etc.\n",
    "- **Negative indexing** works too: -1 is the last dimension, -2 is second-to-last\n",
    "- For a 2D tensor (matrix): dim=0 is rows, dim=1 is columns\n",
    "\n",
    "Let's explore how dimension arguments work with practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db478e0f",
   "metadata": {},
   "source": [
    "### Operations on 2D Tensors (Matrices)\n",
    "\n",
    "Let's start with a simple 2D tensor to understand how dimensions work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 2D tensor (matrix)\n",
    "# Think of this as 3 samples (rows), each with 4 features (columns)\n",
    "matrix = torch.tensor([\n",
    "    [1.0, 2.0, 3.0, 4.0],   # Sample 1\n",
    "    [5.0, 6.0, 7.0, 8.0],   # Sample 2\n",
    "    [9.0, 10.0, 11.0, 12.0] # Sample 3\n",
    "])\n",
    "\n",
    "print(\"Original matrix:\")\n",
    "print(matrix)\n",
    "print(f\"Shape: {matrix.shape}  # (3 rows, 4 columns)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02449b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum without specifying dimension - sums ALL elements\n",
    "total_sum = matrix.sum()\n",
    "print(\"Sum of all elements (no dim specified):\")\n",
    "print(f\"Result: {total_sum}\")\n",
    "print(f\"Shape: {total_sum.shape}  # Scalar (0D tensor)\")\n",
    "print(f\"Calculation: 1+2+3+...+12 = {total_sum.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be390ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum along dimension 0 (down the rows)\n",
    "# This collapses the row dimension, leaving only columns\n",
    "sum_dim0 = matrix.sum(dim=0)\n",
    "print(\"Sum along dimension 0 (sum down each column):\")\n",
    "print(f\"Result: {sum_dim0}\")\n",
    "print(f\"Shape: {sum_dim0.shape}  # (4,) - one value per column\")\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(\"  Column 0: 1 + 5 + 9 = 15\")\n",
    "print(\"  Column 1: 2 + 6 + 10 = 18\")\n",
    "print(\"  Column 2: 3 + 7 + 11 = 21\")\n",
    "print(\"  Column 3: 4 + 8 + 12 = 24\")\n",
    "print()\n",
    "print(\"Use case: Summing features across all samples in a batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6355067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum along dimension 1 (across the columns)\n",
    "# This collapses the column dimension, leaving only rows\n",
    "sum_dim1 = matrix.sum(dim=1)\n",
    "print(\"Sum along dimension 1 (sum across each row):\")\n",
    "print(f\"Result: {sum_dim1}\")\n",
    "print(f\"Shape: {sum_dim1.shape}  # (3,) - one value per row\")\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(\"  Row 0: 1 + 2 + 3 + 4 = 10\")\n",
    "print(\"  Row 1: 5 + 6 + 7 + 8 = 26\")\n",
    "print(\"  Row 2: 9 + 10 + 11 + 12 = 42\")\n",
    "print()\n",
    "print(\"Use case: Summing all features for each sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using negative indexing\n",
    "# dim=-1 refers to the last dimension (same as dim=1 for 2D tensors)\n",
    "sum_dim_neg1 = matrix.sum(dim=-1)\n",
    "print(\"Sum along dimension -1 (last dimension = columns):\")\n",
    "print(f\"Result: {sum_dim_neg1}\")\n",
    "print(f\"Shape: {sum_dim_neg1.shape}\")\n",
    "print()\n",
    "print(\"This gives the same result as dim=1:\")\n",
    "print(f\"dim=1:  {sum_dim1}\")\n",
    "print(f\"dim=-1: {sum_dim_neg1}\")\n",
    "print()\n",
    "print(\"Tip: Using dim=-1 makes code work with tensors of any number of dimensions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5827053",
   "metadata": {},
   "source": [
    "### Mean Operations Across Dimensions\n",
    "\n",
    "The `mean()` operation works the same way as `sum()`, but computes the average instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean across different dimensions\n",
    "print(\"Original matrix:\")\n",
    "print(matrix)\n",
    "print(f\"Shape: {matrix.shape}\")\n",
    "print()\n",
    "\n",
    "# Mean of all elements\n",
    "mean_all = matrix.mean()\n",
    "print(f\"Mean of all elements: {mean_all:.2f}\")\n",
    "print(f\"Calculation: sum(78) / count(12) = {mean_all:.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c26c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean along dimension 0 (average down each column)\n",
    "mean_dim0 = matrix.mean(dim=0)\n",
    "print(\"Mean along dimension 0 (average of each column):\")\n",
    "print(f\"Result: {mean_dim0}\")\n",
    "print(f\"Shape: {mean_dim0.shape}\")\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(\"  Column 0: (1 + 5 + 9) / 3 = 5.0\")\n",
    "print(\"  Column 1: (2 + 6 + 10) / 3 = 6.0\")\n",
    "print(\"  Column 2: (3 + 7 + 11) / 3 = 7.0\")\n",
    "print(\"  Column 3: (4 + 8 + 12) / 3 = 8.0\")\n",
    "print()\n",
    "print(\"Use case: Computing average feature values across a batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e36b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean along dimension 1 (average across each row)\n",
    "mean_dim1 = matrix.mean(dim=1)\n",
    "print(\"Mean along dimension 1 (average of each row):\")\n",
    "print(f\"Result: {mean_dim1}\")\n",
    "print(f\"Shape: {mean_dim1.shape}\")\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(\"  Row 0: (1 + 2 + 3 + 4) / 4 = 2.5\")\n",
    "print(\"  Row 1: (5 + 6 + 7 + 8) / 4 = 6.5\")\n",
    "print(\"  Row 2: (9 + 10 + 11 + 12) / 4 = 10.5\")\n",
    "print()\n",
    "print(\"Use case: Computing average activation per sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cf4d3c",
   "metadata": {},
   "source": [
    "### Operations on 3D Tensors\n",
    "\n",
    "With 3D tensors, dimension arguments become even more important. Let's use a tensor shaped like batch data: (batch_size, sequence_length, features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf24de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D tensor: 2 samples, 3 time steps, 4 features\n",
    "# Common shape for sequence data (like text or time series)\n",
    "tensor_3d = torch.tensor([\n",
    "    # Sample 1\n",
    "    [[1, 2, 3, 4],      # Time step 0\n",
    "     [5, 6, 7, 8],      # Time step 1\n",
    "     [9, 10, 11, 12]],  # Time step 2\n",
    "    \n",
    "    # Sample 2\n",
    "    [[13, 14, 15, 16],  # Time step 0\n",
    "     [17, 18, 19, 20],  # Time step 1\n",
    "     [21, 22, 23, 24]]  # Time step 2\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(\"3D Tensor (batch_size=2, sequence_length=3, features=4):\")\n",
    "print(tensor_3d)\n",
    "print(f\"Shape: {tensor_3d.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a43a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum along dimension 0 (across batch)\n",
    "sum_batch = tensor_3d.sum(dim=0)\n",
    "print(\"Sum along dim=0 (sum across batch):\")\n",
    "print(sum_batch)\n",
    "print(f\"Shape: {sum_batch.shape}  # (3, 4) - batch dimension removed\")\n",
    "print()\n",
    "print(\"This sums corresponding positions from both samples\")\n",
    "print(\"Example: Position [0,0] = 1 + 13 = 14\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ba3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum along dimension 1 (across sequence/time)\n",
    "sum_sequence = tensor_3d.sum(dim=1)\n",
    "print(\"Sum along dim=1 (sum across sequence):\")\n",
    "print(sum_sequence)\n",
    "print(f\"Shape: {sum_sequence.shape}  # (2, 4) - sequence dimension removed\")\n",
    "print()\n",
    "print(\"This sums all time steps for each sample\")\n",
    "print(\"Sample 1, Feature 0: 1 + 5 + 9 = 15\")\n",
    "print(\"Sample 2, Feature 0: 13 + 17 + 21 = 51\")\n",
    "print()\n",
    "print(\"Use case: Aggregating information across time steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324fd676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum along dimension 2 (across features)\n",
    "sum_features = tensor_3d.sum(dim=2)\n",
    "print(\"Sum along dim=2 (sum across features):\")\n",
    "print(sum_features)\n",
    "print(f\"Shape: {sum_features.shape}  # (2, 3) - feature dimension removed\")\n",
    "print()\n",
    "print(\"This sums all features for each time step of each sample\")\n",
    "print(\"Sample 1, Time 0: 1 + 2 + 3 + 4 = 10\")\n",
    "print(\"Sample 1, Time 1: 5 + 6 + 7 + 8 = 26\")\n",
    "print()\n",
    "print(\"Use case: Computing total activation at each time step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b452955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dim=-1 for the last dimension (features)\n",
    "sum_last_dim = tensor_3d.sum(dim=-1)\n",
    "print(\"Sum along dim=-1 (last dimension):\")\n",
    "print(sum_last_dim)\n",
    "print(f\"Shape: {sum_last_dim.shape}\")\n",
    "print()\n",
    "print(\"Same result as dim=2:\")\n",
    "print(f\"Are they equal? {torch.equal(sum_features, sum_last_dim)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0ce3f",
   "metadata": {},
   "source": [
    "### The `keepdim` Parameter - Preserving Dimensions\n",
    "\n",
    "By default, operations like `sum()` and `mean()` remove the dimension they operate on. Sometimes you want to keep that dimension (with size 1) for broadcasting or shape consistency. That's what the `keepdim` parameter does!\n",
    "\n",
    "**Why use keepdim?**\n",
    "- **Broadcasting**: Keep dimensions for element-wise operations with the original tensor\n",
    "- **Shape consistency**: Maintain the same number of dimensions throughout a pipeline\n",
    "- **Normalization**: Useful for operations like batch normalization or layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 2D tensor\n",
    "x = torch.tensor([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0]\n",
    "])\n",
    "\n",
    "print(\"Original tensor:\")\n",
    "print(x)\n",
    "print(f\"Shape: {x.shape}  # (3, 3)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum along dimension 1 WITHOUT keepdim (default)\n",
    "sum_no_keepdim = x.sum(dim=1)\n",
    "print(\"Sum with keepdim=False (default):\")\n",
    "print(sum_no_keepdim)\n",
    "print(f\"Shape: {sum_no_keepdim.shape}  # (3,) - dimension removed!\")\n",
    "print()\n",
    "\n",
    "# Sum along dimension 1 WITH keepdim\n",
    "sum_with_keepdim = x.sum(dim=1, keepdim=True)\n",
    "print(\"Sum with keepdim=True:\")\n",
    "print(sum_with_keepdim)\n",
    "print(f\"Shape: {sum_with_keepdim.shape}  # (3, 1) - dimension kept!\")\n",
    "print()\n",
    "print(\"Notice: keepdim=True preserves the dimension with size 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203fa917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example: Normalizing each row\n",
    "# We want to divide each element by its row sum\n",
    "\n",
    "print(\"Original tensor:\")\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "# Without keepdim - this causes a shape mismatch!\n",
    "row_sums = x.sum(dim=1)  # Shape: (3,)\n",
    "print(f\"Row sums shape: {row_sums.shape}\")\n",
    "print(f\"Original shape: {x.shape}\")\n",
    "print(\"Can't directly divide (3,3) by (3,) - shapes don't align properly!\")\n",
    "print()\n",
    "\n",
    "# With keepdim - broadcasting works perfectly!\n",
    "row_sums_keepdim = x.sum(dim=1, keepdim=True)  # Shape: (3, 1)\n",
    "normalized = x / row_sums_keepdim\n",
    "print(\"With keepdim=True:\")\n",
    "print(f\"Row sums shape: {row_sums_keepdim.shape}  # (3, 1)\")\n",
    "print(f\"Normalized tensor:\")\n",
    "print(normalized)\n",
    "print()\n",
    "print(\"Each row now sums to 1.0:\")\n",
    "print(f\"Row sums: {normalized.sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example: Mean normalization (zero mean, unit variance)\n",
    "data = torch.tensor([\n",
    "    [10.0, 20.0, 30.0],\n",
    "    [15.0, 25.0, 35.0],\n",
    "    [12.0, 22.0, 32.0]\n",
    "])\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "# Normalize each row to have mean=0\n",
    "row_means = data.mean(dim=1, keepdim=True)\n",
    "centered = data - row_means\n",
    "\n",
    "print(\"After centering (subtracting row means):\")\n",
    "print(centered)\n",
    "print(f\"Row means of centered data: {centered.mean(dim=1)}\")\n",
    "print(\"(Should be close to 0)\")\n",
    "print()\n",
    "\n",
    "# Further normalize by standard deviation\n",
    "row_std = data.std(dim=1, keepdim=True)\n",
    "normalized = (data - row_means) / row_std\n",
    "\n",
    "print(\"After full normalization (mean=0, std=1):\")\n",
    "print(normalized)\n",
    "print(f\"Row means: {normalized.mean(dim=1)}\")\n",
    "print(f\"Row stds: {normalized.std(dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246bacfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keepdim with 3D tensors\n",
    "tensor_3d_small = torch.randn(2, 3, 4)  # batch=2, seq=3, features=4\n",
    "\n",
    "print(f\"Original shape: {tensor_3d_small.shape}\")\n",
    "print()\n",
    "\n",
    "# Mean across features without keepdim\n",
    "mean_no_keep = tensor_3d_small.mean(dim=2)\n",
    "print(f\"Mean(dim=2, keepdim=False) shape: {mean_no_keep.shape}  # (2, 3)\")\n",
    "print()\n",
    "\n",
    "# Mean across features with keepdim\n",
    "mean_keep = tensor_3d_small.mean(dim=2, keepdim=True)\n",
    "print(f\"Mean(dim=2, keepdim=True) shape: {mean_keep.shape}  # (2, 3, 1)\")\n",
    "print()\n",
    "print(\"With keepdim=True, we can easily broadcast operations back to original shape!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b062fcab",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**Dimension Arguments:**\n",
    "- The `dim` parameter specifies which dimension to operate along\n",
    "- Dimensions are zero-indexed: 0, 1, 2, ...\n",
    "- Negative indexing works: -1 (last), -2 (second-to-last), etc.\n",
    "- Operations like `sum()`, `mean()`, `max()`, `min()` support dimension arguments\n",
    "- Without `dim`, operations work on ALL elements\n",
    "\n",
    "**Common Patterns:**\n",
    "- `dim=0`: Operate across the batch (aggregate samples)\n",
    "- `dim=1`: Operate across sequences/rows\n",
    "- `dim=-1`: Operate across the last dimension (often features)\n",
    "\n",
    "**The keepdim Parameter:**\n",
    "- `keepdim=False` (default): Removes the dimension being operated on\n",
    "- `keepdim=True`: Keeps the dimension with size 1\n",
    "- Use `keepdim=True` when you need to broadcast operations back to the original shape\n",
    "- Essential for normalization operations (mean normalization, batch norm, layer norm)\n",
    "\n",
    "**Practical Applications:**\n",
    "- Computing batch statistics: `batch.mean(dim=0)`\n",
    "- Aggregating sequences: `sequence.sum(dim=1)`\n",
    "- Normalizing features: `(x - x.mean(dim=-1, keepdim=True)) / x.std(dim=-1, keepdim=True)`\n",
    "- Finding maximum activations: `activations.max(dim=1)`\n",
    "\n",
    "Understanding dimension arguments is fundamental to working with multi-dimensional data in PyTorch. They give you precise control over how operations aggregate or transform your tensors!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3c2eaf",
   "metadata": {},
   "source": [
    "## Matrix Multiplication - The Heart of Neural Networks\n",
    "\n",
    "**What is matrix multiplication?**\n",
    "\n",
    "Matrix multiplication is a fundamental linear algebra operation that combines two matrices to produce a new matrix. Unlike element-wise multiplication (where you multiply corresponding elements), matrix multiplication follows specific rules:\n",
    "- The number of columns in the first matrix must equal the number of rows in the second matrix\n",
    "- The result has the shape (rows of first matrix, columns of second matrix)\n",
    "- Each element in the result is the dot product of a row from the first matrix and a column from the second\n",
    "\n",
    "**Why does matrix multiplication matter?**\n",
    "\n",
    "Matrix multiplication is **the core operation** in neural networks:\n",
    "- **Linear layers**: Transform inputs via weight matrices: `output = input @ weights + bias`\n",
    "- **Attention mechanisms**: Compute query-key interactions in transformers\n",
    "- **Convolutions**: Can be expressed as matrix multiplications\n",
    "- **Batch processing**: Efficiently process multiple samples simultaneously\n",
    "\n",
    "Every forward pass through a neural network involves dozens or hundreds of matrix multiplications. Understanding this operation is essential for understanding how neural networks work!\n",
    "\n",
    "**PyTorch provides three ways to do matrix multiplication:**\n",
    "1. `torch.mm()` - Basic matrix multiplication (2D only)\n",
    "2. `torch.matmul()` - General matrix multiplication (supports broadcasting)\n",
    "3. `@` operator - Pythonic syntax for matmul (recommended)\n",
    "\n",
    "Let's explore each one and see when to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc65af",
   "metadata": {},
   "source": [
    "### torch.mm() - Basic Matrix Multiplication\n",
    "\n",
    "`torch.mm()` performs standard matrix multiplication on 2D tensors (matrices). It's the most straightforward but also the most restrictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a079c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create two matrices\n",
    "# Matrix A: 3 rows, 4 columns\n",
    "A = torch.tensor([\n",
    "    [1.0, 2.0, 3.0, 4.0],\n",
    "    [5.0, 6.0, 7.0, 8.0],\n",
    "    [9.0, 10.0, 11.0, 12.0]\n",
    "])\n",
    "\n",
    "# Matrix B: 4 rows, 2 columns\n",
    "# Note: A's columns (4) must match B's rows (4)\n",
    "B = torch.tensor([\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0],\n",
    "    [5.0, 6.0],\n",
    "    [7.0, 8.0]\n",
    "])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(f\"Shape: {A.shape}  # (3, 4)\")\n",
    "print()\n",
    "\n",
    "print(\"Matrix B:\")\n",
    "print(B)\n",
    "print(f\"Shape: {B.shape}  # (4, 2)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3c295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform matrix multiplication using torch.mm()\n",
    "C = torch.mm(A, B)\n",
    "\n",
    "print(\"Result C = A @ B:\")\n",
    "print(C)\n",
    "print(f\"Shape: {C.shape}  # (3, 2) - rows from A, columns from B\")\n",
    "print()\n",
    "\n",
    "# Let's verify one element manually\n",
    "# C[0, 0] = A[0, :] · B[:, 0]\n",
    "#         = (1*1) + (2*3) + (3*5) + (4*7)\n",
    "#         = 1 + 6 + 15 + 28 = 50\n",
    "print(\"Manual calculation of C[0, 0]:\")\n",
    "print(f\"A[0, :] = {A[0, :]}\")\n",
    "print(f\"B[:, 0] = {B[:, 0]}\")\n",
    "print(f\"Dot product: (1*1) + (2*3) + (3*5) + (4*7) = {C[0, 0].item()}\")\n",
    "print()\n",
    "\n",
    "# Verify another element\n",
    "print(\"Manual calculation of C[1, 1]:\")\n",
    "print(f\"A[1, :] = {A[1, :]}\")\n",
    "print(f\"B[:, 1] = {B[:, 1]}\")\n",
    "print(f\"Dot product: (5*2) + (6*4) + (7*6) + (8*8) = {C[1, 1].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf194da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape compatibility is crucial!\n",
    "# The inner dimensions must match\n",
    "\n",
    "X = torch.randn(3, 5)  # 3 rows, 5 columns\n",
    "Y = torch.randn(5, 2)  # 5 rows, 2 columns\n",
    "\n",
    "result = torch.mm(X, Y)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Y shape: {Y.shape}\")\n",
    "print(f\"Result shape: {result.shape}  # (3, 2)\")\n",
    "print()\n",
    "print(\"Rule: (m, n) @ (n, p) = (m, p)\")\n",
    "print(\"The middle dimension (n) must match!\")\n",
    "print()\n",
    "\n",
    "# This would cause an error:\n",
    "# Z = torch.randn(3, 7)\n",
    "# torch.mm(X, Z)  # Error! X has 5 columns but Z has 3 rows\n",
    "print(\"Note: torch.mm() only works with 2D tensors (matrices)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493d2824",
   "metadata": {},
   "source": [
    "### torch.matmul() and @ Operator - Flexible Matrix Multiplication\n",
    "\n",
    "`torch.matmul()` (and its alias, the `@` operator) is more flexible than `torch.mm()`:\n",
    "- Works with tensors of any dimension (1D, 2D, 3D, etc.)\n",
    "- Supports broadcasting for batch operations\n",
    "- Handles different combinations of tensor dimensions intelligently\n",
    "\n",
    "**The `@` operator is the recommended way** to do matrix multiplication in PyTorch - it's clean, readable, and Pythonic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a3bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using @ operator (equivalent to torch.matmul)\n",
    "A = torch.tensor([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0]\n",
    "])\n",
    "\n",
    "B = torch.tensor([\n",
    "    [7.0, 8.0],\n",
    "    [9.0, 10.0],\n",
    "    [11.0, 12.0]\n",
    "])\n",
    "\n",
    "# Three equivalent ways:\n",
    "result1 = torch.matmul(A, B)\n",
    "result2 = A @ B\n",
    "result3 = torch.mm(A, B)  # Also works for 2D\n",
    "\n",
    "print(\"All three methods give the same result:\")\n",
    "print(f\"torch.matmul(A, B):\\n{result1}\")\n",
    "print()\n",
    "print(f\"A @ B:\\n{result2}\")\n",
    "print()\n",
    "print(f\"torch.mm(A, B):\\n{result3}\")\n",
    "print()\n",
    "print(f\"All equal? {torch.equal(result1, result2) and torch.equal(result2, result3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector-matrix multiplication (1D @ 2D)\n",
    "vector = torch.tensor([1.0, 2.0, 3.0])  # Shape: (3,)\n",
    "matrix = torch.tensor([\n",
    "    [1.0, 2.0],\n",
    "    [3.0, 4.0],\n",
    "    [5.0, 6.0]\n",
    "])  # Shape: (3, 2)\n",
    "\n",
    "result = vector @ matrix\n",
    "print(\"Vector @ Matrix:\")\n",
    "print(f\"Vector shape: {vector.shape}\")\n",
    "print(f\"Matrix shape: {matrix.shape}\")\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Result shape: {result.shape}  # (2,) - produces a vector\")\n",
    "print()\n",
    "print(\"Calculation:\")\n",
    "print(\"Result[0] = (1*1) + (2*3) + (3*5) = 1 + 6 + 15 = 22\")\n",
    "print(\"Result[1] = (1*2) + (2*4) + (3*6) = 2 + 8 + 18 = 28\")\n",
    "print()\n",
    "\n",
    "# Note: torch.mm() doesn't support this!\n",
    "# torch.mm(vector, matrix)  # Error! torch.mm requires 2D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893832f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix-vector multiplication (2D @ 1D)\n",
    "matrix = torch.tensor([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0]\n",
    "])  # Shape: (2, 3)\n",
    "\n",
    "vector = torch.tensor([1.0, 2.0, 3.0])  # Shape: (3,)\n",
    "\n",
    "result = matrix @ vector\n",
    "print(\"Matrix @ Vector:\")\n",
    "print(f\"Matrix shape: {matrix.shape}\")\n",
    "print(f\"Vector shape: {vector.shape}\")\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Result shape: {result.shape}  # (2,) - produces a vector\")\n",
    "print()\n",
    "print(\"Calculation:\")\n",
    "print(\"Result[0] = (1*1) + (2*2) + (3*3) = 1 + 4 + 9 = 14\")\n",
    "print(\"Result[1] = (4*1) + (5*2) + (6*3) = 4 + 10 + 18 = 32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5824beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector dot product (1D @ 1D)\n",
    "v1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "v2 = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "dot_product = v1 @ v2\n",
    "print(\"Vector @ Vector (dot product):\")\n",
    "print(f\"v1: {v1}\")\n",
    "print(f\"v2: {v2}\")\n",
    "print(f\"Dot product: {dot_product}\")\n",
    "print(f\"Result shape: {dot_product.shape}  # Scalar (0D tensor)\")\n",
    "print()\n",
    "print(\"Calculation: (1*4) + (2*5) + (3*6) = 4 + 10 + 18 = 32\")\n",
    "print()\n",
    "\n",
    "# Alternative using torch.dot()\n",
    "dot_alt = torch.dot(v1, v2)\n",
    "print(f\"Using torch.dot(): {dot_alt}\")\n",
    "print(f\"Same result? {dot_product == dot_alt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b592e8",
   "metadata": {},
   "source": [
    "### Batch Matrix Multiplication - Processing Multiple Matrices at Once\n",
    "\n",
    "One of the most powerful features of `torch.matmul()` and `@` is **batch processing**. You can multiply multiple matrices simultaneously, which is essential for efficient neural network training.\n",
    "\n",
    "**Batch dimensions** are the leading dimensions that are broadcast over. The actual matrix multiplication happens on the last two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3db009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch matrix multiplication: (batch, m, n) @ (batch, n, p) = (batch, m, p)\n",
    "\n",
    "# Create a batch of 3 matrices, each 2x3\n",
    "batch_A = torch.tensor([\n",
    "    [[1, 2, 3], [4, 5, 6]],      # Matrix 1\n",
    "    [[7, 8, 9], [10, 11, 12]],   # Matrix 2\n",
    "    [[13, 14, 15], [16, 17, 18]] # Matrix 3\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Create a batch of 3 matrices, each 3x2\n",
    "batch_B = torch.tensor([\n",
    "    [[1, 2], [3, 4], [5, 6]],    # Matrix 1\n",
    "    [[7, 8], [9, 10], [11, 12]], # Matrix 2\n",
    "    [[13, 14], [15, 16], [17, 18]] # Matrix 3\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"Batch A shape: {batch_A.shape}  # (3, 2, 3) - 3 matrices of 2x3\")\n",
    "print(f\"Batch B shape: {batch_B.shape}  # (3, 3, 2) - 3 matrices of 3x2\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform batch matrix multiplication\n",
    "batch_result = batch_A @ batch_B\n",
    "\n",
    "print(f\"Result shape: {batch_result.shape}  # (3, 2, 2) - 3 matrices of 2x2\")\n",
    "print()\n",
    "print(\"Result (3 matrices):\")\n",
    "print(batch_result)\n",
    "print()\n",
    "\n",
    "# Each matrix in the batch is multiplied independently\n",
    "print(\"Verification - multiply first matrices manually:\")\n",
    "manual_result = batch_A[0] @ batch_B[0]\n",
    "print(f\"Manual: \\n{manual_result}\")\n",
    "print()\n",
    "print(f\"From batch: \\n{batch_result[0]}\")\n",
    "print()\n",
    "print(f\"Match? {torch.equal(manual_result, batch_result[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting in batch matrix multiplication\n",
    "# You can multiply a batch of matrices by a single matrix\n",
    "\n",
    "# Batch of matrices: (4, 3, 2)\n",
    "batch = torch.randn(4, 3, 2)\n",
    "\n",
    "# Single matrix: (2, 5)\n",
    "single = torch.randn(2, 5)\n",
    "\n",
    "# The single matrix is broadcast across the batch\n",
    "result = batch @ single\n",
    "\n",
    "print(f\"Batch shape: {batch.shape}  # (4, 3, 2)\")\n",
    "print(f\"Single matrix shape: {single.shape}  # (2, 5)\")\n",
    "print(f\"Result shape: {result.shape}  # (4, 3, 5)\")\n",
    "print()\n",
    "print(\"The single matrix is multiplied with each matrix in the batch!\")\n",
    "print(\"This is equivalent to:\")\n",
    "print(\"  result[0] = batch[0] @ single\")\n",
    "print(\"  result[1] = batch[1] @ single\")\n",
    "print(\"  result[2] = batch[2] @ single\")\n",
    "print(\"  result[3] = batch[3] @ single\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5884b78",
   "metadata": {},
   "source": [
    "### Matrix Multiplication in Neural Networks\n",
    "\n",
    "Now let's see how matrix multiplication powers neural networks. Every linear layer in a neural network is essentially a matrix multiplication!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddcba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating a linear layer: y = x @ W + b\n",
    "# This is what happens inside nn.Linear()\n",
    "\n",
    "# Input: batch of 32 samples, each with 10 features\n",
    "batch_size = 32\n",
    "input_features = 10\n",
    "output_features = 5\n",
    "\n",
    "# Input data\n",
    "x = torch.randn(batch_size, input_features)\n",
    "\n",
    "# Weight matrix (this is learned during training)\n",
    "W = torch.randn(input_features, output_features)\n",
    "\n",
    "# Bias vector (also learned during training)\n",
    "b = torch.randn(output_features)\n",
    "\n",
    "print(\"Linear layer computation: y = x @ W + b\")\n",
    "print()\n",
    "print(f\"Input shape: {x.shape}  # (32, 10) - 32 samples, 10 features each\")\n",
    "print(f\"Weight shape: {W.shape}  # (10, 5) - transform 10 features to 5\")\n",
    "print(f\"Bias shape: {b.shape}  # (5,) - one bias per output feature\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bbd289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the linear transformation\n",
    "y = x @ W + b\n",
    "\n",
    "print(f\"Output shape: {y.shape}  # (32, 5) - 32 samples, 5 features each\")\n",
    "print()\n",
    "print(\"What happened:\")\n",
    "print(\"1. Matrix multiplication: x @ W transforms (32, 10) to (32, 5)\")\n",
    "print(\"2. Bias addition: + b adds (5,) to each of the 32 samples (broadcasting)\")\n",
    "print()\n",
    "print(\"This is exactly what nn.Linear(10, 5) does!\")\n",
    "print()\n",
    "\n",
    "# Verify with actual nn.Linear\n",
    "import torch.nn as nn\n",
    "\n",
    "linear_layer = nn.Linear(input_features, output_features)\n",
    "y_layer = linear_layer(x)\n",
    "\n",
    "print(f\"Using nn.Linear: {y_layer.shape}\")\n",
    "print(\"Same shape as our manual computation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b61397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layer network: chaining matrix multiplications\n",
    "# This is how deep neural networks work!\n",
    "\n",
    "# Input: 16 samples, 20 features\n",
    "x = torch.randn(16, 20)\n",
    "\n",
    "# Layer 1: 20 -> 15 features\n",
    "W1 = torch.randn(20, 15)\n",
    "b1 = torch.randn(15)\n",
    "h1 = x @ W1 + b1\n",
    "h1 = torch.relu(h1)  # Activation function\n",
    "\n",
    "# Layer 2: 15 -> 10 features\n",
    "W2 = torch.randn(15, 10)\n",
    "b2 = torch.randn(10)\n",
    "h2 = h1 @ W2 + b2\n",
    "h2 = torch.relu(h2)  # Activation function\n",
    "\n",
    "# Layer 3: 10 -> 3 features (output)\n",
    "W3 = torch.randn(10, 3)\n",
    "b3 = torch.randn(3)\n",
    "output = h2 @ W3 + b3\n",
    "\n",
    "print(\"Three-layer neural network:\")\n",
    "print(f\"Input:    {x.shape}  # (16, 20)\")\n",
    "print(f\"Layer 1:  {h1.shape}  # (16, 15) after x @ W1 + b1\")\n",
    "print(f\"Layer 2:  {h2.shape}  # (16, 10) after h1 @ W2 + b2\")\n",
    "print(f\"Output:   {output.shape}  # (16, 3) after h2 @ W3 + b3\")\n",
    "print()\n",
    "print(\"Each layer is a matrix multiplication followed by an activation!\")\n",
    "print(\"This is the essence of deep learning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf61c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mechanism example (simplified)\n",
    "# Matrix multiplication is central to transformer models\n",
    "\n",
    "# Sequence: 8 tokens, each with 64-dimensional embeddings\n",
    "sequence_length = 8\n",
    "embedding_dim = 64\n",
    "batch_size = 2\n",
    "\n",
    "# Input: (batch, sequence, embedding)\n",
    "x = torch.randn(batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "# Query, Key, Value projections\n",
    "W_q = torch.randn(embedding_dim, embedding_dim)\n",
    "W_k = torch.randn(embedding_dim, embedding_dim)\n",
    "W_v = torch.randn(embedding_dim, embedding_dim)\n",
    "\n",
    "# Compute Q, K, V\n",
    "Q = x @ W_q  # (2, 8, 64)\n",
    "K = x @ W_k  # (2, 8, 64)\n",
    "V = x @ W_v  # (2, 8, 64)\n",
    "\n",
    "print(\"Attention mechanism (simplified):\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")\n",
    "print()\n",
    "\n",
    "# Attention scores: Q @ K^T\n",
    "# Need to transpose the last two dimensions of K\n",
    "K_transposed = K.transpose(-2, -1)  # (2, 64, 8)\n",
    "attention_scores = Q @ K_transposed  # (2, 8, 8)\n",
    "\n",
    "print(f\"Attention scores shape: {attention_scores.shape}\")\n",
    "print(\"This tells us how much each token should attend to every other token!\")\n",
    "print()\n",
    "\n",
    "# Apply attention to values\n",
    "attention_weights = torch.softmax(attention_scores / (embedding_dim ** 0.5), dim=-1)\n",
    "attention_output = attention_weights @ V  # (2, 8, 64)\n",
    "\n",
    "print(f\"Attention output shape: {attention_output.shape}\")\n",
    "print()\n",
    "print(\"Matrix multiplications in attention:\")\n",
    "print(\"1. x @ W_q, x @ W_k, x @ W_v - project to Q, K, V\")\n",
    "print(\"2. Q @ K^T - compute attention scores\")\n",
    "print(\"3. attention_weights @ V - apply attention to values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4fb81",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**Three Ways to Multiply Matrices:**\n",
    "1. **`torch.mm(A, B)`** - Basic 2D matrix multiplication only\n",
    "2. **`torch.matmul(A, B)`** - Flexible, supports broadcasting and batches\n",
    "3. **`A @ B`** - Pythonic operator, equivalent to matmul (recommended!)\n",
    "\n",
    "**Shape Rules:**\n",
    "- For 2D: `(m, n) @ (n, p) = (m, p)` - inner dimensions must match\n",
    "- For batches: `(batch, m, n) @ (batch, n, p) = (batch, m, p)`\n",
    "- Broadcasting: `(batch, m, n) @ (n, p) = (batch, m, p)`\n",
    "\n",
    "**Special Cases:**\n",
    "- Vector @ Matrix: `(n,) @ (n, m) = (m,)` - produces a vector\n",
    "- Matrix @ Vector: `(m, n) @ (n,) = (m,)` - produces a vector\n",
    "- Vector @ Vector: `(n,) @ (n,) = scalar` - dot product\n",
    "\n",
    "**Neural Network Applications:**\n",
    "- **Linear layers**: `y = x @ W + b` transforms features\n",
    "- **Deep networks**: Chain multiple matrix multiplications with activations\n",
    "- **Attention**: Uses multiple matrix multiplications (Q@K^T, attention@V)\n",
    "- **Batch processing**: Efficiently process multiple samples simultaneously\n",
    "\n",
    "**Best Practices:**\n",
    "- Use `@` operator for readability: `x @ W` instead of `torch.matmul(x, W)`\n",
    "- Always check shape compatibility before multiplying\n",
    "- Use batch dimensions for efficient parallel processing\n",
    "- Remember: matrix multiplication is NOT commutative (A @ B ≠ B @ A)\n",
    "\n",
    "**Why This Matters:**\n",
    "\n",
    "Matrix multiplication is the computational backbone of deep learning. Every time you:\n",
    "- Pass data through a linear layer\n",
    "- Compute attention in a transformer\n",
    "- Apply a convolutional filter\n",
    "- Generate predictions from a model\n",
    "\n",
    "...you're performing matrix multiplications! Understanding this operation deeply helps you:\n",
    "- Debug shape errors (the most common bug in deep learning)\n",
    "- Design efficient network architectures\n",
    "- Understand computational costs\n",
    "- Implement custom layers and operations\n",
    "\n",
    "Master matrix multiplication, and you've mastered the core computation of neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea2aee6",
   "metadata": {},
   "source": [
    "## Reduction Operations - Aggregating Tensor Data\n",
    "\n",
    "**What are reduction operations?**\n",
    "\n",
    "Reduction operations aggregate tensor data by collapsing one or more dimensions into single values. Common reductions include:\n",
    "- **sum()** - Add all elements together\n",
    "- **mean()** - Compute the average value\n",
    "- **max()** - Find the maximum value\n",
    "- **min()** - Find the minimum value\n",
    "- **prod()** - Multiply all elements together\n",
    "- **std()** - Compute standard deviation\n",
    "\n",
    "These operations can work on the entire tensor or along specific dimensions.\n",
    "\n",
    "**Why do reduction operations matter?**\n",
    "\n",
    "Reductions are essential throughout deep learning:\n",
    "- **Loss computation**: Average loss across a batch of samples\n",
    "- **Normalization**: Compute mean and std for standardization\n",
    "- **Pooling**: Max pooling in CNNs reduces spatial dimensions\n",
    "- **Metrics**: Calculate accuracy, precision, recall across datasets\n",
    "- **Gradient aggregation**: Sum gradients across batch dimensions\n",
    "- **Feature statistics**: Understand data distributions\n",
    "\n",
    "**When are reductions useful?**\n",
    "- When you need a single summary statistic from many values\n",
    "- When aggregating information across batches, sequences, or features\n",
    "- When computing statistics for normalization or monitoring\n",
    "- When reducing spatial dimensions in convolutional networks\n",
    "\n",
    "Let's explore each reduction operation and see how they work across different dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabe1915",
   "metadata": {},
   "source": [
    "### Sum - Adding Elements Together\n",
    "\n",
    "The `sum()` operation adds elements together. It's one of the most common reductions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327eba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a sample tensor\n",
    "x = torch.tensor([\n",
    "    [1.0, 2.0, 3.0, 4.0],\n",
    "    [5.0, 6.0, 7.0, 8.0],\n",
    "    [9.0, 10.0, 11.0, 12.0]\n",
    "])\n",
    "\n",
    "print(\"Original tensor:\")\n",
    "print(x)\n",
    "print(f\"Shape: {x.shape}  # (3, 4)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1414cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum all elements (no dimension specified)\n",
    "total_sum = x.sum()\n",
    "print(\"Sum of all elements:\")\n",
    "print(f\"Result: {total_sum}\")\n",
    "print(f\"Shape: {total_sum.shape}  # Scalar (0D tensor)\")\n",
    "print(f\"Calculation: 1+2+3+...+12 = {total_sum.item()}\")\n",
    "print()\n",
    "\n",
    "# Verify manually\n",
    "manual_sum = 1+2+3+4+5+6+7+8+9+10+11+12\n",
    "print(f\"Manual calculation: {manual_sum}\")\n",
    "print(f\"Match? {total_sum.item() == manual_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum along dimension 0 (sum down columns)\n",
    "sum_dim0 = x.sum(dim=0)\n",
    "print(\"Sum along dimension 0 (collapse rows):\")\n",
    "print(f\"Result: {sum_dim0}\")\n",
    "print(f\"Shape: {sum_dim0.shape}  # (4,) - one value per column\")\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(\"  Column 0: 1 + 5 + 9 = 15\")\n",
    "print(\"  Column 1: 2 + 6 + 10 = 18\")\n",
    "print(\"  Column 2: 3 + 7 + 11 = 21\")\n",
    "print(\"  Column 3: 4 + 8 + 12 = 24\")\n",
    "print()\n",
    "print(\"Use case: Summing feature values across all samples in a batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c22f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum along dimension 1 (sum across columns)\n",
    "sum_dim1 = x.sum(dim=1)\n",
    "print(\"Sum along dimension 1 (collapse columns):\")\n",
    "print(f\"Result: {sum_dim1}\")\n",
    "print(f\"Shape: {sum_dim1.shape}  # (3,) - one value per row\")\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(\"  Row 0: 1 + 2 + 3 + 4 = 10\")\n",
    "print(\"  Row 1: 5 + 6 + 7 + 8 = 26\")\n",
    "print(\"  Row 2: 9 + 10 + 11 + 12 = 42\")\n",
    "print()\n",
    "print(\"Use case: Computing total activation per sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f3fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum with keepdim=True\n",
    "sum_keepdim = x.sum(dim=1, keepdim=True)\n",
    "print(\"Sum with keepdim=True:\")\n",
    "print(f\"Result:\\n{sum_keepdim}\")\n",
    "print(f\"Shape: {sum_keepdim.shape}  # (3, 1) - dimension preserved\")\n",
    "print()\n",
    "print(\"This is useful for broadcasting operations back to the original shape!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f877cc2",
   "metadata": {},
   "source": [
    "### Mean - Computing Averages\n",
    "\n",
    "The `mean()` operation computes the average value. It's essential for normalization and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of all elements\n",
    "mean_all = x.mean()\n",
    "print(\"Mean of all elements:\")\n",
    "print(f\"Result: {mean_all:.2f}\")\n",
    "print(f\"Calculation: sum(78) / count(12) = {mean_all:.2f}\")\n",
    "print()\n",
    "\n",
    "# Verify\n",
    "print(f\"Verification: {x.sum().item()} / {x.numel()} = {x.sum().item() / x.numel():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4922775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean along dimension 0 (average down columns)\n",
    "mean_dim0 = x.mean(dim=0)\n",
    "print(\"Mean along dimension 0:\")\n",
    "print(f\"Result: {mean_dim0}\")\n",
    "print(f\"Shape: {mean_dim0.shape}  # (4,)\")\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(\"  Column 0: (1 + 5 + 9) / 3 = 5.0\")\n",
    "print(\"  Column 1: (2 + 6 + 10) / 3 = 6.0\")\n",
    "print(\"  Column 2: (3 + 7 + 11) / 3 = 7.0\")\n",
    "print(\"  Column 3: (4 + 8 + 12) / 3 = 8.0\")\n",
    "print()\n",
    "print(\"Use case: Computing average feature values across a batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean along dimension 1 (average across columns)\n",
    "mean_dim1 = x.mean(dim=1)\n",
    "print(\"Mean along dimension 1:\")\n",
    "print(f\"Result: {mean_dim1}\")\n",
    "print(f\"Shape: {mean_dim1.shape}  # (3,)\")\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(\"  Row 0: (1 + 2 + 3 + 4) / 4 = 2.5\")\n",
    "print(\"  Row 1: (5 + 6 + 7 + 8) / 4 = 6.5\")\n",
    "print(\"  Row 2: (9 + 10 + 11 + 12) / 4 = 10.5\")\n",
    "print()\n",
    "print(\"Use case: Computing average activation per sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2dd452",
   "metadata": {},
   "source": [
    "### Max - Finding Maximum Values\n",
    "\n",
    "The `max()` operation finds the largest value. When used with a dimension, it returns both the maximum values AND their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ed269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max of all elements\n",
    "max_all = x.max()\n",
    "print(\"Maximum of all elements:\")\n",
    "print(f\"Result: {max_all}\")\n",
    "print(f\"This is the largest value in the entire tensor\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce40678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max along dimension 0\n",
    "# Returns a named tuple with 'values' and 'indices'\n",
    "max_dim0 = x.max(dim=0)\n",
    "print(\"Max along dimension 0:\")\n",
    "print(f\"Values: {max_dim0.values}\")\n",
    "print(f\"Indices: {max_dim0.indices}\")\n",
    "print(f\"Shape: {max_dim0.values.shape}  # (4,)\")\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(\"  Column 0: max(1, 5, 9) = 9 at index 2\")\n",
    "print(\"  Column 1: max(2, 6, 10) = 10 at index 2\")\n",
    "print(\"  Column 2: max(3, 7, 11) = 11 at index 2\")\n",
    "print(\"  Column 3: max(4, 8, 12) = 12 at index 2\")\n",
    "print()\n",
    "print(\"Use case: Max pooling in CNNs, finding peak activations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max along dimension 1\n",
    "max_dim1 = x.max(dim=1)\n",
    "print(\"Max along dimension 1:\")\n",
    "print(f\"Values: {max_dim1.values}\")\n",
    "print(f\"Indices: {max_dim1.indices}\")\n",
    "print(f\"Shape: {max_dim1.values.shape}  # (3,)\")\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(\"  Row 0: max(1, 2, 3, 4) = 4 at index 3\")\n",
    "print(\"  Row 1: max(5, 6, 7, 8) = 8 at index 3\")\n",
    "print(\"  Row 2: max(9, 10, 11, 12) = 12 at index 3\")\n",
    "print()\n",
    "print(\"Use case: Finding the strongest feature per sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60b998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing values and indices separately\n",
    "max_values = x.max(dim=1).values\n",
    "max_indices = x.max(dim=1).indices\n",
    "\n",
    "print(\"Accessing values and indices:\")\n",
    "print(f\"Max values: {max_values}\")\n",
    "print(f\"Max indices: {max_indices}\")\n",
    "print()\n",
    "\n",
    "# You can also unpack them\n",
    "values, indices = x.max(dim=1)\n",
    "print(\"Unpacking:\")\n",
    "print(f\"Values: {values}\")\n",
    "print(f\"Indices: {indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65947192",
   "metadata": {},
   "source": [
    "### Min - Finding Minimum Values\n",
    "\n",
    "The `min()` operation works just like `max()`, but finds the smallest values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bffcad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min of all elements\n",
    "min_all = x.min()\n",
    "print(\"Minimum of all elements:\")\n",
    "print(f\"Result: {min_all}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bbe954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min along dimension 0\n",
    "min_dim0 = x.min(dim=0)\n",
    "print(\"Min along dimension 0:\")\n",
    "print(f\"Values: {min_dim0.values}\")\n",
    "print(f\"Indices: {min_dim0.indices}\")\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(\"  Column 0: min(1, 5, 9) = 1 at index 0\")\n",
    "print(\"  Column 1: min(2, 6, 10) = 2 at index 0\")\n",
    "print(\"  Column 2: min(3, 7, 11) = 3 at index 0\")\n",
    "print(\"  Column 3: min(4, 8, 12) = 4 at index 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ccfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min along dimension 1\n",
    "min_dim1 = x.min(dim=1)\n",
    "print(\"Min along dimension 1:\")\n",
    "print(f\"Values: {min_dim1.values}\")\n",
    "print(f\"Indices: {min_dim1.indices}\")\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(\"  Row 0: min(1, 2, 3, 4) = 1 at index 0\")\n",
    "print(\"  Row 1: min(5, 6, 7, 8) = 5 at index 0\")\n",
    "print(\"  Row 2: min(9, 10, 11, 12) = 9 at index 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3152a4",
   "metadata": {},
   "source": [
    "### Practical Examples - Reductions in Deep Learning\n",
    "\n",
    "Let's see how reduction operations are used in real deep learning scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625b7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Computing batch loss\n",
    "# Simulate predictions and targets for a batch\n",
    "batch_size = 32\n",
    "predictions = torch.randn(batch_size, 10)  # 32 samples, 10 classes\n",
    "targets = torch.randint(0, 10, (batch_size,))  # True class labels\n",
    "\n",
    "# Compute loss per sample (simplified)\n",
    "# In reality, you'd use nn.CrossEntropyLoss()\n",
    "losses_per_sample = torch.randn(batch_size).abs()  # Simulated losses\n",
    "\n",
    "print(\"Batch loss computation:\")\n",
    "print(f\"Losses per sample shape: {losses_per_sample.shape}\")\n",
    "print(f\"Sample losses: {losses_per_sample[:5]}...\")\n",
    "print()\n",
    "\n",
    "# Average loss across the batch\n",
    "batch_loss = losses_per_sample.mean()\n",
    "print(f\"Average batch loss: {batch_loss:.4f}\")\n",
    "print()\n",
    "print(\"This single number is what we use to compute gradients!\")\n",
    "print(\"Use case: Every training step computes mean loss across the batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea72e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Feature normalization (standardization)\n",
    "# Normalize features to have mean=0 and std=1\n",
    "data = torch.tensor([\n",
    "    [10.0, 100.0, 1000.0],\n",
    "    [15.0, 150.0, 1500.0],\n",
    "    [12.0, 120.0, 1200.0],\n",
    "    [18.0, 180.0, 1800.0]\n",
    "])\n",
    "\n",
    "print(\"Original data (different scales):\")\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "# Compute mean and std per feature (across samples)\n",
    "feature_mean = data.mean(dim=0, keepdim=True)\n",
    "feature_std = data.std(dim=0, keepdim=True)\n",
    "\n",
    "print(f\"Feature means: {feature_mean}\")\n",
    "print(f\"Feature stds: {feature_std}\")\n",
    "print()\n",
    "\n",
    "# Normalize\n",
    "normalized = (data - feature_mean) / feature_std\n",
    "\n",
    "print(\"Normalized data:\")\n",
    "print(normalized)\n",
    "print()\n",
    "print(\"Verify normalization:\")\n",
    "print(f\"New means: {normalized.mean(dim=0)}\")\n",
    "print(f\"New stds: {normalized.std(dim=0)}\")\n",
    "print()\n",
    "print(\"Use case: Preprocessing data before training neural networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebbf1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Finding best predictions (classification)\n",
    "# Simulate model outputs (logits) for 5 samples, 3 classes\n",
    "logits = torch.tensor([\n",
    "    [2.1, 0.5, -1.0],  # Sample 1: highest score for class 0\n",
    "    [-0.5, 3.2, 1.0],  # Sample 2: highest score for class 1\n",
    "    [0.1, -1.0, 2.8],  # Sample 3: highest score for class 2\n",
    "    [1.5, 1.6, 1.4],   # Sample 4: highest score for class 1\n",
    "    [-2.0, -1.5, 0.5]  # Sample 5: highest score for class 2\n",
    "])\n",
    "\n",
    "print(\"Model logits (raw outputs):\")\n",
    "print(logits)\n",
    "print(f\"Shape: {logits.shape}  # (5 samples, 3 classes)\")\n",
    "print()\n",
    "\n",
    "# Find the predicted class (highest logit) for each sample\n",
    "max_logits, predicted_classes = logits.max(dim=1)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(f\"Max logit values: {max_logits}\")\n",
    "print(f\"Predicted classes: {predicted_classes}\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "for i, pred in enumerate(predicted_classes):\n",
    "    print(f\"  Sample {i}: Predicted class {pred.item()} (logit: {max_logits[i]:.2f})\")\n",
    "print()\n",
    "print(\"Use case: Converting model outputs to class predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a893fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Global average pooling\n",
    "# Common in CNNs to reduce spatial dimensions\n",
    "# Simulate feature maps: (batch=2, channels=64, height=8, width=8)\n",
    "feature_maps = torch.randn(2, 64, 8, 8)\n",
    "\n",
    "print(\"Feature maps from CNN:\")\n",
    "print(f\"Shape: {feature_maps.shape}  # (batch, channels, height, width)\")\n",
    "print()\n",
    "\n",
    "# Global average pooling: average over spatial dimensions (height, width)\n",
    "# We want to reduce (2, 64, 8, 8) to (2, 64)\n",
    "pooled = feature_maps.mean(dim=(2, 3))  # Average over dimensions 2 and 3\n",
    "\n",
    "print(\"After global average pooling:\")\n",
    "print(f\"Shape: {pooled.shape}  # (batch, channels)\")\n",
    "print()\n",
    "print(\"What happened:\")\n",
    "print(\"  - Each 8×8 spatial feature map was reduced to a single value\")\n",
    "print(\"  - We averaged all 64 spatial positions for each channel\")\n",
    "print(\"  - Result: One value per channel per sample\")\n",
    "print()\n",
    "print(\"Use case: Reducing spatial dimensions before classification layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Computing accuracy\n",
    "# Compare predictions to ground truth\n",
    "predictions = torch.tensor([0, 1, 2, 1, 0, 2, 1, 0])  # Predicted classes\n",
    "targets = torch.tensor([0, 1, 2, 2, 0, 2, 1, 1])      # True classes\n",
    "\n",
    "print(\"Predictions:\", predictions.tolist())\n",
    "print(\"Targets:    \", targets.tolist())\n",
    "print()\n",
    "\n",
    "# Check which predictions are correct\n",
    "correct = (predictions == targets)\n",
    "print(f\"Correct predictions: {correct}\")\n",
    "print(f\"Type: {correct.dtype}  # Boolean tensor\")\n",
    "print()\n",
    "\n",
    "# Count correct predictions\n",
    "num_correct = correct.sum()\n",
    "print(f\"Number correct: {num_correct.item()} out of {len(predictions)}\")\n",
    "print()\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = correct.float().mean()  # Convert to float first\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print()\n",
    "print(\"Use case: Evaluating model performance on validation/test sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e067e",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**Common Reduction Operations:**\n",
    "- **`sum()`** - Add all elements together\n",
    "- **`mean()`** - Compute average value\n",
    "- **`max()`** - Find maximum value (returns values and indices when used with dim)\n",
    "- **`min()`** - Find minimum value (returns values and indices when used with dim)\n",
    "- **`std()`** - Compute standard deviation\n",
    "- **`var()`** - Compute variance\n",
    "- **`prod()`** - Multiply all elements together\n",
    "\n",
    "**Usage Patterns:**\n",
    "- **No dimension**: Reduce entire tensor to a single value\n",
    "- **With dimension**: Reduce along specific dimension(s)\n",
    "- **keepdim=True**: Preserve dimension with size 1 for broadcasting\n",
    "- **Multiple dimensions**: `tensor.mean(dim=(0, 1))` reduces multiple dimensions\n",
    "\n",
    "**When to Use Reductions:**\n",
    "1. **Loss computation**: Average loss across batch → `losses.mean()`\n",
    "2. **Normalization**: Compute statistics → `data.mean(dim=0)`, `data.std(dim=0)`\n",
    "3. **Pooling**: Reduce spatial dimensions → `features.max(dim=2)`\n",
    "4. **Metrics**: Calculate accuracy, precision → `(pred == target).float().mean()`\n",
    "5. **Feature aggregation**: Summarize information → `embeddings.mean(dim=1)`\n",
    "6. **Finding extremes**: Get max/min activations → `activations.max(dim=-1)`\n",
    "\n",
    "**Important Notes:**\n",
    "- `max()` and `min()` with dimension return **both values and indices**\n",
    "- Use `.values` and `.indices` to access them separately\n",
    "- For boolean tensors, `sum()` counts True values\n",
    "- `mean()` requires float tensors (convert with `.float()` if needed)\n",
    "- Multiple dimensions can be reduced at once: `dim=(0, 1, 2)`\n",
    "\n",
    "**Deep Learning Applications:**\n",
    "- Every training step uses `loss.mean()` to get a scalar for backpropagation\n",
    "- Batch normalization uses `mean()` and `std()` across the batch dimension\n",
    "- Max pooling uses `max()` to downsample feature maps\n",
    "- Global average pooling uses `mean()` to reduce spatial dimensions\n",
    "- Accuracy computation uses `sum()` to count correct predictions\n",
    "\n",
    "Reduction operations are fundamental tools for aggregating and summarizing tensor data. Master these operations, and you'll be able to compute statistics, normalize data, and implement custom loss functions and metrics with ease!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e0c7a",
   "metadata": {},
   "source": [
    "## Argmax - Finding Indices of Maximum Values\n",
    "\n",
    "**What is argmax?**\n",
    "\n",
    "While `max()` returns the maximum *value* in a tensor, `argmax()` returns the *index* (position) of the maximum value. The name comes from \"argument of the maximum\" - it tells you *where* the maximum occurs, not what the maximum is.\n",
    "\n",
    "**Why does argmax matter?**\n",
    "\n",
    "Argmax is **crucial for classification tasks** in deep learning:\n",
    "- **Making predictions**: Convert model probabilities to class labels\n",
    "- **Accuracy computation**: Compare predicted classes to true labels\n",
    "- **Top-k predictions**: Find the most likely classes\n",
    "- **Attention mechanisms**: Identify which positions have highest attention scores\n",
    "\n",
    "**Real-world example**: If your model outputs `[0.1, 0.7, 0.2]` for classes [cat, dog, bird], `argmax()` returns `1` (the index of dog), which is your prediction!\n",
    "\n",
    "**PyTorch provides:**\n",
    "- `torch.argmax()` - Find index of maximum value\n",
    "- `torch.argmin()` - Find index of minimum value (less common)\n",
    "\n",
    "Let's explore how argmax works and why it's essential for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443851b9",
   "metadata": {},
   "source": [
    "### Basic Argmax - Finding Maximum Indices\n",
    "\n",
    "Let's start with simple examples to understand what argmax returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfde55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Simple 1D tensor\n",
    "scores = torch.tensor([0.2, 0.8, 0.5, 0.3, 0.9, 0.1])\n",
    "\n",
    "print(\"Scores:\")\n",
    "print(scores)\n",
    "print()\n",
    "\n",
    "# Find the maximum value\n",
    "max_value = scores.max()\n",
    "print(f\"Maximum value: {max_value}\")\n",
    "print()\n",
    "\n",
    "# Find the INDEX of the maximum value\n",
    "max_index = torch.argmax(scores)\n",
    "print(f\"Index of maximum value (argmax): {max_index}\")\n",
    "print(f\"Value at that index: {scores[max_index]}\")\n",
    "print()\n",
    "print(\"Explanation: The maximum value 0.9 is at index 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argmax with different values\n",
    "values = torch.tensor([10.0, 5.0, 20.0, 15.0, 3.0])\n",
    "\n",
    "print(f\"Values: {values}\")\n",
    "print(f\"Argmax: {torch.argmax(values)}\")\n",
    "print(f\"Maximum value: {values[torch.argmax(values)]}\")\n",
    "print()\n",
    "\n",
    "# Negative values work too\n",
    "negative_values = torch.tensor([-5.0, -2.0, -8.0, -1.0, -10.0])\n",
    "print(f\"Negative values: {negative_values}\")\n",
    "print(f\"Argmax: {torch.argmax(negative_values)}\")\n",
    "print(f\"Maximum value: {negative_values[torch.argmax(negative_values)]}\")\n",
    "print()\n",
    "print(\"Note: -1.0 is the maximum (least negative) value at index 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if there are multiple maximum values?\n",
    "# Argmax returns the FIRST occurrence\n",
    "duplicates = torch.tensor([3.0, 7.0, 7.0, 2.0, 7.0, 1.0])\n",
    "\n",
    "print(f\"Values with duplicates: {duplicates}\")\n",
    "print(f\"Argmax: {torch.argmax(duplicates)}\")\n",
    "print()\n",
    "print(\"Explanation: There are three 7.0 values (indices 1, 2, 4)\")\n",
    "print(\"Argmax returns 1 - the FIRST occurrence of the maximum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62baae9",
   "metadata": {},
   "source": [
    "### Argmax Across Dimensions - The Power of dim Parameter\n",
    "\n",
    "Just like reduction operations, argmax can work across specific dimensions. This is where argmax becomes incredibly useful for batch processing in neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5766157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D tensor: Think of this as 4 samples, each with 3 class scores\n",
    "# Each row represents one sample's predictions for 3 classes\n",
    "class_scores = torch.tensor([\n",
    "    [0.2, 0.7, 0.1],  # Sample 0: highest score for class 1\n",
    "    [0.8, 0.1, 0.1],  # Sample 1: highest score for class 0\n",
    "    [0.1, 0.2, 0.7],  # Sample 2: highest score for class 2\n",
    "    [0.3, 0.5, 0.2]   # Sample 3: highest score for class 1\n",
    "])\n",
    "\n",
    "print(\"Class scores (4 samples, 3 classes):\")\n",
    "print(class_scores)\n",
    "print(f\"Shape: {class_scores.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5686d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argmax without dimension - finds the global maximum\n",
    "global_argmax = torch.argmax(class_scores)\n",
    "print(\"Argmax without dimension (flattened):\")\n",
    "print(f\"Result: {global_argmax}\")\n",
    "print()\n",
    "print(\"This treats the tensor as flattened: [0.2, 0.7, 0.1, 0.8, ...]\")\n",
    "print(f\"Index 3 corresponds to class_scores[1, 0] = {class_scores.flatten()[global_argmax]}\")\n",
    "print()\n",
    "print(\"Usually not what we want for classification!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c4e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argmax along dimension 1 (across classes for each sample)\n",
    "# This is THE most common use case in classification!\n",
    "predictions = torch.argmax(class_scores, dim=1)\n",
    "\n",
    "print(\"Argmax along dim=1 (predicted class for each sample):\")\n",
    "print(f\"Predictions: {predictions}\")\n",
    "print(f\"Shape: {predictions.shape}  # (4,) - one prediction per sample\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(f\"  Sample 0: Predicted class {predictions[0]} (score: {class_scores[0, predictions[0]]:.1f})\")\n",
    "print(f\"  Sample 1: Predicted class {predictions[1]} (score: {class_scores[1, predictions[1]]:.1f})\")\n",
    "print(f\"  Sample 2: Predicted class {predictions[2]} (score: {class_scores[2, predictions[2]]:.1f})\")\n",
    "print(f\"  Sample 3: Predicted class {predictions[3]} (score: {class_scores[3, predictions[3]]:.1f})\")\n",
    "print()\n",
    "print(\"This is how neural networks make classification predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argmax along dimension 0 (across samples for each class)\n",
    "# Less common, but useful for finding which sample has highest score per class\n",
    "best_samples = torch.argmax(class_scores, dim=0)\n",
    "\n",
    "print(\"Argmax along dim=0 (which sample has highest score for each class):\")\n",
    "print(f\"Result: {best_samples}\")\n",
    "print(f\"Shape: {best_samples.shape}  # (3,) - one sample index per class\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(f\"  Class 0: Sample {best_samples[0]} has highest score ({class_scores[best_samples[0], 0]:.1f})\")\n",
    "print(f\"  Class 1: Sample {best_samples[1]} has highest score ({class_scores[best_samples[1], 1]:.1f})\")\n",
    "print(f\"  Class 2: Sample {best_samples[2]} has highest score ({class_scores[best_samples[2], 2]:.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e0bfd8",
   "metadata": {},
   "source": [
    "### Argmax with 3D Tensors - Batch Processing\n",
    "\n",
    "In real neural networks, you often work with 3D tensors for sequence classification or other structured predictions. Let's see how argmax handles these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf332ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D tensor: (batch_size=2, sequence_length=4, num_classes=3)\n",
    "# Example: Classifying each token in a sequence\n",
    "sequence_scores = torch.tensor([\n",
    "    # Batch 0: 4 tokens, each with 3 class scores\n",
    "    [[0.1, 0.7, 0.2],  # Token 0\n",
    "     [0.8, 0.1, 0.1],  # Token 1\n",
    "     [0.2, 0.3, 0.5],  # Token 2\n",
    "     [0.6, 0.3, 0.1]], # Token 3\n",
    "    \n",
    "    # Batch 1: 4 tokens, each with 3 class scores\n",
    "    [[0.3, 0.5, 0.2],  # Token 0\n",
    "     [0.1, 0.1, 0.8],  # Token 1\n",
    "     [0.7, 0.2, 0.1],  # Token 2\n",
    "     [0.2, 0.6, 0.2]]  # Token 3\n",
    "])\n",
    "\n",
    "print(\"Sequence scores (2 batches, 4 tokens, 3 classes):\")\n",
    "print(f\"Shape: {sequence_scores.shape}\")\n",
    "print(sequence_scores)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argmax along last dimension (dim=-1 or dim=2)\n",
    "# Predict class for each token in each sequence\n",
    "token_predictions = torch.argmax(sequence_scores, dim=-1)\n",
    "\n",
    "print(\"Argmax along dim=-1 (predicted class for each token):\")\n",
    "print(token_predictions)\n",
    "print(f\"Shape: {token_predictions.shape}  # (2, 4) - predictions for each token\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\"Batch 0 predictions:\", token_predictions[0].tolist())\n",
    "print(\"  Token 0: class 1, Token 1: class 0, Token 2: class 2, Token 3: class 0\")\n",
    "print()\n",
    "print(\"Batch 1 predictions:\", token_predictions[1].tolist())\n",
    "print(\"  Token 0: class 1, Token 1: class 2, Token 2: class 0, Token 3: class 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argmax along sequence dimension (dim=1)\n",
    "# Find which token has highest score for each class\n",
    "best_tokens = torch.argmax(sequence_scores, dim=1)\n",
    "\n",
    "print(\"Argmax along dim=1 (which token has highest score for each class):\")\n",
    "print(best_tokens)\n",
    "print(f\"Shape: {best_tokens.shape}  # (2, 3) - best token per class per batch\")\n",
    "print()\n",
    "print(\"Batch 0: For each class, which token has the highest score?\")\n",
    "print(f\"  Class 0: Token {best_tokens[0, 0]}\")\n",
    "print(f\"  Class 1: Token {best_tokens[0, 1]}\")\n",
    "print(f\"  Class 2: Token {best_tokens[0, 2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ca0cf",
   "metadata": {},
   "source": [
    "### Real-World Classification Example\n",
    "\n",
    "Let's simulate a complete classification workflow, like what happens in image classification or text classification models. This shows how argmax connects model outputs to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38060bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a batch of model outputs (logits before softmax)\n",
    "# Scenario: Classifying images into 5 categories\n",
    "batch_size = 8\n",
    "num_classes = 5\n",
    "\n",
    "# Raw model outputs (logits)\n",
    "logits = torch.randn(batch_size, num_classes)\n",
    "\n",
    "print(\"Raw model outputs (logits):\")\n",
    "print(logits)\n",
    "print(f\"Shape: {logits.shape}  # (8 samples, 5 classes)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to probabilities using softmax\n",
    "import torch.nn.functional as F\n",
    "\n",
    "probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "print(\"Probabilities (after softmax):\")\n",
    "print(probabilities)\n",
    "print()\n",
    "print(\"Each row sums to 1.0:\")\n",
    "print(f\"Row sums: {probabilities.sum(dim=1)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6815fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using argmax\n",
    "predictions = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "print(\"Predicted classes (using argmax):\")\n",
    "print(predictions)\n",
    "print(f\"Shape: {predictions.shape}  # (8,) - one prediction per sample\")\n",
    "print()\n",
    "\n",
    "# Show predictions with confidence scores\n",
    "print(\"Predictions with confidence:\")\n",
    "for i in range(batch_size):\n",
    "    pred_class = predictions[i].item()\n",
    "    confidence = probabilities[i, pred_class].item()\n",
    "    print(f\"  Sample {i}: Predicted class {pred_class} (confidence: {confidence:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: Argmax on logits gives same result as argmax on probabilities!\n",
    "# Softmax preserves the order of values\n",
    "\n",
    "predictions_from_logits = torch.argmax(logits, dim=1)\n",
    "predictions_from_probs = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "print(\"Predictions from logits:\", predictions_from_logits)\n",
    "print(\"Predictions from probabilities:\", predictions_from_probs)\n",
    "print()\n",
    "print(f\"Are they the same? {torch.equal(predictions_from_logits, predictions_from_probs)}\")\n",
    "print()\n",
    "print(\"Tip: You can apply argmax directly to logits without softmax!\")\n",
    "print(\"Only compute softmax when you need actual probability values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3cb9bb",
   "metadata": {},
   "source": [
    "### Computing Accuracy with Argmax\n",
    "\n",
    "One of the most common uses of argmax is computing classification accuracy. Let's see how to compare predictions to ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e321e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate model outputs and true labels\n",
    "batch_size = 10\n",
    "num_classes = 4\n",
    "\n",
    "# Model outputs (logits)\n",
    "model_outputs = torch.randn(batch_size, num_classes)\n",
    "\n",
    "# True labels (ground truth)\n",
    "true_labels = torch.tensor([0, 1, 2, 3, 0, 1, 2, 3, 0, 1])\n",
    "\n",
    "print(f\"Model outputs shape: {model_outputs.shape}\")\n",
    "print(f\"True labels: {true_labels}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c56670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions using argmax\n",
    "predictions = torch.argmax(model_outputs, dim=1)\n",
    "\n",
    "print(f\"Predictions: {predictions}\")\n",
    "print(f\"True labels: {true_labels}\")\n",
    "print()\n",
    "\n",
    "# Compare predictions to true labels\n",
    "correct = (predictions == true_labels)\n",
    "print(f\"Correct predictions (boolean): {correct}\")\n",
    "print()\n",
    "\n",
    "# Count correct predictions\n",
    "num_correct = correct.sum().item()\n",
    "print(f\"Number of correct predictions: {num_correct} out of {batch_size}\")\n",
    "print()\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = correct.float().mean().item()\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b00f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete accuracy function\n",
    "def compute_accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute classification accuracy.\n",
    "    \n",
    "    Args:\n",
    "        outputs: Model outputs (logits or probabilities), shape (batch_size, num_classes)\n",
    "        labels: True class labels, shape (batch_size,)\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: Fraction of correct predictions (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    correct = (predictions == labels).float()\n",
    "    accuracy = correct.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Test the function\n",
    "test_outputs = torch.randn(100, 10)  # 100 samples, 10 classes\n",
    "test_labels = torch.randint(0, 10, (100,))  # Random labels\n",
    "\n",
    "acc = compute_accuracy(test_outputs, test_labels)\n",
    "print(f\"Test accuracy: {acc:.2%}\")\n",
    "print()\n",
    "print(\"This function is used in every classification training loop!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0237c534",
   "metadata": {},
   "source": [
    "### Beyond Argmax - Top-K Predictions\n",
    "\n",
    "Sometimes you want the top K predictions, not just the best one. PyTorch provides `torch.topk()` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Top-3 predictions for image classification\n",
    "# Imagine classifying an image that could be multiple things\n",
    "class_names = ['cat', 'dog', 'bird', 'fish', 'horse', 'deer', 'frog', 'car', 'plane', 'ship']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Model output for one image\n",
    "logits = torch.randn(num_classes)\n",
    "probabilities = F.softmax(logits, dim=0)\n",
    "\n",
    "print(\"Class probabilities:\")\n",
    "for i, (name, prob) in enumerate(zip(class_names, probabilities)):\n",
    "    print(f\"  {name:8s}: {prob:.3f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779bf682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top-3 predictions\n",
    "k = 3\n",
    "top_probs, top_indices = torch.topk(probabilities, k)\n",
    "\n",
    "print(f\"Top-{k} predictions:\")\n",
    "for i in range(k):\n",
    "    class_idx = top_indices[i].item()\n",
    "    prob = top_probs[i].item()\n",
    "    print(f\"  {i+1}. {class_names[class_idx]:8s}: {prob:.2%}\")\n",
    "print()\n",
    "\n",
    "# Note: argmax gives the same result as topk with k=1\n",
    "argmax_result = torch.argmax(probabilities)\n",
    "topk_result = torch.topk(probabilities, 1).indices[0]\n",
    "print(f\"Argmax: {argmax_result}\")\n",
    "print(f\"Top-1:  {topk_result}\")\n",
    "print(f\"Same? {argmax_result == topk_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8470277",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**What Argmax Does:**\n",
    "- Returns the **index** of the maximum value, not the value itself\n",
    "- `torch.argmax(tensor)` - Find global maximum index\n",
    "- `torch.argmax(tensor, dim=d)` - Find maximum index along dimension d\n",
    "- If multiple maxima exist, returns the **first** occurrence\n",
    "\n",
    "**Common Usage Patterns:**\n",
    "- **Classification predictions**: `predictions = torch.argmax(logits, dim=1)`\n",
    "- **Accuracy computation**: `correct = (predictions == labels).float().mean()`\n",
    "- **Sequence labeling**: `token_classes = torch.argmax(scores, dim=-1)`\n",
    "- **Attention mechanisms**: Find positions with highest attention\n",
    "\n",
    "**Important Facts:**\n",
    "- Argmax on logits = Argmax on probabilities (softmax preserves order)\n",
    "- Only compute softmax when you need actual probability values\n",
    "- For top-K predictions, use `torch.topk()` instead\n",
    "- Argmax returns indices as **long** (int64) tensors\n",
    "\n",
    "**Dimension Guidelines:**\n",
    "- **No dim**: Returns single index (flattened tensor)\n",
    "- **dim=1** (most common): Predict class for each sample in batch\n",
    "- **dim=-1**: Operate on last dimension (works for any number of dimensions)\n",
    "- **dim=0**: Find best sample for each class (less common)\n",
    "\n",
    "**Real-World Applications:**\n",
    "1. **Image Classification**: Convert model outputs to predicted class labels\n",
    "2. **Text Classification**: Determine sentiment, topic, or intent\n",
    "3. **Object Detection**: Select highest confidence bounding boxes\n",
    "4. **Sequence Labeling**: Predict tags for each token (NER, POS tagging)\n",
    "5. **Recommendation Systems**: Select top-ranked items\n",
    "6. **Reinforcement Learning**: Choose action with highest Q-value\n",
    "\n",
    "**Complete Classification Workflow:**\n",
    "```python\n",
    "# 1. Model forward pass\n",
    "logits = model(inputs)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "# 2. Get predictions\n",
    "predictions = torch.argmax(logits, dim=1)  # Shape: (batch_size,)\n",
    "\n",
    "# 3. Compute accuracy\n",
    "accuracy = (predictions == labels).float().mean()\n",
    "```\n",
    "\n",
    "**Argmax vs Max:**\n",
    "- `max()` returns the maximum **value**\n",
    "- `argmax()` returns the **index** of the maximum value\n",
    "- When `max()` is used with `dim`, it returns both values and indices\n",
    "- `argmax()` is cleaner when you only need indices\n",
    "\n",
    "Argmax is one of the most frequently used operations in deep learning classification tasks. Every time you see a model make a prediction, argmax is working behind the scenes to convert continuous scores into discrete class labels. Master argmax, and you'll understand how neural networks make decisions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fec2a3",
   "metadata": {},
   "source": [
    "## requires_grad - Enabling Gradient Tracking\n",
    "\n",
    "**What is requires_grad?**\n",
    "\n",
    "`requires_grad` is a boolean flag on tensors that tells PyTorch whether to track operations on that tensor for automatic differentiation. When `requires_grad=True`, PyTorch builds a computational graph of all operations performed on the tensor, enabling gradient computation.\n",
    "\n",
    "**Why does requires_grad matter?**\n",
    "\n",
    "Gradient tracking is the foundation of neural network training:\n",
    "- **Training**: Only tensors with `requires_grad=True` get gradients computed\n",
    "- **Parameters**: Model weights and biases have `requires_grad=True` by default\n",
    "- **Efficiency**: Input data typically has `requires_grad=False` to save memory\n",
    "- **Control**: You can selectively enable/disable gradient tracking\n",
    "\n",
    "**Key principle**: Gradients flow backward through the computational graph, but only for tensors that require gradients. This allows PyTorch to optimize memory and computation.\n",
    "\n",
    "Let's explore how to use `requires_grad` and understand its impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8370f3",
   "metadata": {},
   "source": [
    "### Creating Tensors with requires_grad\n",
    "\n",
    "You can set `requires_grad` when creating a tensor or change it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04ea4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor WITHOUT gradient tracking (default)\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(\"Tensor without gradient tracking:\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"requires_grad: {x.requires_grad}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor WITH gradient tracking\n",
    "y = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(\"Tensor with gradient tracking:\")\n",
    "print(f\"y = {y}\")\n",
    "print(f\"requires_grad: {y.requires_grad}\")\n",
    "print()\n",
    "print(\"PyTorch will now track all operations on y!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d6b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enabling requires_grad on existing tensors\n",
    "z = torch.tensor([4.0, 5.0, 6.0])\n",
    "print(f\"Initially: requires_grad = {z.requires_grad}\")\n",
    "\n",
    "# Method 1: Using requires_grad_()\n",
    "z.requires_grad_(True)\n",
    "print(f\"After requires_grad_(True): {z.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# Method 2: Using requires_grad property (creates a new tensor)\n",
    "w = torch.tensor([7.0, 8.0, 9.0])\n",
    "w_tracked = w.requires_grad_(True)\n",
    "print(f\"w_tracked requires_grad: {w_tracked.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e3dc1",
   "metadata": {},
   "source": [
    "### How requires_grad Affects Operations\n",
    "\n",
    "When you perform operations on tensors with `requires_grad=True`, the result also has `requires_grad=True` (gradient tracking propagates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a38f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operations with requires_grad tensors\n",
    "a = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "b = torch.tensor([4.0, 5.0], requires_grad=True)\n",
    "\n",
    "# Perform operations\n",
    "c = a + b\n",
    "d = c * 2\n",
    "e = d.sum()\n",
    "\n",
    "print(\"Gradient tracking propagates through operations:\")\n",
    "print(f\"a.requires_grad: {a.requires_grad}\")\n",
    "print(f\"b.requires_grad: {b.requires_grad}\")\n",
    "print(f\"c.requires_grad: {c.requires_grad}  # a + b\")\n",
    "print(f\"d.requires_grad: {d.requires_grad}  # c * 2\")\n",
    "print(f\"e.requires_grad: {e.requires_grad}  # d.sum()\")\n",
    "print()\n",
    "print(\"All intermediate results track gradients!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf1a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed operations: requires_grad and non-requires_grad\n",
    "x_grad = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "x_no_grad = torch.tensor([3.0, 4.0], requires_grad=False)\n",
    "\n",
    "# Operation between them\n",
    "result = x_grad + x_no_grad\n",
    "\n",
    "print(\"Mixed operations:\")\n",
    "print(f\"x_grad.requires_grad: {x_grad.requires_grad}\")\n",
    "print(f\"x_no_grad.requires_grad: {x_no_grad.requires_grad}\")\n",
    "print(f\"result.requires_grad: {result.requires_grad}\")\n",
    "print()\n",
    "print(\"If ANY input requires gradients, the output requires gradients!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d90404",
   "metadata": {},
   "source": [
    "### Practical Usage in Neural Networks\n",
    "\n",
    "In practice, you rarely set `requires_grad` manually. PyTorch handles it automatically:\n",
    "- **Model parameters** (weights, biases) have `requires_grad=True` by default\n",
    "- **Input data** has `requires_grad=False` by default\n",
    "- **Loss values** inherit `requires_grad=True` from parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create a simple linear layer\n",
    "layer = nn.Linear(3, 2)\n",
    "\n",
    "print(\"Model parameters automatically have requires_grad=True:\")\n",
    "print(f\"Weight requires_grad: {layer.weight.requires_grad}\")\n",
    "print(f\"Bias requires_grad: {layer.bias.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# Input data (no gradient tracking needed)\n",
    "input_data = torch.randn(5, 3)\n",
    "print(f\"Input data requires_grad: {input_data.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# Forward pass\n",
    "output = layer(input_data)\n",
    "print(f\"Output requires_grad: {output.requires_grad}\")\n",
    "print(\"Output tracks gradients because it depends on parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a5b46a",
   "metadata": {},
   "source": [
    "### Disabling Gradient Tracking\n",
    "\n",
    "Sometimes you want to temporarily disable gradient tracking (e.g., during inference). PyTorch provides several ways to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165beb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: torch.no_grad() context manager\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "# With gradients\n",
    "y = x * 2\n",
    "print(f\"With gradients: y.requires_grad = {y.requires_grad}\")\n",
    "\n",
    "# Without gradients\n",
    "with torch.no_grad():\n",
    "    z = x * 2\n",
    "    print(f\"Inside no_grad(): z.requires_grad = {z.requires_grad}\")\n",
    "\n",
    "print()\n",
    "print(\"Use torch.no_grad() during inference to save memory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: .detach() - creates a new tensor without gradient tracking\n",
    "a = torch.tensor([3.0, 4.0], requires_grad=True)\n",
    "b = a * 2\n",
    "\n",
    "print(f\"b.requires_grad: {b.requires_grad}\")\n",
    "\n",
    "# Detach from computational graph\n",
    "c = b.detach()\n",
    "print(f\"c.requires_grad: {c.requires_grad}\")\n",
    "print()\n",
    "print(\"c shares data with b but doesn't track gradients\")\n",
    "print(f\"Same values? {torch.equal(b, c)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbead2a1",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**What is requires_grad?**\n",
    "- A boolean flag that enables gradient tracking on tensors\n",
    "- When `True`, PyTorch builds a computational graph for backpropagation\n",
    "- When `False`, operations are not tracked (saves memory and computation)\n",
    "\n",
    "**Setting requires_grad:**\n",
    "- At creation: `torch.tensor([1, 2], requires_grad=True)`\n",
    "- On existing tensor: `tensor.requires_grad_(True)`\n",
    "- Model parameters have it enabled by default\n",
    "- Input data typically has it disabled\n",
    "\n",
    "**Gradient Propagation:**\n",
    "- Operations on `requires_grad=True` tensors produce `requires_grad=True` results\n",
    "- If ANY input requires gradients, the output requires gradients\n",
    "- This allows gradients to flow backward through the entire computation\n",
    "\n",
    "**Disabling Gradients:**\n",
    "- `torch.no_grad()` context: Temporarily disable tracking\n",
    "- `.detach()` method: Create a new tensor without gradient tracking\n",
    "- Use during inference to save memory\n",
    "\n",
    "**When to Use:**\n",
    "- **Enable** for parameters you want to train (weights, biases)\n",
    "- **Disable** for input data and during inference\n",
    "- **Temporarily disable** when computing metrics or doing inference\n",
    "\n",
    "Understanding `requires_grad` is essential for controlling what PyTorch tracks and optimizes. It's the first step in PyTorch's automatic differentiation system!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61762d",
   "metadata": {},
   "source": [
    "## Computation Graph - How PyTorch Tracks Operations\n",
    "\n",
    "**What is a computation graph?**\n",
    "\n",
    "A computation graph is a directed acyclic graph (DAG) that PyTorch automatically builds to track all operations performed on tensors with `requires_grad=True`. Each node in the graph represents:\n",
    "- **Tensors** (data): The inputs, intermediate values, and outputs\n",
    "- **Operations** (functions): The mathematical operations applied (add, multiply, etc.)\n",
    "\n",
    "The graph records the sequence of operations, allowing PyTorch to compute gradients by traversing the graph backward (backpropagation).\n",
    "\n",
    "**Why does the computation graph matter?**\n",
    "\n",
    "The computation graph is the foundation of automatic differentiation:\n",
    "- **Automatic gradients**: PyTorch uses the graph to compute derivatives automatically\n",
    "- **Backpropagation**: Gradients flow backward through the graph from output to inputs\n",
    "- **Dynamic**: The graph is built on-the-fly during the forward pass\n",
    "- **Memory efficient**: PyTorch can free intermediate values after backward pass\n",
    "\n",
    "**Connection to deep learning**: Every forward pass through a neural network builds a computation graph. When you call `.backward()`, PyTorch traverses this graph to compute gradients for all parameters.\n",
    "\n",
    "Let's visualize how computation graphs work with simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a7e86",
   "metadata": {},
   "source": [
    "### Building a Simple Computation Graph\n",
    "\n",
    "Let's trace how PyTorch builds a graph for a simple computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3865a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create input tensors with gradient tracking\n",
    "a = torch.tensor([2.0], requires_grad=True)\n",
    "b = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "print(\"Input tensors:\")\n",
    "print(f\"a = {a.item()}, requires_grad = {a.requires_grad}\")\n",
    "print(f\"b = {b.item()}, requires_grad = {b.requires_grad}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbb6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform operations (PyTorch builds the graph automatically)\n",
    "c = a + b      # c = 2 + 3 = 5\n",
    "d = c * 2      # d = 5 * 2 = 10\n",
    "e = d ** 2     # e = 10^2 = 100\n",
    "\n",
    "print(\"Computation steps:\")\n",
    "print(f\"c = a + b = {c.item()}\")\n",
    "print(f\"d = c * 2 = {d.item()}\")\n",
    "print(f\"e = d ** 2 = {e.item()}\")\n",
    "print()\n",
    "\n",
    "print(\"Computation graph (conceptual):\")\n",
    "print(\"  a (2.0) \\\\\")\n",
    "print(\"            + --> c (5.0) --> * 2 --> d (10.0) --> ^2 --> e (100.0)\")\n",
    "print(\"  b (3.0) /\")\n",
    "print()\n",
    "print(\"Each operation creates a node in the graph!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4850a5",
   "metadata": {},
   "source": [
    "### The grad_fn Attribute - Tracking Operations\n",
    "\n",
    "Every tensor created by an operation has a `grad_fn` attribute that points to the function that created it. This is how PyTorch builds the computation graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine grad_fn for each tensor\n",
    "print(\"grad_fn attributes (how each tensor was created):\")\n",
    "print(f\"a.grad_fn: {a.grad_fn}  # None - it's a leaf tensor (input)\")\n",
    "print(f\"b.grad_fn: {b.grad_fn}  # None - it's a leaf tensor (input)\")\n",
    "print(f\"c.grad_fn: {c.grad_fn}  # AddBackward - created by addition\")\n",
    "print(f\"d.grad_fn: {d.grad_fn}  # MulBackward - created by multiplication\")\n",
    "print(f\"e.grad_fn: {e.grad_fn}  # PowBackward - created by power operation\")\n",
    "print()\n",
    "print(\"The grad_fn tells PyTorch how to compute gradients backward!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ada91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaf tensors vs non-leaf tensors\n",
    "print(\"Leaf tensors (inputs):\")\n",
    "print(f\"a.is_leaf: {a.is_leaf}  # True - created by user\")\n",
    "print(f\"b.is_leaf: {b.is_leaf}  # True - created by user\")\n",
    "print()\n",
    "\n",
    "print(\"Non-leaf tensors (intermediate results):\")\n",
    "print(f\"c.is_leaf: {c.is_leaf}  # False - created by operation\")\n",
    "print(f\"d.is_leaf: {d.is_leaf}  # False - created by operation\")\n",
    "print(f\"e.is_leaf: {e.is_leaf}  # False - created by operation\")\n",
    "print()\n",
    "print(\"Leaf tensors are the starting points of the graph!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca46cb",
   "metadata": {},
   "source": [
    "### More Complex Computation Graph\n",
    "\n",
    "Let's see a more realistic example with multiple operations and branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c3f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex computation\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "w = torch.tensor([0.5, 0.3, 0.2], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Simulate a simple neural network computation\n",
    "# y = sum(x * w) + b\n",
    "product = x * w           # Element-wise multiplication\n",
    "sum_product = product.sum()  # Sum all elements\n",
    "y = sum_product + b       # Add bias\n",
    "\n",
    "print(\"Neural network-like computation:\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"w = {w}\")\n",
    "print(f\"b = {b.item()}\")\n",
    "print()\n",
    "print(f\"product = x * w = {product}\")\n",
    "print(f\"sum_product = sum(product) = {sum_product.item()}\")\n",
    "print(f\"y = sum_product + b = {y.item()}\")\n",
    "print()\n",
    "\n",
    "print(\"Computation graph:\")\n",
    "print(\"  x \\\\\")\n",
    "print(\"      * --> product --> sum --> sum_product \\\\\")\n",
    "print(\"  w /                                         + --> y\")\n",
    "print(\"                                      b ------/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cee418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the graph structure\n",
    "print(\"Graph structure through grad_fn:\")\n",
    "print(f\"y.grad_fn: {y.grad_fn}\")\n",
    "print(f\"sum_product.grad_fn: {sum_product.grad_fn}\")\n",
    "print(f\"product.grad_fn: {product.grad_fn}\")\n",
    "print()\n",
    "\n",
    "# The graph connects operations\n",
    "print(\"Following the graph backward from y:\")\n",
    "print(f\"y was created by: {y.grad_fn}\")\n",
    "print(f\"  which has inputs from: {y.grad_fn.next_functions}\")\n",
    "print()\n",
    "print(\"PyTorch uses this structure to compute gradients!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f35ba",
   "metadata": {},
   "source": [
    "### Dynamic Computation Graphs\n",
    "\n",
    "PyTorch uses **dynamic computation graphs** (also called define-by-run). This means the graph is built during the forward pass and can change every time!\n",
    "\n",
    "**Benefits of dynamic graphs:**\n",
    "- **Flexibility**: Different graph structure for each input\n",
    "- **Debugging**: Use standard Python debugging tools\n",
    "- **Control flow**: Use if/else, loops naturally\n",
    "- **Intuitive**: The graph matches your Python code exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bbc3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Graph changes based on input\n",
    "def dynamic_computation(x, use_square=True):\n",
    "    \"\"\"Computation graph changes based on condition\"\"\"\n",
    "    y = x * 2\n",
    "    \n",
    "    if use_square:\n",
    "        # Graph includes square operation\n",
    "        z = y ** 2\n",
    "    else:\n",
    "        # Graph includes cube operation\n",
    "        z = y ** 3\n",
    "    \n",
    "    return z\n",
    "\n",
    "# Test with different conditions\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "result1 = dynamic_computation(x, use_square=True)\n",
    "print(f\"With square: {result1.item()}\")\n",
    "print(f\"grad_fn: {result1.grad_fn}\")\n",
    "print()\n",
    "\n",
    "result2 = dynamic_computation(x, use_square=False)\n",
    "print(f\"With cube: {result2.item()}\")\n",
    "print(f\"grad_fn: {result2.grad_fn}\")\n",
    "print()\n",
    "print(\"Different graph structure for different inputs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf594897",
   "metadata": {},
   "source": [
    "### Computation Graph Lifecycle\n",
    "\n",
    "Understanding when graphs are created and destroyed is important for memory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph is created during forward pass\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y = x * 2\n",
    "z = y.sum()\n",
    "\n",
    "print(\"After forward pass:\")\n",
    "print(f\"z = {z.item()}\")\n",
    "print(f\"z.grad_fn exists: {z.grad_fn is not None}\")\n",
    "print(\"Graph is built and ready for backward pass\")\n",
    "print()\n",
    "\n",
    "# Compute gradients (backward pass)\n",
    "z.backward()\n",
    "\n",
    "print(\"After backward pass:\")\n",
    "print(f\"x.grad = {x.grad}\")\n",
    "print()\n",
    "print(\"Graph is freed after backward() to save memory!\")\n",
    "print(\"(Unless you set retain_graph=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c1d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to backward twice fails (graph is freed)\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "# First backward - works\n",
    "y.backward()\n",
    "print(f\"First backward: x.grad = {x.grad}\")\n",
    "print()\n",
    "\n",
    "# Second backward - would fail\n",
    "# Uncomment to see the error:\n",
    "# y.backward()  # RuntimeError: Trying to backward through the graph a second time\n",
    "\n",
    "print(\"Can't backward twice - graph is freed after first backward!\")\n",
    "print()\n",
    "\n",
    "# Solution: retain_graph=True\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward(retain_graph=True)\n",
    "print(\"With retain_graph=True, you can backward multiple times\")\n",
    "print(\"(But this uses more memory!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4275ebbb",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**What is a Computation Graph?**\n",
    "- A directed acyclic graph (DAG) tracking all operations on tensors\n",
    "- Nodes represent tensors and operations\n",
    "- Edges represent data flow from inputs to outputs\n",
    "- Built automatically during the forward pass\n",
    "\n",
    "**Key Attributes:**\n",
    "- **`grad_fn`**: Points to the function that created a tensor\n",
    "- **`is_leaf`**: True for input tensors, False for intermediate results\n",
    "- **`next_functions`**: Links to previous operations in the graph\n",
    "\n",
    "**Dynamic Graphs:**\n",
    "- Built on-the-fly during execution (define-by-run)\n",
    "- Can change structure based on input or conditions\n",
    "- Allows natural Python control flow (if/else, loops)\n",
    "- Makes debugging easier\n",
    "\n",
    "**Graph Lifecycle:**\n",
    "1. **Forward pass**: Graph is built as operations execute\n",
    "2. **Backward pass**: Gradients computed by traversing graph backward\n",
    "3. **After backward**: Graph is freed to save memory (unless `retain_graph=True`)\n",
    "\n",
    "**Why This Matters:**\n",
    "- The computation graph is how PyTorch implements automatic differentiation\n",
    "- Understanding it helps you debug gradient issues\n",
    "- It explains why you can't backward twice without `retain_graph=True`\n",
    "- It's the foundation for training neural networks\n",
    "\n",
    "**Connection to Training:**\n",
    "Every time you do a forward pass through a neural network, PyTorch builds a computation graph. When you call `loss.backward()`, PyTorch traverses this graph to compute gradients for all parameters. This is the magic that makes training neural networks possible!\n",
    "\n",
    "The computation graph is PyTorch's secret weapon - it makes automatic differentiation both powerful and intuitive!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5649a7",
   "metadata": {},
   "source": [
    "## Autograd - Automatic Differentiation\n",
    "\n",
    "**What is autograd?**\n",
    "\n",
    "Autograd (automatic differentiation) is PyTorch's system for automatically computing derivatives. Given a computation graph, autograd can compute the gradient (derivative) of any output with respect to any input, using the chain rule from calculus.\n",
    "\n",
    "**Why does autograd matter?**\n",
    "\n",
    "Autograd is the engine that powers neural network training:\n",
    "- **Training**: Computes gradients needed to update model parameters\n",
    "- **Backpropagation**: Implements the backpropagation algorithm automatically\n",
    "- **Chain rule**: Handles complex compositions of functions effortlessly\n",
    "- **Efficiency**: Computes gradients in one backward pass\n",
    "\n",
    "Without autograd, you'd have to manually derive and code the gradient for every model architecture. Autograd does this automatically, making deep learning practical!\n",
    "\n",
    "**Connection to deep learning**: Every time you train a neural network, autograd computes how to adjust each parameter to reduce the loss. It's the mathematical foundation of learning.\n",
    "\n",
    "Let's see autograd in action with simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39413ea",
   "metadata": {},
   "source": [
    "### Computing Simple Derivatives\n",
    "\n",
    "Let's start with a simple function and compute its derivative using autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49670208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Simple function: y = x^2\n",
    "# Derivative: dy/dx = 2x\n",
    "\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "print(\"Function: y = x^2\")\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"y = {y.item()}\")\n",
    "print()\n",
    "\n",
    "# Compute gradient\n",
    "y.backward()\n",
    "\n",
    "print(\"Gradient (derivative):\")\n",
    "print(f\"dy/dx = {x.grad.item()}\")\n",
    "print()\n",
    "print(\"Manual calculation: dy/dx = 2x = 2 * 3 = 6\")\n",
    "print(f\"Autograd computed: {x.grad.item()}\")\n",
    "print(\"Perfect match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c47093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example: y = 3x^2 + 2x + 1\n",
    "# Derivative: dy/dx = 6x + 2\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = 3 * x**2 + 2 * x + 1\n",
    "\n",
    "print(\"Function: y = 3x^2 + 2x + 1\")\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"y = {y.item()}\")\n",
    "print()\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"Gradient:\")\n",
    "print(f\"dy/dx = {x.grad.item()}\")\n",
    "print()\n",
    "print(\"Manual calculation: dy/dx = 6x + 2 = 6*2 + 2 = 14\")\n",
    "print(f\"Autograd computed: {x.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c22f303",
   "metadata": {},
   "source": [
    "### The Chain Rule in Action\n",
    "\n",
    "Autograd automatically applies the chain rule to compute gradients through composed functions. This is essential for deep neural networks, which are compositions of many functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97afcad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composed function: z = (x^2 + 1)^3\n",
    "# Using chain rule: dz/dx = 3(x^2 + 1)^2 * 2x = 6x(x^2 + 1)^2\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# Build computation step by step\n",
    "u = x ** 2      # u = x^2\n",
    "v = u + 1       # v = u + 1 = x^2 + 1\n",
    "z = v ** 3      # z = v^3 = (x^2 + 1)^3\n",
    "\n",
    "print(\"Composed function: z = (x^2 + 1)^3\")\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"u = x^2 = {u.item()}\")\n",
    "print(f\"v = u + 1 = {v.item()}\")\n",
    "print(f\"z = v^3 = {z.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient using autograd\n",
    "z.backward()\n",
    "\n",
    "print(\"Gradient computed by autograd:\")\n",
    "print(f\"dz/dx = {x.grad.item()}\")\n",
    "print()\n",
    "\n",
    "# Manual calculation using chain rule\n",
    "# dz/dx = dz/dv * dv/du * du/dx\n",
    "# dz/dv = 3v^2 = 3(5)^2 = 75\n",
    "# dv/du = 1\n",
    "# du/dx = 2x = 4\n",
    "# dz/dx = 75 * 1 * 4 = 300\n",
    "\n",
    "manual_gradient = 6 * x.item() * (x.item()**2 + 1)**2\n",
    "print(f\"Manual calculation: dz/dx = 6x(x^2 + 1)^2 = {manual_gradient}\")\n",
    "print(f\"Autograd computed: {x.grad.item()}\")\n",
    "print()\n",
    "print(\"Autograd applied the chain rule automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52699768",
   "metadata": {},
   "source": [
    "### Gradients with Multiple Inputs\n",
    "\n",
    "Autograd can compute partial derivatives with respect to multiple inputs simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function with two inputs: z = x^2 + 3xy + y^2\n",
    "# Partial derivatives:\n",
    "# dz/dx = 2x + 3y\n",
    "# dz/dy = 3x + 2y\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "z = x**2 + 3*x*y + y**2\n",
    "\n",
    "print(\"Function: z = x^2 + 3xy + y^2\")\n",
    "print(f\"x = {x.item()}, y = {y.item()}\")\n",
    "print(f\"z = {z.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcad868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "print(\"Gradients:\")\n",
    "print(f\"dz/dx = {x.grad.item()}\")\n",
    "print(f\"dz/dy = {y.grad.item()}\")\n",
    "print()\n",
    "\n",
    "# Manual calculation\n",
    "dz_dx_manual = 2 * x.item() + 3 * y.item()  # 2*2 + 3*3 = 13\n",
    "dz_dy_manual = 3 * x.item() + 2 * y.item()  # 3*2 + 2*3 = 12\n",
    "\n",
    "print(\"Manual calculation:\")\n",
    "print(f\"dz/dx = 2x + 3y = {dz_dx_manual}\")\n",
    "print(f\"dz/dy = 3x + 2y = {dz_dy_manual}\")\n",
    "print()\n",
    "print(\"Autograd computed both partial derivatives correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d01e1",
   "metadata": {},
   "source": [
    "### Gradients of Vector-Valued Functions\n",
    "\n",
    "When working with vectors, autograd computes gradients element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4078a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector input: x = [x1, x2, x3]\n",
    "# Function: y = sum(x^2) = x1^2 + x2^2 + x3^2\n",
    "# Gradients: dy/dx1 = 2x1, dy/dx2 = 2x2, dy/dx3 = 2x3\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = (x ** 2).sum()\n",
    "\n",
    "print(\"Vector function: y = sum(x^2)\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = {y.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient\n",
    "y.backward()\n",
    "\n",
    "print(\"Gradient vector:\")\n",
    "print(f\"dy/dx = {x.grad}\")\n",
    "print()\n",
    "print(\"Manual calculation: dy/dx = 2x = [2*1, 2*2, 2*3] = [2, 4, 6]\")\n",
    "print(f\"Autograd computed: {x.grad.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192645aa",
   "metadata": {},
   "source": [
    "### Autograd in Neural Networks\n",
    "\n",
    "Let's see how autograd works in a simple neural network scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple neural network: y = w * x + b\n",
    "# This is a single neuron (linear layer)\n",
    "\n",
    "# Input (data - no gradient needed)\n",
    "x = torch.tensor([2.0, 3.0, 4.0])\n",
    "\n",
    "# Parameters (need gradients for training)\n",
    "w = torch.tensor([0.5, 0.3, 0.2], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y = (w * x).sum() + b\n",
    "\n",
    "print(\"Simple neuron: y = sum(w * x) + b\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"w = {w}\")\n",
    "print(f\"b = {b.item()}\")\n",
    "print(f\"y = {y.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea6b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "y.backward()\n",
    "\n",
    "print(\"Gradients (how to adjust parameters):\")\n",
    "print(f\"dy/dw = {w.grad}  # Gradient with respect to weights\")\n",
    "print(f\"dy/db = {b.grad}  # Gradient with respect to bias\")\n",
    "print()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- dy/dw tells us how changing each weight affects the output\")\n",
    "print(\"- dy/db tells us how changing the bias affects the output\")\n",
    "print(\"- These gradients are used to update parameters during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c48b323",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**What is Autograd?**\n",
    "- PyTorch's automatic differentiation system\n",
    "- Computes derivatives automatically using the chain rule\n",
    "- Works by traversing the computation graph backward\n",
    "- The foundation of neural network training\n",
    "\n",
    "**How It Works:**\n",
    "1. **Forward pass**: Build computation graph, compute output\n",
    "2. **Backward pass**: Call `.backward()` to compute gradients\n",
    "3. **Chain rule**: Autograd applies chain rule automatically\n",
    "4. **Gradients**: Stored in `.grad` attribute of leaf tensors\n",
    "\n",
    "**Key Capabilities:**\n",
    "- Compute derivatives of any differentiable function\n",
    "- Handle multiple inputs (partial derivatives)\n",
    "- Work with vectors and matrices\n",
    "- Apply chain rule through arbitrarily deep compositions\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Training**: Gradients tell us how to adjust parameters to reduce loss\n",
    "- **Backpropagation**: Autograd implements backpropagation automatically\n",
    "- **Flexibility**: Works with any differentiable operations\n",
    "- **Efficiency**: Computes all gradients in one backward pass\n",
    "\n",
    "**Connection to Deep Learning:**\n",
    "Every neural network training step uses autograd:\n",
    "1. Forward pass: Compute predictions and loss\n",
    "2. Backward pass: Autograd computes gradients\n",
    "3. Update: Use gradients to adjust parameters\n",
    "\n",
    "Without autograd, implementing and training neural networks would be incredibly tedious. Autograd makes deep learning practical by handling all the calculus automatically. It's one of PyTorch's most powerful features!\n",
    "\n",
    "**Remember**: Autograd doesn't just compute derivatives - it computes them efficiently and correctly, even for complex, deeply nested functions. This is the magic that makes training modern neural networks possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e3752f",
   "metadata": {},
   "source": [
    "## Gradient Attributes - Accessing Computed Gradients\n",
    "\n",
    "**What are gradient attributes?**\n",
    "\n",
    "After calling `.backward()`, PyTorch stores the computed gradients in the `.grad` attribute of each tensor that has `requires_grad=True`. The `.grad` attribute is a tensor with the same shape as the original tensor, containing the gradient (derivative) at each element.\n",
    "\n",
    "**Why do gradient attributes matter?**\n",
    "\n",
    "The `.grad` attribute is how you access the gradients computed by autograd:\n",
    "- **Training**: Optimizers use `.grad` to update parameters\n",
    "- **Inspection**: You can examine gradients for debugging\n",
    "- **Custom updates**: Implement custom optimization algorithms\n",
    "- **Gradient clipping**: Modify gradients to prevent exploding gradients\n",
    "\n",
    "**Key principle**: Gradients accumulate in `.grad` by default. This means if you call `.backward()` multiple times without clearing gradients, they add up!\n",
    "\n",
    "Let's explore how to work with gradient attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f85a9",
   "metadata": {},
   "source": [
    "### Accessing Gradients with .grad\n",
    "\n",
    "After computing gradients with `.backward()`, access them via the `.grad` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with gradient tracking\n",
    "x = torch.tensor([2.0, 3.0, 4.0], requires_grad=True)\n",
    "\n",
    "print(\"Before backward():\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"x.grad = {x.grad}  # None - no gradients computed yet\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a function and its gradient\n",
    "y = (x ** 2).sum()  # y = x1^2 + x2^2 + x3^2\n",
    "\n",
    "print(f\"y = sum(x^2) = {y.item()}\")\n",
    "print()\n",
    "\n",
    "# Compute gradients\n",
    "y.backward()\n",
    "\n",
    "print(\"After backward():\")\n",
    "print(f\"x.grad = {x.grad}  # dy/dx = 2x\")\n",
    "print()\n",
    "print(\"Verification: dy/dx = 2x = [2*2, 2*3, 2*4] = [4, 6, 8]\")\n",
    "print(f\"Computed: {x.grad.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient shape matches tensor shape\n",
    "w = torch.randn(3, 4, requires_grad=True)\n",
    "loss = w.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(\"Gradient shape matches tensor shape:\")\n",
    "print(f\"w.shape = {w.shape}\")\n",
    "print(f\"w.grad.shape = {w.grad.shape}\")\n",
    "print()\n",
    "print(\"Each element of w has a corresponding gradient in w.grad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663aff3",
   "metadata": {},
   "source": [
    "### Gradient Accumulation - Gradients Add Up!\n",
    "\n",
    "**Important**: By default, gradients accumulate in `.grad`. If you call `.backward()` multiple times, the new gradients are **added** to the existing ones. This is useful for some advanced techniques but can cause bugs if you forget to zero gradients between training steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc35c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate gradient accumulation\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "# First computation\n",
    "y1 = (x ** 2).sum()\n",
    "y1.backward()\n",
    "\n",
    "print(\"After first backward():\")\n",
    "print(f\"x.grad = {x.grad}  # dy1/dx = 2x = [2, 4]\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e593d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second computation WITHOUT zeroing gradients\n",
    "y2 = (x ** 3).sum()\n",
    "y2.backward()\n",
    "\n",
    "print(\"After second backward() WITHOUT zeroing:\")\n",
    "print(f\"x.grad = {x.grad}\")\n",
    "print()\n",
    "print(\"Expected if we zeroed: dy2/dx = 3x^2 = [3, 12]\")\n",
    "print(\"But we got: [2, 4] + [3, 12] = [5, 16]\")\n",
    "print()\n",
    "print(\"Gradients accumulated! This is usually NOT what you want in training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca32db0",
   "metadata": {},
   "source": [
    "### Zeroing Gradients - Essential for Training\n",
    "\n",
    "To prevent gradient accumulation, you must zero gradients before each backward pass. This is a critical step in the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4073a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper way: Zero gradients between backward passes\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "# First computation\n",
    "y1 = (x ** 2).sum()\n",
    "y1.backward()\n",
    "print(f\"After first backward: x.grad = {x.grad}\")\n",
    "print()\n",
    "\n",
    "# Zero gradients\n",
    "x.grad.zero_()\n",
    "print(f\"After zeroing: x.grad = {x.grad}\")\n",
    "print()\n",
    "\n",
    "# Second computation\n",
    "y2 = (x ** 3).sum()\n",
    "y2.backward()\n",
    "print(f\"After second backward: x.grad = {x.grad}\")\n",
    "print()\n",
    "print(\"Now we get the correct gradient: dy2/dx = 3x^2 = [3, 12]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc4be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Set grad to None\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "print(f\"After backward: x.grad = {x.grad}\")\n",
    "\n",
    "# Set to None (slightly more efficient than zero_())\n",
    "x.grad = None\n",
    "print(f\"After setting to None: x.grad = {x.grad}\")\n",
    "print()\n",
    "print(\"Setting to None is slightly faster than zero_()\")\n",
    "print(\"PyTorch optimizers support both methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0acd0",
   "metadata": {},
   "source": [
    "### Gradients for Non-Leaf Tensors\n",
    "\n",
    "By default, PyTorch only retains gradients for leaf tensors (inputs). Intermediate results don't keep their gradients to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaf vs non-leaf tensors\n",
    "x = torch.tensor([2.0], requires_grad=True)  # Leaf tensor\n",
    "y = x * 2  # Non-leaf tensor\n",
    "z = y ** 2  # Non-leaf tensor\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(\"Gradients:\")\n",
    "print(f\"x.grad = {x.grad}  # Leaf tensor - gradient retained\")\n",
    "print(f\"y.grad = {y.grad}  # Non-leaf tensor - gradient NOT retained\")\n",
    "print(f\"z.grad = {z.grad}  # Non-leaf tensor - gradient NOT retained\")\n",
    "print()\n",
    "print(\"Only leaf tensors keep gradients by default!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746fc639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retaining gradients for non-leaf tensors\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x * 2\n",
    "y.retain_grad()  # Explicitly retain gradient for y\n",
    "z = y ** 2\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(\"With retain_grad():\")\n",
    "print(f\"x.grad = {x.grad}\")\n",
    "print(f\"y.grad = {y.grad}  # Now retained!\")\n",
    "print(f\"z.grad = {z.grad}  # Still None (didn't call retain_grad on z)\")\n",
    "print()\n",
    "print(\"Use retain_grad() when you need intermediate gradients for debugging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66fae3e",
   "metadata": {},
   "source": [
    "### Practical Example - Training Loop Pattern\n",
    "\n",
    "Here's how gradient attributes are used in a typical training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce84b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Simple model\n",
    "model = nn.Linear(3, 1)\n",
    "\n",
    "# Sample data\n",
    "x = torch.randn(5, 3)  # 5 samples, 3 features\n",
    "y_true = torch.randn(5, 1)  # 5 target values\n",
    "\n",
    "print(\"Initial parameter gradients:\")\n",
    "print(f\"model.weight.grad = {model.weight.grad}\")\n",
    "print(f\"model.bias.grad = {model.bias.grad}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e6195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "y_pred = model(x)\n",
    "loss = ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(\"After backward():\")\n",
    "print(f\"model.weight.grad shape: {model.weight.grad.shape}\")\n",
    "print(f\"model.bias.grad shape: {model.bias.grad.shape}\")\n",
    "print()\n",
    "print(\"Gradients computed! Ready to update parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect gradient values\n",
    "print(\"Gradient values:\")\n",
    "print(f\"Weight gradients:\\n{model.weight.grad}\")\n",
    "print()\n",
    "print(f\"Bias gradient: {model.bias.grad}\")\n",
    "print()\n",
    "\n",
    "# Check gradient statistics\n",
    "print(\"Gradient statistics:\")\n",
    "print(f\"Weight grad mean: {model.weight.grad.mean().item():.6f}\")\n",
    "print(f\"Weight grad std: {model.weight.grad.std().item():.6f}\")\n",
    "print(f\"Weight grad max: {model.weight.grad.max().item():.6f}\")\n",
    "print(f\"Weight grad min: {model.weight.grad.min().item():.6f}\")\n",
    "print()\n",
    "print(\"Inspecting gradients helps debug training issues!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868405cc",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**The .grad Attribute:**\n",
    "- Stores computed gradients after `.backward()`\n",
    "- Has the same shape as the tensor\n",
    "- Initially `None` before any backward pass\n",
    "- Only retained for leaf tensors by default\n",
    "\n",
    "**Gradient Accumulation:**\n",
    "- Gradients **accumulate** (add up) by default\n",
    "- Calling `.backward()` multiple times adds to existing gradients\n",
    "- This is usually NOT what you want in training\n",
    "- Must zero gradients between training steps\n",
    "\n",
    "**Zeroing Gradients:**\n",
    "- Use `tensor.grad.zero_()` to zero gradients in-place\n",
    "- Or set `tensor.grad = None` (slightly more efficient)\n",
    "- Optimizers provide `.zero_grad()` to zero all parameter gradients\n",
    "- **Critical**: Always zero gradients before each training step!\n",
    "\n",
    "**Leaf vs Non-Leaf Tensors:**\n",
    "- **Leaf tensors**: Created by user, gradients retained\n",
    "- **Non-leaf tensors**: Created by operations, gradients NOT retained (saves memory)\n",
    "- Use `.retain_grad()` to keep gradients for non-leaf tensors (debugging)\n",
    "\n",
    "**Practical Usage:**\n",
    "```python\n",
    "# Typical training step\n",
    "optimizer.zero_grad()  # Zero gradients\n",
    "output = model(input)  # Forward pass\n",
    "loss = criterion(output, target)  # Compute loss\n",
    "loss.backward()  # Compute gradients (stored in .grad)\n",
    "optimizer.step()  # Update parameters using .grad\n",
    "```\n",
    "\n",
    "**Common Pitfalls:**\n",
    "- Forgetting to zero gradients → gradients accumulate → wrong updates\n",
    "- Trying to access `.grad` before calling `.backward()` → None\n",
    "- Expecting gradients for non-leaf tensors → None (use `.retain_grad()`)\n",
    "\n",
    "**Why This Matters:**\n",
    "The `.grad` attribute is the bridge between autograd and optimization. Autograd computes gradients and stores them in `.grad`. Optimizers read `.grad` to update parameters. Understanding gradient attributes is essential for:\n",
    "- Implementing training loops correctly\n",
    "- Debugging gradient issues\n",
    "- Implementing custom optimization algorithms\n",
    "- Understanding how PyTorch training works under the hood\n",
    "\n",
    "Master gradient attributes, and you'll understand the complete training pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d81202",
   "metadata": {},
   "source": [
    "## Backward Pass - Computing Gradients via Backpropagation\n",
    "\n",
    "**What is the backward pass?**\n",
    "\n",
    "The backward pass is the process of computing gradients by traversing the computation graph from output to inputs. In PyTorch, you trigger the backward pass by calling `.backward()` on a scalar tensor (usually the loss). PyTorch then uses backpropagation to compute gradients for all tensors that have `requires_grad=True`.\n",
    "\n",
    "**Why does the backward pass matter?**\n",
    "\n",
    "The backward pass is how neural networks learn:\n",
    "- **Training**: Computes gradients needed to update parameters\n",
    "- **Backpropagation**: Implements the backpropagation algorithm efficiently\n",
    "- **Chain rule**: Applies the chain rule automatically through the entire network\n",
    "- **Optimization**: Provides gradients to optimizers for parameter updates\n",
    "\n",
    "**Connection to deep learning**: Every training step has two phases:\n",
    "1. **Forward pass**: Compute predictions and loss\n",
    "2. **Backward pass**: Compute gradients via backpropagation\n",
    "\n",
    "The backward pass is where the \"learning\" happens - it tells us how to adjust parameters to reduce the loss.\n",
    "\n",
    "Let's explore how the backward pass works in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05121d1",
   "metadata": {},
   "source": [
    "### Basic Backward Pass\n",
    "\n",
    "The simplest backward pass: compute a scalar output and call `.backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e15f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create tensors with gradient tracking\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "w = torch.tensor([0.5, 0.3], requires_grad=True)\n",
    "\n",
    "# Forward pass: compute output\n",
    "y = (x * w).sum()\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"w = {w}\")\n",
    "print(f\"y = sum(x * w) = {y.item()}\")\n",
    "print()\n",
    "\n",
    "print(\"Before backward pass:\")\n",
    "print(f\"x.grad = {x.grad}\")\n",
    "print(f\"w.grad = {w.grad}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81058268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass: compute gradients\n",
    "y.backward()\n",
    "\n",
    "print(\"After backward pass:\")\n",
    "print(f\"x.grad = {x.grad}  # dy/dx\")\n",
    "print(f\"w.grad = {w.grad}  # dy/dw\")\n",
    "print()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- dy/dx tells us how y changes when x changes\")\n",
    "print(\"- dy/dw tells us how y changes when w changes\")\n",
    "print(\"- These gradients guide parameter updates during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717f6124",
   "metadata": {},
   "source": [
    "### Backward Pass Through Multiple Operations\n",
    "\n",
    "The backward pass automatically handles complex computations with many operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f422ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex computation\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Multiple operations\n",
    "a = x ** 2          # Square\n",
    "b = a * 3           # Multiply by 3\n",
    "c = b + 10          # Add 10\n",
    "d = c.mean()        # Take mean\n",
    "\n",
    "print(\"Forward pass through multiple operations:\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"a = x^2 = {a}\")\n",
    "print(f\"b = a * 3 = {b}\")\n",
    "print(f\"c = b + 10 = {c}\")\n",
    "print(f\"d = mean(c) = {d.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96352fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass computes dd/dx through all operations\n",
    "d.backward()\n",
    "\n",
    "print(\"After backward pass:\")\n",
    "print(f\"x.grad = {x.grad}\")\n",
    "print()\n",
    "\n",
    "print(\"The gradient dd/dx was computed by:\")\n",
    "print(\"1. Traversing the graph backward: d -> c -> b -> a -> x\")\n",
    "print(\"2. Applying chain rule at each step\")\n",
    "print(\"3. Accumulating gradients\")\n",
    "print()\n",
    "print(\"All of this happened automatically with one .backward() call!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6716e54",
   "metadata": {},
   "source": [
    "### Backward Pass in a Simple Neural Network\n",
    "\n",
    "Let's see how the backward pass works in a neural network context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create a simple two-layer network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3, 4),  # Input: 3 features, Output: 4 features\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 1)   # Output: 1 value\n",
    ")\n",
    "\n",
    "# Sample input and target\n",
    "x = torch.randn(1, 3)  # 1 sample, 3 features\n",
    "y_true = torch.tensor([[1.0]])  # Target value\n",
    "\n",
    "print(\"Neural network setup:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Target shape: {y_true.shape}\")\n",
    "print(f\"Model: {model}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e356085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "y_pred = model(x)\n",
    "loss = ((y_pred - y_true) ** 2).mean()  # MSE loss\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "print(f\"Prediction: {y_pred.item():.4f}\")\n",
    "print(f\"Target: {y_true.item():.4f}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Before backward pass:\")\n",
    "print(f\"First layer weight grad: {model[0].weight.grad}\")\n",
    "print(f\"Last layer weight grad: {model[2].weight.grad}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(\"After backward pass:\")\n",
    "print(f\"First layer weight grad shape: {model[0].weight.grad.shape}\")\n",
    "print(f\"First layer bias grad shape: {model[0].bias.grad.shape}\")\n",
    "print(f\"Last layer weight grad shape: {model[2].weight.grad.shape}\")\n",
    "print(f\"Last layer bias grad shape: {model[2].bias.grad.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Gradients computed for ALL parameters!\")\n",
    "print(\"The backward pass:\")\n",
    "print(\"1. Started at the loss\")\n",
    "print(\"2. Flowed backward through the last linear layer\")\n",
    "print(\"3. Flowed through the ReLU\")\n",
    "print(\"4. Flowed through the first linear layer\")\n",
    "print(\"5. Computed gradients for all weights and biases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e43ea0",
   "metadata": {},
   "source": [
    "### Understanding Gradient Flow\n",
    "\n",
    "Let's visualize how gradients flow backward through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4a6160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example to show gradient flow\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "w1 = torch.tensor([3.0], requires_grad=True)\n",
    "w2 = torch.tensor([4.0], requires_grad=True)\n",
    "\n",
    "# Forward: y = w2 * (w1 * x)\n",
    "h = w1 * x  # Hidden value\n",
    "y = w2 * h  # Output\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"w1 = {w1.item()}\")\n",
    "print(f\"w2 = {w2.item()}\")\n",
    "print(f\"h = w1 * x = {h.item()}\")\n",
    "print(f\"y = w2 * h = {y.item()}\")\n",
    "print()\n",
    "\n",
    "print(\"Computation graph:\")\n",
    "print(\"  x (2) \\\\\")\n",
    "print(\"          * --> h (6) --> * --> y (24)\")\n",
    "print(\"  w1 (3) /          w2 (4)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "y.backward()\n",
    "\n",
    "print(\"Backward pass (gradient flow):\")\n",
    "print()\n",
    "print(\"Starting from y, gradients flow backward:\")\n",
    "print(f\"dy/dy = 1.0 (starting point)\")\n",
    "print()\n",
    "print(f\"dy/dw2 = h = {w2.grad.item()}  # Gradient for w2\")\n",
    "print(f\"dy/dh = w2 = {w2.item()}  # Gradient flowing to h\")\n",
    "print()\n",
    "print(f\"dy/dw1 = (dy/dh) * x = {w1.grad.item()}  # Gradient for w1\")\n",
    "print(f\"dy/dx = (dy/dh) * w1 = {x.grad.item()}  # Gradient for x\")\n",
    "print()\n",
    "print(\"Gradients computed using chain rule!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f62b6",
   "metadata": {},
   "source": [
    "### Important: Backward Pass Clears the Graph\n",
    "\n",
    "By default, the computation graph is freed after `.backward()` to save memory. This means you can't call `.backward()` twice on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f9d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate graph clearing\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "# First backward - works\n",
    "y.backward()\n",
    "print(f\"First backward: x.grad = {x.grad}\")\n",
    "print()\n",
    "\n",
    "# Try second backward - would fail\n",
    "# Uncomment to see the error:\n",
    "# y.backward()  # RuntimeError: Trying to backward through the graph a second time\n",
    "\n",
    "print(\"Can't backward twice - graph is freed!\")\n",
    "print(\"This saves memory but means you need to recompute if you want another backward pass.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ca3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: retain_graph=True\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "# First backward with retain_graph\n",
    "y.backward(retain_graph=True)\n",
    "print(f\"First backward: x.grad = {x.grad}\")\n",
    "\n",
    "# Zero gradients\n",
    "x.grad.zero_()\n",
    "\n",
    "# Second backward - now works!\n",
    "y.backward()\n",
    "print(f\"Second backward: x.grad = {x.grad}\")\n",
    "print()\n",
    "print(\"With retain_graph=True, you can backward multiple times\")\n",
    "print(\"(But this uses more memory!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50266a6",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**What is the Backward Pass?**\n",
    "- The process of computing gradients via backpropagation\n",
    "- Triggered by calling `.backward()` on a scalar tensor\n",
    "- Traverses the computation graph from output to inputs\n",
    "- Applies the chain rule automatically at each step\n",
    "\n",
    "**How It Works:**\n",
    "1. **Forward pass**: Build computation graph, compute output\n",
    "2. **Call .backward()**: Start backpropagation from the output\n",
    "3. **Traverse graph**: Visit each operation in reverse order\n",
    "4. **Apply chain rule**: Compute gradients using chain rule\n",
    "5. **Store gradients**: Save gradients in `.grad` attributes\n",
    "\n",
    "**Key Properties:**\n",
    "- Must call `.backward()` on a **scalar** (single value)\n",
    "- Computes gradients for all tensors with `requires_grad=True`\n",
    "- Graph is freed after backward (unless `retain_graph=True`)\n",
    "- Gradients accumulate by default (must zero between steps)\n",
    "\n",
    "**In Neural Networks:**\n",
    "```python\n",
    "# Typical training step\n",
    "optimizer.zero_grad()        # Zero previous gradients\n",
    "output = model(input)        # Forward pass\n",
    "loss = criterion(output, target)  # Compute loss\n",
    "loss.backward()              # Backward pass - compute gradients\n",
    "optimizer.step()             # Update parameters using gradients\n",
    "```\n",
    "\n",
    "**Gradient Flow:**\n",
    "- Gradients flow backward from loss to parameters\n",
    "- Each operation contributes to the gradient via chain rule\n",
    "- Final gradients tell us how to adjust parameters to reduce loss\n",
    "\n",
    "**Common Patterns:**\n",
    "- **Single backward**: `loss.backward()` - most common\n",
    "- **Retain graph**: `loss.backward(retain_graph=True)` - for multiple backwards\n",
    "- **Gradient accumulation**: Multiple forwards, one backward (for large batches)\n",
    "\n",
    "**Why This Matters:**\n",
    "The backward pass is where neural networks learn. It computes the gradients that tell us:\n",
    "- Which direction to adjust each parameter\n",
    "- How much to adjust each parameter\n",
    "- How changes in parameters affect the loss\n",
    "\n",
    "Without the backward pass, we couldn't train neural networks. It's the implementation of backpropagation - the algorithm that made deep learning possible!\n",
    "\n",
    "**Remember**: Forward pass computes predictions, backward pass computes gradients. Together, they form the foundation of neural network training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d4606a",
   "metadata": {},
   "source": [
    "## loss.backward() - Computing Gradients from Loss\n",
    "\n",
    "**What is loss.backward()?**\n",
    "\n",
    "`loss.backward()` is the specific call to `.backward()` on the loss tensor in neural network training. The loss is a scalar value that measures how wrong your model's predictions are. Calling `.backward()` on the loss computes gradients for all model parameters, telling you how to adjust them to reduce the loss.\n",
    "\n",
    "**Why does loss.backward() matter?**\n",
    "\n",
    "`loss.backward()` is the single most important line in neural network training:\n",
    "- **Learning**: Computes gradients that enable the model to learn\n",
    "- **Optimization**: Provides gradients to the optimizer for parameter updates\n",
    "- **Training loop**: The bridge between forward pass and parameter updates\n",
    "- **Backpropagation**: Implements backpropagation through the entire network\n",
    "\n",
    "**Connection to deep learning**: Every training iteration follows this pattern:\n",
    "1. Forward pass: Compute predictions\n",
    "2. Compute loss: Measure prediction error\n",
    "3. **loss.backward()**: Compute gradients\n",
    "4. Optimizer step: Update parameters\n",
    "\n",
    "Without `loss.backward()`, the model can't learn. It's the moment where the model figures out how to improve!\n",
    "\n",
    "Let's see `loss.backward()` in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a042a6",
   "metadata": {},
   "source": [
    "### Basic loss.backward() Example\n",
    "\n",
    "Let's start with a simple example showing the complete training pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Simple model: single linear layer\n",
    "model = nn.Linear(2, 1)\n",
    "\n",
    "# Sample data\n",
    "x = torch.tensor([[1.0, 2.0]])  # Input\n",
    "y_true = torch.tensor([[3.0]])  # Target\n",
    "\n",
    "print(\"Setup:\")\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Target: {y_true}\")\n",
    "print(f\"Initial weights: {model.weight.data}\")\n",
    "print(f\"Initial bias: {model.bias.data}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c57cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "y_pred = model(x)\n",
    "print(f\"Prediction: {y_pred.item():.4f}\")\n",
    "print(f\"Target: {y_true.item():.4f}\")\n",
    "print()\n",
    "\n",
    "# Compute loss\n",
    "loss = ((y_pred - y_true) ** 2).mean()  # MSE loss\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Before loss.backward():\")\n",
    "print(f\"Weight gradients: {model.weight.grad}\")\n",
    "print(f\"Bias gradients: {model.bias.grad}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE KEY STEP: Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "print(\"After loss.backward():\")\n",
    "print(f\"Weight gradients: {model.weight.grad}\")\n",
    "print(f\"Bias gradients: {model.bias.grad}\")\n",
    "print()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Negative gradients mean: increase parameter to reduce loss\")\n",
    "print(\"- Positive gradients mean: decrease parameter to reduce loss\")\n",
    "print(\"- Magnitude indicates how much the parameter affects the loss\")\n",
    "print()\n",
    "print(\"These gradients tell the optimizer how to update parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c244bdc",
   "metadata": {},
   "source": [
    "### Complete Training Step with loss.backward()\n",
    "\n",
    "Let's see a complete training step including the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605221eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Create model and optimizer\n",
    "model = nn.Linear(3, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Sample data\n",
    "x = torch.randn(5, 3)  # 5 samples, 3 features\n",
    "y_true = torch.randn(5, 1)  # 5 targets\n",
    "\n",
    "print(\"Before training step:\")\n",
    "print(f\"Weight: {model.weight.data[0, :3]}...\")\n",
    "print(f\"Bias: {model.bias.data.item():.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc12a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE TRAINING STEP\n",
    "\n",
    "# 1. Zero gradients from previous step\n",
    "optimizer.zero_grad()\n",
    "print(\"1. Zeroed gradients\")\n",
    "\n",
    "# 2. Forward pass\n",
    "y_pred = model(x)\n",
    "print(f\"2. Forward pass complete\")\n",
    "\n",
    "# 3. Compute loss\n",
    "loss = ((y_pred - y_true) ** 2).mean()\n",
    "print(f\"3. Loss computed: {loss.item():.4f}\")\n",
    "\n",
    "# 4. Backward pass - THE KEY STEP\n",
    "loss.backward()\n",
    "print(f\"4. loss.backward() computed gradients\")\n",
    "\n",
    "# 5. Update parameters\n",
    "optimizer.step()\n",
    "print(f\"5. Parameters updated\")\n",
    "print()\n",
    "\n",
    "print(\"After training step:\")\n",
    "print(f\"Weight: {model.weight.data[0, :3]}...\")\n",
    "print(f\"Bias: {model.bias.data.item():.4f}\")\n",
    "print()\n",
    "print(\"Parameters changed! The model learned from this batch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b9bdd",
   "metadata": {},
   "source": [
    "### Multiple Training Steps - Watching the Model Learn\n",
    "\n",
    "Let's run multiple training steps and watch the loss decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e2ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(100, 3)\n",
    "true_weights = torch.tensor([[2.0, -1.0, 0.5]])\n",
    "true_bias = torch.tensor([1.0])\n",
    "y = X @ true_weights.T + true_bias + torch.randn(100, 1) * 0.1\n",
    "\n",
    "# Create model and optimizer\n",
    "model = nn.Linear(3, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Training a model to learn the true weights and bias...\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True bias: {true_bias.item()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076c7a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Training step\n",
    "    optimizer.zero_grad()      # 1. Zero gradients\n",
    "    y_pred = model(X)          # 2. Forward pass\n",
    "    loss = ((y_pred - y) ** 2).mean()  # 3. Compute loss\n",
    "    loss.backward()            # 4. Backward pass\n",
    "    optimizer.step()           # 5. Update parameters\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: Loss = {loss.item():.6f}\")\n",
    "\n",
    "print()\n",
    "print(\"Final learned parameters:\")\n",
    "print(f\"Learned weights: {model.weight.data}\")\n",
    "print(f\"Learned bias: {model.bias.data.item():.4f}\")\n",
    "print()\n",
    "print(f\"Loss decreased from {losses[0]:.6f} to {losses[-1]:.6f}\")\n",
    "print(\"The model learned! All thanks to loss.backward() computing gradients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42ea72",
   "metadata": {},
   "source": [
    "### loss.backward() with Different Loss Functions\n",
    "\n",
    "`loss.backward()` works with any differentiable loss function. Let's see a few common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc13bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "model = nn.Linear(5, 3)  # 5 inputs, 3 outputs (3 classes)\n",
    "x = torch.randn(10, 5)   # 10 samples\n",
    "y_true = torch.randint(0, 3, (10,))  # Class labels 0, 1, or 2\n",
    "\n",
    "# Forward pass\n",
    "logits = model(x)\n",
    "\n",
    "print(\"Different loss functions:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74babb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cross Entropy Loss (classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(logits, y_true)\n",
    "\n",
    "print(\"1. Cross Entropy Loss (for classification):\")\n",
    "print(f\"   Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Compute gradients\n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "print(f\"   Gradients computed: {model.weight.grad is not None}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0684bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. MSE Loss (regression)\n",
    "model_reg = nn.Linear(5, 1)\n",
    "y_true_reg = torch.randn(10, 1)\n",
    "y_pred_reg = model_reg(x)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(y_pred_reg, y_true_reg)\n",
    "\n",
    "print(\"2. MSE Loss (for regression):\")\n",
    "print(f\"   Loss: {loss.item():.4f}\")\n",
    "\n",
    "model_reg.zero_grad()\n",
    "loss.backward()\n",
    "print(f\"   Gradients computed: {model_reg.weight.grad is not None}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f1191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Custom loss function\n",
    "def custom_loss(pred, target):\n",
    "    \"\"\"Custom loss: MAE + L2 regularization\"\"\"\n",
    "    mae = (pred - target).abs().mean()\n",
    "    l2_reg = (model_reg.weight ** 2).sum()\n",
    "    return mae + 0.01 * l2_reg\n",
    "\n",
    "y_pred_reg = model_reg(x)\n",
    "loss = custom_loss(y_pred_reg, y_true_reg)\n",
    "\n",
    "print(\"3. Custom Loss (MAE + L2 regularization):\")\n",
    "print(f\"   Loss: {loss.item():.4f}\")\n",
    "\n",
    "model_reg.zero_grad()\n",
    "loss.backward()\n",
    "print(f\"   Gradients computed: {model_reg.weight.grad is not None}\")\n",
    "print()\n",
    "\n",
    "print(\"loss.backward() works with ANY differentiable loss function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb80446",
   "metadata": {},
   "source": [
    "### Common Patterns with loss.backward()\n",
    "\n",
    "Here are some common patterns you'll see in PyTorch code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c3b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Basic training loop\n",
    "print(\"Pattern 1: Basic Training Loop\")\n",
    "print(\"```python\")\n",
    "print(\"for epoch in range(num_epochs):\")\n",
    "print(\"    for batch in dataloader:\")\n",
    "print(\"        optimizer.zero_grad()\")\n",
    "print(\"        output = model(batch['input'])\")\n",
    "print(\"        loss = criterion(output, batch['target'])\")\n",
    "print(\"        loss.backward()  # Compute gradients\")\n",
    "print(\"        optimizer.step()\")\n",
    "print(\"```\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2273b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 2: Gradient accumulation (for large batches)\n",
    "print(\"Pattern 2: Gradient Accumulation\")\n",
    "print(\"```python\")\n",
    "print(\"accumulation_steps = 4\")\n",
    "print(\"optimizer.zero_grad()\")\n",
    "print(\"\")\n",
    "print(\"for i, batch in enumerate(dataloader):\")\n",
    "print(\"    output = model(batch['input'])\")\n",
    "print(\"    loss = criterion(output, batch['target'])\")\n",
    "print(\"    loss = loss / accumulation_steps  # Scale loss\")\n",
    "print(\"    loss.backward()  # Accumulate gradients\")\n",
    "print(\"    \")\n",
    "print(\"    if (i + 1) % accumulation_steps == 0:\")\n",
    "print(\"        optimizer.step()  # Update after N batches\")\n",
    "print(\"        optimizer.zero_grad()\")\n",
    "print(\"```\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfba92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 3: Mixed precision training\n",
    "print(\"Pattern 3: Mixed Precision Training (with autocast)\")\n",
    "print(\"```python\")\n",
    "print(\"from torch.cuda.amp import autocast, GradScaler\")\n",
    "print(\"\")\n",
    "print(\"scaler = GradScaler()\")\n",
    "print(\"\")\n",
    "print(\"for batch in dataloader:\")\n",
    "print(\"    optimizer.zero_grad()\")\n",
    "print(\"    \")\n",
    "print(\"    with autocast():  # Use mixed precision\")\n",
    "print(\"        output = model(batch['input'])\")\n",
    "print(\"        loss = criterion(output, batch['target'])\")\n",
    "print(\"    \")\n",
    "print(\"    scaler.scale(loss).backward()  # Scaled backward\")\n",
    "print(\"    scaler.step(optimizer)\")\n",
    "print(\"    scaler.update()\")\n",
    "print(\"```\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0632e3e",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**What is loss.backward()?**\n",
    "- Calling `.backward()` specifically on the loss tensor\n",
    "- Computes gradients for all model parameters\n",
    "- The single most important line in neural network training\n",
    "- Implements backpropagation through the entire network\n",
    "\n",
    "**The Training Pattern:**\n",
    "```python\n",
    "optimizer.zero_grad()        # 1. Clear old gradients\n",
    "output = model(input)        # 2. Forward pass\n",
    "loss = criterion(output, target)  # 3. Compute loss\n",
    "loss.backward()              # 4. Compute gradients ← THE KEY STEP\n",
    "optimizer.step()             # 5. Update parameters\n",
    "```\n",
    "\n",
    "**What loss.backward() Does:**\n",
    "1. Starts at the loss (a scalar value)\n",
    "2. Traverses the computation graph backward\n",
    "3. Applies chain rule at each operation\n",
    "4. Computes gradients for all parameters\n",
    "5. Stores gradients in `.grad` attributes\n",
    "\n",
    "**Why It Matters:**\n",
    "- **Learning**: Gradients tell the model how to improve\n",
    "- **Direction**: Gradients show which way to adjust parameters\n",
    "- **Magnitude**: Gradient size indicates how much to adjust\n",
    "- **Optimization**: Enables gradient-based optimization algorithms\n",
    "\n",
    "**Works With Any Loss:**\n",
    "- Classification: CrossEntropyLoss, BCELoss, etc.\n",
    "- Regression: MSELoss, L1Loss, etc.\n",
    "- Custom: Any differentiable function\n",
    "\n",
    "**Common Patterns:**\n",
    "- **Basic training**: One forward, one backward per batch\n",
    "- **Gradient accumulation**: Multiple forwards, one backward (large batches)\n",
    "- **Mixed precision**: Scaled backward for memory efficiency\n",
    "\n",
    "**Critical Rules:**\n",
    "- Loss must be a scalar (single value)\n",
    "- Always zero gradients before backward\n",
    "- Call backward before optimizer.step()\n",
    "- Graph is freed after backward (unless retain_graph=True)\n",
    "\n",
    "**The Big Picture:**\n",
    "\n",
    "Neural network training is a cycle:\n",
    "1. **Forward pass**: Model makes predictions\n",
    "2. **Loss computation**: Measure how wrong predictions are\n",
    "3. **loss.backward()**: Figure out how to improve ← YOU ARE HERE\n",
    "4. **Optimizer step**: Actually improve the model\n",
    "5. Repeat\n",
    "\n",
    "Without `loss.backward()`, the model would never learn. It's the moment where:\n",
    "- The model analyzes its mistakes\n",
    "- Figures out how each parameter contributed to the error\n",
    "- Computes exactly how to adjust parameters to do better\n",
    "\n",
    "**Remember**: `loss.backward()` is where the magic happens. It's the implementation of backpropagation - the algorithm that revolutionized machine learning and made deep learning possible. Every time you train a neural network, `loss.backward()` is working behind the scenes to make your model smarter!\n",
    "\n",
    "Master `loss.backward()`, and you understand the heart of neural network training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ff210",
   "metadata": {},
   "source": [
    "## nn.Module - The Foundation of Neural Networks\n",
    "\n",
    "**What is nn.Module?**\n",
    "\n",
    "`nn.Module` is the base class for all neural network modules in PyTorch. Every neural network you build - from simple linear layers to complex transformers - inherits from `nn.Module`. It's the fundamental building block that makes PyTorch's neural network API so powerful and flexible.\n",
    "\n",
    "Think of `nn.Module` as a container that:\n",
    "- Holds learnable parameters (weights and biases)\n",
    "- Defines how data flows through the network (forward pass)\n",
    "- Automatically tracks all parameters for optimization\n",
    "- Provides utilities for training, evaluation, and device management\n",
    "\n",
    "**Why does nn.Module matter?**\n",
    "\n",
    "nn.Module is **essential** for building neural networks because it:\n",
    "- **Automatic parameter tracking**: Registers all parameters automatically\n",
    "- **Composability**: Modules can contain other modules (layers within networks)\n",
    "- **Device management**: Easy movement between CPU and GPU\n",
    "- **Training modes**: Handles train vs eval behavior (dropout, batch norm)\n",
    "- **State management**: Save and load model weights easily\n",
    "\n",
    "**Key principle**: Every neural network component is an nn.Module - individual layers (Linear, Conv2d), activation functions (ReLU), and complete models (ResNet, BERT) all inherit from nn.Module.\n",
    "\n",
    "Let's explore how to create and use nn.Module to build neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8f0c30",
   "metadata": {},
   "source": [
    "### Creating Your First nn.Module\n",
    "\n",
    "To create a custom neural network module, you:\n",
    "1. Inherit from `nn.Module`\n",
    "2. Define `__init__()` to set up layers and parameters\n",
    "3. Define `forward()` to specify how data flows through the network\n",
    "\n",
    "Let's start with the simplest possible example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d81923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple neural network module\n",
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Always call parent's __init__\n",
    "        # Define a single linear layer: 10 input features -> 5 output features\n",
    "        self.linear = nn.Linear(10, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define how data flows through the module\n",
    "        return self.linear(x)\n",
    "\n",
    "# Create an instance of our module\n",
    "model = SimpleModule()\n",
    "print(\"Created a simple neural network module:\")\n",
    "print(model)\n",
    "print()\n",
    "print(f\"Type: {type(model)}\")\n",
    "print(f\"Is it an nn.Module? {isinstance(model, nn.Module)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the module to process data\n",
    "# Create a batch of 3 samples, each with 10 features\n",
    "input_data = torch.randn(3, 10)\n",
    "\n",
    "print(\"Input data:\")\n",
    "print(f\"Shape: {input_data.shape}  # (3 samples, 10 features)\")\n",
    "print()\n",
    "\n",
    "# Forward pass: call the module like a function\n",
    "output = model(input_data)\n",
    "\n",
    "print(\"Output:\")\n",
    "print(f\"Shape: {output.shape}  # (3 samples, 5 features)\")\n",
    "print(output)\n",
    "print()\n",
    "print(\"The module transformed 10 features into 5 features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20213777",
   "metadata": {},
   "source": [
    "**Important**: When you call `model(input_data)`, PyTorch automatically calls the `forward()` method. You should **never** call `forward()` directly - always use `model(input)` instead. This ensures hooks and other PyTorch machinery work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee76c607",
   "metadata": {},
   "source": [
    "### Automatic Parameter Tracking\n",
    "\n",
    "One of nn.Module's superpowers is **automatic parameter tracking**. Any `nn.Parameter` or sub-module you assign in `__init__()` is automatically registered and tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ead8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model's parameters\n",
    "print(\"Model parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name:20s} - Shape: {param.shape}\")\n",
    "print()\n",
    "\n",
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print()\n",
    "print(\"Breakdown:\")\n",
    "print(f\"  Weight matrix: 10 × 5 = 50 parameters\")\n",
    "print(f\"  Bias vector: 5 parameters\")\n",
    "print(f\"  Total: 50 + 5 = 55 parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa4f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access specific parameters\n",
    "print(\"Linear layer weight:\")\n",
    "print(f\"Shape: {model.linear.weight.shape}  # (5 output, 10 input)\")\n",
    "print(model.linear.weight)\n",
    "print()\n",
    "\n",
    "print(\"Linear layer bias:\")\n",
    "print(f\"Shape: {model.linear.bias.shape}  # (5,)\")\n",
    "print(model.linear.bias)\n",
    "print()\n",
    "print(\"These parameters are what the model learns during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b74ac6",
   "metadata": {},
   "source": [
    "### Building Multi-Layer Networks\n",
    "\n",
    "Real neural networks have multiple layers. Let's build a more complex module with multiple layers and activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73173f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        # Define multiple layers\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.ReLU()  # Non-linear activation\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass through all layers\n",
    "        x = self.layer1(x)      # First linear transformation\n",
    "        x = self.activation(x)  # Apply ReLU activation\n",
    "        x = self.layer2(x)      # Second linear transformation\n",
    "        return x\n",
    "\n",
    "# Create a network: 20 inputs -> 64 hidden -> 10 outputs\n",
    "network = MultiLayerNetwork(input_size=20, hidden_size=64, output_size=10)\n",
    "print(\"Multi-layer network:\")\n",
    "print(network)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fadd186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network\n",
    "batch_size = 5\n",
    "input_data = torch.randn(batch_size, 20)\n",
    "\n",
    "print(f\"Input shape: {input_data.shape}\")\n",
    "print()\n",
    "\n",
    "# Forward pass\n",
    "output = network(input_data)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in network.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print()\n",
    "print(\"Parameter breakdown:\")\n",
    "print(f\"  Layer 1 weights: 20 × 64 = 1,280\")\n",
    "print(f\"  Layer 1 bias: 64\")\n",
    "print(f\"  Layer 2 weights: 64 × 10 = 640\")\n",
    "print(f\"  Layer 2 bias: 10\")\n",
    "print(f\"  Total: 1,280 + 64 + 640 + 10 = 1,994\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983279b8",
   "metadata": {},
   "source": [
    "### Module Composition - Modules Within Modules\n",
    "\n",
    "One of nn.Module's most powerful features is **composability**. You can build complex networks by combining simpler modules. This is how large models like ResNet and BERT are constructed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba59e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a reusable block\n",
    "class LinearBlock(nn.Module):\n",
    "    \"\"\"A linear layer followed by ReLU activation.\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.linear(x))\n",
    "\n",
    "# Build a network using multiple blocks\n",
    "class ComposedNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Stack multiple LinearBlocks\n",
    "        self.block1 = LinearBlock(10, 20)\n",
    "        self.block2 = LinearBlock(20, 20)\n",
    "        self.block3 = LinearBlock(20, 10)\n",
    "        self.output = nn.Linear(10, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "composed_model = ComposedNetwork()\n",
    "print(\"Composed network (modules within modules):\")\n",
    "print(composed_model)\n",
    "print()\n",
    "print(f\"Total parameters: {sum(p.numel() for p in composed_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b650b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the composed network\n",
    "test_input = torch.randn(3, 10)\n",
    "test_output = composed_model(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print()\n",
    "\n",
    "# All parameters are still tracked!\n",
    "print(\"All parameters (including nested modules):\")\n",
    "for name, param in composed_model.named_parameters():\n",
    "    print(f\"  {name:30s} - Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a4a768",
   "metadata": {},
   "source": [
    "### nn.Sequential - Quick Network Building\n",
    "\n",
    "For simple sequential architectures, PyTorch provides `nn.Sequential` - a convenient way to stack layers without writing a custom class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a network using nn.Sequential\n",
    "sequential_model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "print(\"Sequential model:\")\n",
    "print(sequential_model)\n",
    "print()\n",
    "\n",
    "# Use it just like any other module\n",
    "input_data = torch.randn(2, 10)\n",
    "output = sequential_model(input_data)\n",
    "\n",
    "print(f\"Input shape: {input_data.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print()\n",
    "print(\"nn.Sequential is perfect for simple feed-forward architectures!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0005d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access layers in Sequential\n",
    "print(\"Accessing layers in Sequential:\")\n",
    "print(f\"First layer: {sequential_model[0]}\")\n",
    "print(f\"Second layer: {sequential_model[1]}\")\n",
    "print()\n",
    "\n",
    "# You can also name the layers\n",
    "from collections import OrderedDict\n",
    "\n",
    "named_sequential = nn.Sequential(OrderedDict([\n",
    "    ('input_layer', nn.Linear(10, 20)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('hidden_layer', nn.Linear(20, 20)),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('output_layer', nn.Linear(20, 5))\n",
    "]))\n",
    "\n",
    "print(\"Named sequential model:\")\n",
    "print(named_sequential)\n",
    "print()\n",
    "print(\"Access by name:\")\n",
    "print(f\"Input layer: {named_sequential.input_layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e815dc",
   "metadata": {},
   "source": [
    "### Device Management with nn.Module\n",
    "\n",
    "nn.Module makes it easy to move entire models (with all their parameters) between CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62221ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = MultiLayerNetwork(20, 64, 10)\n",
    "\n",
    "# Check current device\n",
    "print(\"Initial device:\")\n",
    "print(f\"Model parameters on: {next(model.parameters()).device}\")\n",
    "print()\n",
    "\n",
    "# Move to device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"After moving to {device}:\")\n",
    "print(f\"Model parameters on: {next(model.parameters()).device}\")\n",
    "print()\n",
    "\n",
    "# Now inputs must also be on the same device\n",
    "input_data = torch.randn(5, 20).to(device)\n",
    "output = model(input_data)\n",
    "\n",
    "print(f\"Input device: {input_data.device}\")\n",
    "print(f\"Output device: {output.device}\")\n",
    "print()\n",
    "print(\"All tensors (model and data) must be on the same device!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c64b3",
   "metadata": {},
   "source": [
    "### Training vs Evaluation Mode\n",
    "\n",
    "nn.Module has two modes: **training** and **evaluation**. Some layers (like Dropout and BatchNorm) behave differently in each mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with dropout\n",
    "class ModelWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(10, 20)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Drop 50% of activations\n",
    "        self.linear2 = nn.Linear(20, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout(x)  # Dropout is applied here\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "model = ModelWithDropout()\n",
    "\n",
    "# Check current mode\n",
    "print(f\"Training mode: {model.training}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da27994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training mode (default)\n",
    "model.train()  # Set to training mode\n",
    "print(\"Training mode:\")\n",
    "print(f\"model.training = {model.training}\")\n",
    "print(\"Dropout is ACTIVE - randomly drops activations\")\n",
    "print()\n",
    "\n",
    "# Evaluation mode\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(\"Evaluation mode:\")\n",
    "print(f\"model.training = {model.training}\")\n",
    "print(\"Dropout is INACTIVE - uses all activations\")\n",
    "print()\n",
    "\n",
    "print(\"Important: Always use model.eval() during inference/testing!\")\n",
    "print(\"Always use model.train() during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e0f86c",
   "metadata": {},
   "source": [
    "### Practical Example - Complete Classification Model\n",
    "\n",
    "Let's build a complete model for a real task: classifying images into 10 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple image classifier.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: Flattened image (e.g., 28×28 = 784 pixels)\n",
    "    - Hidden layer 1: 784 -> 256 with ReLU\n",
    "    - Hidden layer 2: 256 -> 128 with ReLU\n",
    "    - Output layer: 128 -> 10 classes\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=784, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),  # 20% dropout for regularization\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 784)\n",
    "        return self.network(x)\n",
    "\n",
    "# Create the classifier\n",
    "classifier = ImageClassifier()\n",
    "print(\"Image Classifier:\")\n",
    "print(classifier)\n",
    "print()\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in classifier.parameters())\n",
    "trainable_params = sum(p.numel() for p in classifier.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a batch of images\n",
    "batch_size = 32\n",
    "image_size = 28 * 28  # 784 pixels\n",
    "images = torch.randn(batch_size, image_size)\n",
    "\n",
    "print(f\"Input batch shape: {images.shape}\")\n",
    "print()\n",
    "\n",
    "# Forward pass\n",
    "classifier.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():  # Don't compute gradients for inference\n",
    "    logits = classifier(images)\n",
    "\n",
    "print(f\"Output logits shape: {logits.shape}  # (32 samples, 10 classes)\")\n",
    "print()\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(logits, dim=1)\n",
    "print(f\"Predictions: {predictions}\")\n",
    "print(f\"Predictions shape: {predictions.shape}  # (32,)\")\n",
    "print()\n",
    "print(\"Each number represents the predicted class (0-9) for each image!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a22bd2a",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**What is nn.Module?**\n",
    "- Base class for ALL neural network components in PyTorch\n",
    "- Provides automatic parameter tracking and management\n",
    "- Enables composability - modules can contain other modules\n",
    "- Handles device management, training modes, and state saving\n",
    "\n",
    "**Creating a Custom Module:**\n",
    "```python\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Always call this first!\n",
    "        # Define layers and parameters here\n",
    "        self.layer = nn.Linear(10, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define forward pass here\n",
    "        return self.layer(x)\n",
    "```\n",
    "\n",
    "**Key Methods:**\n",
    "- `__init__()`: Initialize layers and parameters\n",
    "- `forward(x)`: Define how data flows through the module\n",
    "- `.parameters()`: Get all learnable parameters\n",
    "- `.to(device)`: Move module to CPU/GPU\n",
    "- `.train()`: Set to training mode\n",
    "- `.eval()`: Set to evaluation mode\n",
    "\n",
    "**Important Rules:**\n",
    "1. Always call `super().__init__()` first in `__init__()`\n",
    "2. Call the module like a function: `output = model(input)`, not `model.forward(input)`\n",
    "3. Define all layers in `__init__()`, use them in `forward()`\n",
    "4. Use `model.train()` during training, `model.eval()` during inference\n",
    "5. Move model to device before creating optimizer: `model.to(device)`\n",
    "\n",
    "**Common Patterns:**\n",
    "- **Custom modules**: For complex architectures with custom logic\n",
    "- **nn.Sequential**: For simple sequential layer stacks\n",
    "- **Module composition**: Build complex models from simpler modules\n",
    "- **Reusable blocks**: Define common patterns (ResBlock, TransformerBlock)\n",
    "\n",
    "**Why nn.Module is Powerful:**\n",
    "- **Automatic differentiation**: All operations are tracked for backprop\n",
    "- **Parameter management**: No manual tracking of weights and biases\n",
    "- **Flexibility**: Can implement any architecture you can imagine\n",
    "- **Ecosystem**: Works seamlessly with optimizers, loss functions, data loaders\n",
    "- **Debugging**: Easy to inspect layers, parameters, and intermediate outputs\n",
    "\n",
    "**The Big Picture:**\n",
    "\n",
    "nn.Module is the foundation of PyTorch's neural network API. Every model you build - from simple linear regression to GPT-4 - is an nn.Module. Understanding nn.Module means understanding how to:\n",
    "- Structure neural networks\n",
    "- Manage parameters\n",
    "- Compose complex architectures\n",
    "- Integrate with PyTorch's training ecosystem\n",
    "\n",
    "Master nn.Module, and you can build any neural network architecture you can imagine!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe2ac8",
   "metadata": {},
   "source": [
    "## Parameters - Learnable Weights in Neural Networks\n",
    "\n",
    "**What are Parameters?**\n",
    "\n",
    "`nn.Parameter` is a special type of tensor that tells PyTorch: \"This tensor contains values that should be learned during training.\" When you assign an `nn.Parameter` to a module, PyTorch automatically:\n",
    "- Tracks it as a learnable parameter\n",
    "- Includes it in `.parameters()` and `.named_parameters()`\n",
    "- Passes it to optimizers for updates\n",
    "- Moves it when you call `.to(device)`\n",
    "\n",
    "**Why do Parameters matter?**\n",
    "\n",
    "Parameters are **the heart of machine learning** - they're the values that the model learns:\n",
    "- **Weights**: Connection strengths between neurons\n",
    "- **Biases**: Offset terms that shift activations\n",
    "- **Embeddings**: Learned representations of discrete tokens\n",
    "- **Custom learnable values**: Any tensor you want the model to optimize\n",
    "\n",
    "Without parameters, a neural network would just be a fixed function. Parameters are what make neural networks \"learn\" from data!\n",
    "\n",
    "**Key insight**: Most of the time, you don't create `nn.Parameter` directly - layers like `nn.Linear` create them for you. But understanding parameters is essential for building custom layers and debugging models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2a8e5",
   "metadata": {},
   "source": [
    "### Creating and Using Parameters\n",
    "\n",
    "Let's see how to create parameters and how PyTorch tracks them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba08f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a simple module with a custom parameter\n",
    "class ModuleWithParameter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a learnable parameter: a 3x3 matrix\n",
    "        self.weight = nn.Parameter(torch.randn(3, 3))\n",
    "        # Create another parameter: a bias vector\n",
    "        self.bias = nn.Parameter(torch.randn(3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Use the parameters in computation\n",
    "        return x @ self.weight + self.bias\n",
    "\n",
    "model = ModuleWithParameter()\n",
    "print(\"Module with custom parameters:\")\n",
    "print(model)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the parameters\n",
    "print(\"Model parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Shape: {param.shape}\")\n",
    "    print(f\"    Type: {type(param)}\")\n",
    "    print(f\"    Requires grad: {param.requires_grad}\")\n",
    "    print(f\"    Device: {param.device}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60afefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters vs regular tensors\n",
    "print(\"What makes nn.Parameter special?\\n\")\n",
    "\n",
    "# Regular tensor - NOT tracked\n",
    "class ModuleWithTensor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.regular_tensor = torch.randn(3, 3)  # Just a tensor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x @ self.regular_tensor\n",
    "\n",
    "model_tensor = ModuleWithTensor()\n",
    "print(\"Module with regular tensor:\")\n",
    "print(f\"Number of parameters: {len(list(model_tensor.parameters()))}\")\n",
    "print(\"Parameters:\", list(model_tensor.parameters()))\n",
    "print()\n",
    "\n",
    "# With nn.Parameter - IS tracked\n",
    "class ModuleWithParam(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param = nn.Parameter(torch.randn(3, 3))  # Parameter!\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x @ self.param\n",
    "\n",
    "model_param = ModuleWithParam()\n",
    "print(\"Module with nn.Parameter:\")\n",
    "print(f\"Number of parameters: {len(list(model_param.parameters()))}\")\n",
    "print(f\"Parameter shape: {list(model_param.parameters())[0].shape}\")\n",
    "print()\n",
    "print(\"Key difference: nn.Parameter is automatically tracked and optimized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982dec8a",
   "metadata": {},
   "source": [
    "### How Built-in Layers Use Parameters\n",
    "\n",
    "Let's look at how standard PyTorch layers (like `nn.Linear`) use parameters internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c22de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear layer\n",
    "linear = nn.Linear(in_features=5, out_features=3)\n",
    "\n",
    "print(\"nn.Linear layer:\")\n",
    "print(linear)\n",
    "print()\n",
    "\n",
    "# Inspect its parameters\n",
    "print(\"Parameters in nn.Linear:\")\n",
    "for name, param in linear.named_parameters():\n",
    "    print(f\"  {name}: shape {param.shape}\")\n",
    "print()\n",
    "\n",
    "# Access parameters directly\n",
    "print(\"Weight matrix:\")\n",
    "print(f\"  Shape: {linear.weight.shape}  # (out_features, in_features)\")\n",
    "print(f\"  Type: {type(linear.weight)}\")\n",
    "print(f\"  Values:\\n{linear.weight}\")\n",
    "print()\n",
    "\n",
    "print(\"Bias vector:\")\n",
    "print(f\"  Shape: {linear.bias.shape}  # (out_features,)\")\n",
    "print(f\"  Type: {type(linear.bias)}\")\n",
    "print(f\"  Values: {linear.bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899751c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters in a layer\n",
    "def count_parameters(module):\n",
    "    \"\"\"Count total number of parameters in a module.\"\"\"\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "print(f\"Total parameters in linear layer: {count_parameters(linear)}\")\n",
    "print()\n",
    "print(\"Breakdown:\")\n",
    "print(f\"  Weight: {linear.weight.numel()} = 3 × 5 = 15\")\n",
    "print(f\"  Bias: {linear.bias.numel()} = 3\")\n",
    "print(f\"  Total: 15 + 3 = 18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782ef985",
   "metadata": {},
   "source": [
    "### Parameter Initialization\n",
    "\n",
    "How parameters are initialized matters! Poor initialization can make training slow or impossible. PyTorch layers use sensible defaults, but you can customize initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954bf7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default initialization (what PyTorch does automatically)\n",
    "layer = nn.Linear(10, 5)\n",
    "print(\"Default initialization:\")\n",
    "print(f\"Weight mean: {layer.weight.mean():.4f}\")\n",
    "print(f\"Weight std: {layer.weight.std():.4f}\")\n",
    "print(f\"Bias mean: {layer.bias.mean():.4f}\")\n",
    "print(f\"Bias std: {layer.bias.std():.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom initialization\n",
    "import torch.nn.init as init\n",
    "\n",
    "layer = nn.Linear(10, 5)\n",
    "\n",
    "# Xavier/Glorot initialization (good for tanh/sigmoid)\n",
    "init.xavier_uniform_(layer.weight)\n",
    "init.zeros_(layer.bias)\n",
    "\n",
    "print(\"After Xavier initialization:\")\n",
    "print(f\"Weight mean: {layer.weight.mean():.4f}\")\n",
    "print(f\"Weight std: {layer.weight.std():.4f}\")\n",
    "print(f\"Bias: {layer.bias}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb3c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different initialization methods\n",
    "layer1 = nn.Linear(10, 5)\n",
    "layer2 = nn.Linear(10, 5)\n",
    "layer3 = nn.Linear(10, 5)\n",
    "\n",
    "# Kaiming/He initialization (good for ReLU)\n",
    "init.kaiming_normal_(layer1.weight)\n",
    "print(\"Kaiming initialization (for ReLU):\")\n",
    "print(f\"  Weight std: {layer1.weight.std():.4f}\")\n",
    "print()\n",
    "\n",
    "# Normal distribution\n",
    "init.normal_(layer2.weight, mean=0, std=0.01)\n",
    "print(\"Normal initialization (mean=0, std=0.01):\")\n",
    "print(f\"  Weight mean: {layer2.weight.mean():.4f}\")\n",
    "print(f\"  Weight std: {layer2.weight.std():.4f}\")\n",
    "print()\n",
    "\n",
    "# Constant initialization\n",
    "init.constant_(layer3.weight, 0.5)\n",
    "init.constant_(layer3.bias, 0.0)\n",
    "print(\"Constant initialization (weight=0.5, bias=0):\")\n",
    "print(f\"  All weights: {layer3.weight[0, :3]}...\")\n",
    "print(f\"  All biases: {layer3.bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2661e37",
   "metadata": {},
   "source": [
    "### Freezing Parameters - Transfer Learning\n",
    "\n",
    "Sometimes you want to prevent certain parameters from being updated during training. This is common in **transfer learning** where you freeze pre-trained layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f81fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "print(\"Initial state:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name:20s} - requires_grad: {param.requires_grad}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70616c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the first layer\n",
    "for param in model[0].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"After freezing first layer:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name:20s} - requires_grad: {param.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable} / {total}\")\n",
    "print()\n",
    "print(\"Only the second layer will be updated during training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a370c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"After unfreezing all:\")\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable} / {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb70cc0",
   "metadata": {},
   "source": [
    "### Parameter Sharing - Reusing Weights\n",
    "\n",
    "Sometimes you want multiple parts of a network to share the same parameters. This is useful for weight tying, siamese networks, and reducing model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ac358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a module with shared parameters\n",
    "class SharedParameterModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create one linear layer\n",
    "        self.shared_layer = nn.Linear(10, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply the same layer twice (parameter sharing)\n",
    "        x = self.shared_layer(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.shared_layer(x)  # Same weights used again!\n",
    "        return x\n",
    "\n",
    "shared_model = SharedParameterModule()\n",
    "print(\"Model with shared parameters:\")\n",
    "print(shared_model)\n",
    "print()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in shared_model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print()\n",
    "print(\"Note: Only 110 parameters (10×10 + 10) even though layer is used twice!\")\n",
    "print(\"The same weights are reused, not duplicated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1af7b6",
   "metadata": {},
   "source": [
    "### Practical Example - Inspecting Model Parameters\n",
    "\n",
    "Let's build a complete model and thoroughly inspect its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94854ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a small neural network\n",
    "class SmallNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(784, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "model = SmallNet()\n",
    "print(\"SmallNet architecture:\")\n",
    "print(model)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b043a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed parameter analysis\n",
    "print(\"Detailed parameter breakdown:\\n\")\n",
    "\n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    total_params += num_params\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Shape: {param.shape}\")\n",
    "    print(f\"  Parameters: {num_params:,}\")\n",
    "    print(f\"  Requires grad: {param.requires_grad}\")\n",
    "    print(f\"  Data type: {param.dtype}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print()\n",
    "print(\"Memory usage:\")\n",
    "# Each float32 parameter uses 4 bytes\n",
    "memory_mb = (total_params * 4) / (1024 * 1024)\n",
    "print(f\"  {memory_mb:.2f} MB (for float32 parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af2ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter statistics\n",
    "print(\"Parameter statistics:\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Mean: {param.mean():.4f}\")\n",
    "        print(f\"  Std: {param.std():.4f}\")\n",
    "        print(f\"  Min: {param.min():.4f}\")\n",
    "        print(f\"  Max: {param.max():.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccc761c",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**What are Parameters?**\n",
    "- Special tensors that represent learnable values in neural networks\n",
    "- Created with `nn.Parameter(tensor)`\n",
    "- Automatically tracked by nn.Module\n",
    "- Updated by optimizers during training\n",
    "\n",
    "**Parameter vs Regular Tensor:**\n",
    "- **Regular tensor**: Just data, not tracked, not optimized\n",
    "- **nn.Parameter**: Tracked by module, included in `.parameters()`, optimized\n",
    "- **Rule**: Use nn.Parameter for any tensor you want to learn\n",
    "\n",
    "**Common Parameter Operations:**\n",
    "- **Access**: `model.layer.weight`, `model.layer.bias`\n",
    "- **List all**: `model.parameters()` or `model.named_parameters()`\n",
    "- **Count**: `sum(p.numel() for p in model.parameters())`\n",
    "- **Freeze**: `param.requires_grad = False`\n",
    "- **Unfreeze**: `param.requires_grad = True`\n",
    "\n",
    "**Parameter Initialization:**\n",
    "- **Default**: PyTorch uses sensible defaults (usually uniform or normal)\n",
    "- **Xavier/Glorot**: Good for tanh/sigmoid activations\n",
    "- **Kaiming/He**: Good for ReLU activations\n",
    "- **Custom**: Use `torch.nn.init` module for custom initialization\n",
    "\n",
    "**Advanced Techniques:**\n",
    "- **Freezing**: Prevent parameters from updating (transfer learning)\n",
    "- **Sharing**: Reuse same parameters in multiple places\n",
    "- **Regularization**: Add penalties to parameter values (L1, L2)\n",
    "- **Pruning**: Remove unnecessary parameters to reduce model size\n",
    "\n",
    "**Why Parameters Matter:**\n",
    "- **Learning**: Parameters are what the model learns from data\n",
    "- **Capacity**: More parameters = more capacity to learn complex patterns\n",
    "- **Memory**: Parameters determine model size and memory usage\n",
    "- **Training**: Parameter count affects training time and convergence\n",
    "\n",
    "**The Big Picture:**\n",
    "\n",
    "Parameters are the **learnable memory** of neural networks. When you train a model:\n",
    "1. Forward pass uses current parameter values to make predictions\n",
    "2. Loss measures how wrong the predictions are\n",
    "3. Backward pass computes gradients for each parameter\n",
    "4. Optimizer updates parameters to reduce loss\n",
    "5. Repeat until parameters converge to good values\n",
    "\n",
    "Understanding parameters means understanding:\n",
    "- What the model is learning\n",
    "- How much memory the model needs\n",
    "- How to debug training issues\n",
    "- How to customize and optimize models\n",
    "\n",
    "Every successful neural network is just a set of well-tuned parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6df8b0",
   "metadata": {},
   "source": [
    "## Weights - The Connection Strengths in Neural Networks\n",
    "\n",
    "**What are Weights?**\n",
    "\n",
    "Weights are the **connection strengths** between neurons in a neural network. In a linear layer, the weight matrix defines how input features are combined to produce output features. Each weight value represents how strongly one input feature influences one output feature.\n",
    "\n",
    "Think of weights as:\n",
    "- **Amplifiers**: Large weights amplify signals\n",
    "- **Filters**: Weights select which inputs matter\n",
    "- **Transformations**: Weight matrices transform input spaces to output spaces\n",
    "\n",
    "**Why do Weights matter?**\n",
    "\n",
    "Weights are **the core of what neural networks learn**:\n",
    "- **Pattern detection**: Weights learn to recognize patterns in data\n",
    "- **Feature extraction**: Weights transform raw inputs into useful representations\n",
    "- **Decision making**: Weights determine how inputs map to outputs\n",
    "- **Model capacity**: More weights = more complex patterns the model can learn\n",
    "\n",
    "**Key insight**: Training a neural network is essentially finding the right weight values. When we say \"the model learned,\" we mean \"the weights were adjusted to minimize error.\"\n",
    "\n",
    "Let's explore how weights work and how they're used in neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e06ed",
   "metadata": {},
   "source": [
    "### Weight Matrices in Linear Layers\n",
    "\n",
    "The most common place you'll encounter weights is in linear (fully-connected) layers. Let's understand their structure and how they transform data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d374071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a linear layer\n",
    "# Transforms 4 input features to 3 output features\n",
    "layer = nn.Linear(in_features=4, out_features=3)\n",
    "\n",
    "print(\"Linear layer: 4 inputs → 3 outputs\")\n",
    "print(layer)\n",
    "print()\n",
    "\n",
    "# Inspect the weight matrix\n",
    "print(\"Weight matrix:\")\n",
    "print(f\"Shape: {layer.weight.shape}  # (out_features, in_features)\")\n",
    "print(f\"Values:\\n{layer.weight}\")\n",
    "print()\n",
    "print(\"Note: Shape is (3, 4) = (outputs, inputs)\")\n",
    "print(\"Each row corresponds to one output neuron\")\n",
    "print(\"Each column corresponds to one input feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c181761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding weight matrix structure\n",
    "print(\"Weight matrix structure:\\n\")\n",
    "print(\"         Input 0   Input 1   Input 2   Input 3\")\n",
    "print(\"Output 0   w[0,0]    w[0,1]    w[0,2]    w[0,3]\")\n",
    "print(\"Output 1   w[1,0]    w[1,1]    w[1,2]    w[1,3]\")\n",
    "print(\"Output 2   w[2,0]    w[2,1]    w[2,2]    w[2,3]\")\n",
    "print()\n",
    "print(\"Each row: weights for one output neuron\")\n",
    "print(f\"  Output 0 weights: {layer.weight[0]}\")\n",
    "print(f\"  Output 1 weights: {layer.weight[1]}\")\n",
    "print(f\"  Output 2 weights: {layer.weight[2]}\")\n",
    "print()\n",
    "print(\"Each column: how one input affects all outputs\")\n",
    "print(f\"  Input 0 weights: {layer.weight[:, 0]}\")\n",
    "print(f\"  Input 1 weights: {layer.weight[:, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a3e680",
   "metadata": {},
   "source": [
    "### How Weights Transform Data\n",
    "\n",
    "Let's see exactly how weights transform input data through matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c16bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple example with known weights\n",
    "layer = nn.Linear(3, 2, bias=False)  # No bias for clarity\n",
    "\n",
    "# Set specific weight values\n",
    "layer.weight.data = torch.tensor([\n",
    "    [1.0, 2.0, 3.0],  # Weights for output 0\n",
    "    [4.0, 5.0, 6.0]   # Weights for output 1\n",
    "])\n",
    "\n",
    "print(\"Weight matrix:\")\n",
    "print(layer.weight)\n",
    "print()\n",
    "\n",
    "# Input vector\n",
    "x = torch.tensor([10.0, 20.0, 30.0])\n",
    "print(f\"Input: {x}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eae6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "output = layer(x)\n",
    "print(f\"Output: {output}\")\n",
    "print()\n",
    "\n",
    "# Manual calculation to understand what happened\n",
    "print(\"Manual calculation:\")\n",
    "print()\n",
    "print(\"Output[0] = (1.0 × 10) + (2.0 × 20) + (3.0 × 30)\")\n",
    "print(f\"          = 10 + 40 + 90 = {output[0].item()}\")\n",
    "print()\n",
    "print(\"Output[1] = (4.0 × 10) + (5.0 × 20) + (6.0 × 30)\")\n",
    "print(f\"          = 40 + 100 + 180 = {output[1].item()}\")\n",
    "print()\n",
    "print(\"Each output is a weighted sum of inputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b70103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing\n",
    "# Multiple samples processed simultaneously\n",
    "batch = torch.tensor([\n",
    "    [1.0, 2.0, 3.0],  # Sample 1\n",
    "    [4.0, 5.0, 6.0],  # Sample 2\n",
    "    [7.0, 8.0, 9.0]   # Sample 3\n",
    "])\n",
    "\n",
    "print(f\"Batch input shape: {batch.shape}  # (3 samples, 3 features)\")\n",
    "print()\n",
    "\n",
    "# Forward pass on batch\n",
    "batch_output = layer(batch)\n",
    "print(f\"Batch output shape: {batch_output.shape}  # (3 samples, 2 outputs)\")\n",
    "print(f\"Batch output:\\n{batch_output}\")\n",
    "print()\n",
    "print(\"Same weights applied to each sample independently!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009bb06f",
   "metadata": {},
   "source": [
    "### Interpreting Weight Values\n",
    "\n",
    "Weight values tell us how the network processes information. Let's explore what different weight patterns mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad6269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Weights that select specific features\n",
    "selector_layer = nn.Linear(5, 3, bias=False)\n",
    "\n",
    "# Set weights to select specific inputs\n",
    "selector_layer.weight.data = torch.tensor([\n",
    "    [1.0, 0.0, 0.0, 0.0, 0.0],  # Output 0: only uses input 0\n",
    "    [0.0, 0.0, 1.0, 0.0, 0.0],  # Output 1: only uses input 2\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0]   # Output 2: only uses input 4\n",
    "])\n",
    "\n",
    "print(\"Selector weights (one-hot patterns):\")\n",
    "print(selector_layer.weight)\n",
    "print()\n",
    "\n",
    "# Test input\n",
    "x = torch.tensor([10.0, 20.0, 30.0, 40.0, 50.0])\n",
    "output = selector_layer(x)\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {output}\")\n",
    "print()\n",
    "print(\"The layer selected inputs 0, 2, and 4!\")\n",
    "print(\"This is like feature selection through weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Weights that average inputs\n",
    "averaging_layer = nn.Linear(4, 2, bias=False)\n",
    "\n",
    "# Set weights to compute averages\n",
    "averaging_layer.weight.data = torch.tensor([\n",
    "    [0.25, 0.25, 0.25, 0.25],  # Output 0: average of all inputs\n",
    "    [0.5, 0.5, 0.0, 0.0]        # Output 1: average of first two inputs\n",
    "])\n",
    "\n",
    "print(\"Averaging weights:\")\n",
    "print(averaging_layer.weight)\n",
    "print()\n",
    "\n",
    "# Test input\n",
    "x = torch.tensor([10.0, 20.0, 30.0, 40.0])\n",
    "output = averaging_layer(x)\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {output}\")\n",
    "print()\n",
    "print(f\"Output[0] = (10+20+30+40)/4 = {output[0].item()}\")\n",
    "print(f\"Output[1] = (10+20)/2 = {output[1].item()}\")\n",
    "print()\n",
    "print(\"Weights can implement averaging, pooling, or aggregation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Positive vs negative weights\n",
    "mixed_layer = nn.Linear(3, 2, bias=False)\n",
    "\n",
    "mixed_layer.weight.data = torch.tensor([\n",
    "    [2.0, -1.0, 1.0],   # Positive and negative weights\n",
    "    [-1.0, -1.0, -1.0]  # All negative weights\n",
    "])\n",
    "\n",
    "print(\"Mixed positive/negative weights:\")\n",
    "print(mixed_layer.weight)\n",
    "print()\n",
    "\n",
    "x = torch.tensor([1.0, 1.0, 1.0])\n",
    "output = mixed_layer(x)\n",
    "\n",
    "print(f\"Input (all ones): {x}\")\n",
    "print(f\"Output: {output}\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\"  Output[0] = 2 - 1 + 1 = 2 (mixed effects)\")\n",
    "print(\"  Output[1] = -1 - 1 - 1 = -3 (inhibitory)\")\n",
    "print()\n",
    "print(\"Positive weights: excitatory (amplify signal)\")\n",
    "print(\"Negative weights: inhibitory (suppress signal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e9544",
   "metadata": {},
   "source": [
    "### Weight Magnitudes - Size Matters\n",
    "\n",
    "The magnitude (absolute value) of weights affects how strongly inputs influence outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4843e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different weight magnitudes\n",
    "small_weights = nn.Linear(3, 1, bias=False)\n",
    "large_weights = nn.Linear(3, 1, bias=False)\n",
    "\n",
    "# Small weights\n",
    "small_weights.weight.data = torch.tensor([[0.1, 0.1, 0.1]])\n",
    "\n",
    "# Large weights\n",
    "large_weights.weight.data = torch.tensor([[10.0, 10.0, 10.0]])\n",
    "\n",
    "# Same input\n",
    "x = torch.tensor([1.0, 1.0, 1.0])\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print()\n",
    "print(f\"Small weights {small_weights.weight.data}: output = {small_weights(x).item():.1f}\")\n",
    "print(f\"Large weights {large_weights.weight.data}: output = {large_weights(x).item():.1f}\")\n",
    "print()\n",
    "print(\"Large weights → large outputs (strong influence)\")\n",
    "print(\"Small weights → small outputs (weak influence)\")\n",
    "print()\n",
    "print(\"During training, the model learns appropriate weight magnitudes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ce3d8f",
   "metadata": {},
   "source": [
    "### How Weights Change During Training\n",
    "\n",
    "Let's simulate how weights are updated during training to minimize loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91211f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple training example\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a simple model\n",
    "model = nn.Linear(2, 1, bias=False)\n",
    "print(\"Initial weights:\")\n",
    "print(model.weight.data)\n",
    "print()\n",
    "\n",
    "# Training data: y = 2*x1 + 3*x2\n",
    "X = torch.tensor([[1.0, 1.0], [2.0, 1.0], [1.0, 2.0]])\n",
    "y = torch.tensor([[5.0], [7.0], [8.0]])  # True outputs\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train for a few steps\n",
    "print(\"Training...\\n\")\n",
    "for epoch in range(5):\n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    loss = criterion(predictions, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"  Weights: {model.weight.data.squeeze()}\")\n",
    "    print(f\"  Loss: {loss.item():.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"Final weights (should be close to [2.0, 3.0]):\")\n",
    "print(model.weight.data.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908fbe2",
   "metadata": {},
   "source": [
    "### Weight Patterns in Deep Networks\n",
    "\n",
    "In deep networks, weights in different layers learn different types of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470206f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-layer network\n",
    "class MultiLayerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(10, 20)\n",
    "        self.layer2 = nn.Linear(20, 20)\n",
    "        self.layer3 = nn.Linear(20, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "net = MultiLayerNet()\n",
    "\n",
    "print(\"Weight statistics for each layer:\\n\")\n",
    "for name, param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Shape: {param.shape}\")\n",
    "        print(f\"  Mean: {param.mean():.4f}\")\n",
    "        print(f\"  Std: {param.std():.4f}\")\n",
    "        print(f\"  Min: {param.min():.4f}\")\n",
    "        print(f\"  Max: {param.max():.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze weight distribution\n",
    "import torch\n",
    "\n",
    "weights = net.layer1.weight.data.flatten()\n",
    "print(f\"Layer 1 weight distribution:\")\n",
    "print(f\"  Total weights: {len(weights)}\")\n",
    "print(f\"  Positive weights: {(weights > 0).sum().item()}\")\n",
    "print(f\"  Negative weights: {(weights < 0).sum().item()}\")\n",
    "print(f\"  Near-zero weights (|w| < 0.1): {(weights.abs() < 0.1).sum().item()}\")\n",
    "print()\n",
    "print(\"In trained networks:\")\n",
    "print(\"  - Early layers: learn low-level features (edges, textures)\")\n",
    "print(\"  - Middle layers: learn mid-level features (shapes, parts)\")\n",
    "print(\"  - Late layers: learn high-level features (objects, concepts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0629fe40",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**What are Weights?**\n",
    "- Connection strengths between neurons in a neural network\n",
    "- Stored in weight matrices (2D tensors)\n",
    "- The primary learnable parameters in most layers\n",
    "- Determine how inputs are transformed to outputs\n",
    "\n",
    "**Weight Matrix Structure:**\n",
    "- Shape: `(out_features, in_features)` for linear layers\n",
    "- Each row: weights for one output neuron\n",
    "- Each column: how one input affects all outputs\n",
    "- Computation: `output = input @ weight.T + bias`\n",
    "\n",
    "**Weight Interpretation:**\n",
    "- **Positive weights**: Excitatory (amplify signal)\n",
    "- **Negative weights**: Inhibitory (suppress signal)\n",
    "- **Large magnitude**: Strong influence\n",
    "- **Small magnitude**: Weak influence\n",
    "- **Zero weights**: No connection\n",
    "\n",
    "**Weight Patterns:**\n",
    "- **Selection**: One-hot patterns select specific inputs\n",
    "- **Averaging**: Uniform weights compute averages\n",
    "- **Filtering**: Specific patterns detect features\n",
    "- **Transformation**: General patterns transform representations\n",
    "\n",
    "**Training and Weights:**\n",
    "- **Initialization**: Start with random small values\n",
    "- **Learning**: Gradients tell how to adjust weights\n",
    "- **Updates**: Optimizer modifies weights to reduce loss\n",
    "- **Convergence**: Weights stabilize at good values\n",
    "\n",
    "**Weight Properties:**\n",
    "- **Capacity**: More weights = more patterns the model can learn\n",
    "- **Overfitting**: Too many weights can memorize training data\n",
    "- **Regularization**: Penalties on weight magnitudes prevent overfitting\n",
    "- **Sparsity**: Many weights can be near zero after training\n",
    "\n",
    "**Practical Considerations:**\n",
    "- **Memory**: Weights dominate model memory usage\n",
    "- **Computation**: Matrix multiplications with weights are expensive\n",
    "- **Initialization**: Proper initialization is crucial for training\n",
    "- **Monitoring**: Track weight statistics during training\n",
    "\n",
    "**The Big Picture:**\n",
    "\n",
    "Weights are the **knowledge** of a neural network. When you:\n",
    "- **Train** a model: You're finding good weight values\n",
    "- **Save** a model: You're saving the weight values\n",
    "- **Load** a model: You're loading pre-trained weights\n",
    "- **Fine-tune**: You're adjusting existing weights\n",
    "\n",
    "Understanding weights means understanding:\n",
    "- How neural networks process information\n",
    "- What the model has learned\n",
    "- Why certain architectures work\n",
    "- How to debug training problems\n",
    "\n",
    "Every breakthrough in deep learning - from ImageNet to GPT - is fundamentally about finding the right weight values!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5575ced",
   "metadata": {},
   "source": [
    "## Bias Parameters - The Offset Terms\n",
    "\n",
    "**What are Bias Parameters?**\n",
    "\n",
    "Bias parameters are **offset terms** added to the weighted sum in neural network layers. While weights determine how inputs are combined, biases shift the output up or down. In a linear layer, the computation is: `output = input @ weight + bias`\n",
    "\n",
    "Think of bias as:\n",
    "- **Baseline activation**: The output when all inputs are zero\n",
    "- **Threshold adjuster**: Shifts the activation threshold\n",
    "- **Flexibility**: Allows the model to fit data that doesn't pass through the origin\n",
    "\n",
    "**Why do Bias Parameters matter?**\n",
    "\n",
    "Biases are **essential for model flexibility**:\n",
    "- **Non-zero intercepts**: Without bias, all functions must pass through zero\n",
    "- **Activation control**: Bias determines when neurons activate\n",
    "- **Better fitting**: Bias allows models to fit a wider range of patterns\n",
    "- **Convergence**: Proper bias initialization can speed up training\n",
    "\n",
    "**Example**: To learn `y = 2x + 3`, you need:\n",
    "- Weight = 2 (the slope)\n",
    "- Bias = 3 (the intercept)\n",
    "\n",
    "Without bias, you could only learn `y = 2x` (forced through origin)!\n",
    "\n",
    "Let's explore how bias parameters work and why they're crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb67da",
   "metadata": {},
   "source": [
    "### Understanding Bias in Linear Layers\n",
    "\n",
    "Let's see how bias parameters work alongside weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a linear layer with bias\n",
    "layer_with_bias = nn.Linear(3, 2, bias=True)  # bias=True is default\n",
    "\n",
    "print(\"Linear layer with bias:\")\n",
    "print(layer_with_bias)\n",
    "print()\n",
    "\n",
    "# Inspect parameters\n",
    "print(\"Weight matrix:\")\n",
    "print(f\"  Shape: {layer_with_bias.weight.shape}  # (2, 3)\")\n",
    "print(f\"  Values:\\n{layer_with_bias.weight}\")\n",
    "print()\n",
    "\n",
    "print(\"Bias vector:\")\n",
    "print(f\"  Shape: {layer_with_bias.bias.shape}  # (2,)\")\n",
    "print(f\"  Values: {layer_with_bias.bias}\")\n",
    "print()\n",
    "print(\"Note: One bias value per output feature!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with and without bias\n",
    "layer_no_bias = nn.Linear(3, 2, bias=False)\n",
    "\n",
    "print(\"Layer WITHOUT bias:\")\n",
    "print(f\"Parameters: {list(layer_no_bias.named_parameters())}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in layer_no_bias.parameters())}\")\n",
    "print()\n",
    "\n",
    "print(\"Layer WITH bias:\")\n",
    "print(f\"Parameters: {[name for name, _ in layer_with_bias.named_parameters()]}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in layer_with_bias.parameters())}\")\n",
    "print()\n",
    "print(\"Bias adds one parameter per output feature.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a11062",
   "metadata": {},
   "source": [
    "### How Bias Affects Output\n",
    "\n",
    "Let's see exactly how bias shifts the output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21554def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create layers with controlled weights and biases\n",
    "layer = nn.Linear(2, 1)\n",
    "\n",
    "# Set specific values\n",
    "layer.weight.data = torch.tensor([[2.0, 3.0]])\n",
    "layer.bias.data = torch.tensor([5.0])\n",
    "\n",
    "print(\"Layer configuration:\")\n",
    "print(f\"  Weight: {layer.weight.data.squeeze()}\")\n",
    "print(f\"  Bias: {layer.bias.data.item()}\")\n",
    "print()\n",
    "print(\"Formula: output = (2.0 * x1) + (3.0 * x2) + 5.0\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a0b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different inputs\n",
    "inputs = [\n",
    "    torch.tensor([0.0, 0.0]),\n",
    "    torch.tensor([1.0, 0.0]),\n",
    "    torch.tensor([0.0, 1.0]),\n",
    "    torch.tensor([1.0, 1.0]),\n",
    "]\n",
    "\n",
    "print(\"Testing different inputs:\\n\")\n",
    "for x in inputs:\n",
    "    output = layer(x)\n",
    "    print(f\"Input: {x.tolist()}\")\n",
    "    print(f\"  Output: {output.item():.1f}\")\n",
    "    print(f\"  Calculation: (2.0 * {x[0].item()}) + (3.0 * {x[1].item()}) + 5.0 = {output.item():.1f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97843295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key observation: bias is the output when input is zero\n",
    "zero_input = torch.tensor([0.0, 0.0])\n",
    "output_at_zero = layer(zero_input)\n",
    "\n",
    "print(\"Important observation:\")\n",
    "print(f\"When input = {zero_input.tolist()}\")\n",
    "print(f\"Output = {output_at_zero.item():.1f}\")\n",
    "print(f\"Bias = {layer.bias.item():.1f}\")\n",
    "print()\n",
    "print(\"The bias is exactly the output when all inputs are zero!\")\n",
    "print(\"This is why bias is called the 'intercept' or 'offset' term.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557cae7",
   "metadata": {},
   "source": [
    "### With Bias vs Without Bias\n",
    "\n",
    "Let's compare how models behave with and without bias parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b8ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two models: one with bias, one without\n",
    "model_with_bias = nn.Linear(1, 1, bias=True)\n",
    "model_no_bias = nn.Linear(1, 1, bias=False)\n",
    "\n",
    "# Set same weights\n",
    "model_with_bias.weight.data = torch.tensor([[2.0]])\n",
    "model_no_bias.weight.data = torch.tensor([[2.0]])\n",
    "\n",
    "# Set bias\n",
    "model_with_bias.bias.data = torch.tensor([3.0])\n",
    "\n",
    "print(\"Model WITH bias: y = 2x + 3\")\n",
    "print(\"Model WITHOUT bias: y = 2x\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test both models\n",
    "test_inputs = torch.tensor([[0.0], [1.0], [2.0], [3.0]])\n",
    "\n",
    "outputs_with_bias = model_with_bias(test_inputs)\n",
    "outputs_no_bias = model_no_bias(test_inputs)\n",
    "\n",
    "print(\"Comparison:\\n\")\n",
    "print(\"Input  | With Bias (y=2x+3) | Without Bias (y=2x)\")\n",
    "print(\"-------|--------------------|-----------------\")\n",
    "for i, x in enumerate(test_inputs):\n",
    "    print(f\"  {x.item():.1f}  |       {outputs_with_bias[i].item():.1f}          |        {outputs_no_bias[i].item():.1f}\")\n",
    "print()\n",
    "print(\"Without bias, the function MUST pass through the origin (0, 0)!\")\n",
    "print(\"With bias, the function can have any y-intercept.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13907ce",
   "metadata": {},
   "source": [
    "### Learning Bias During Training\n",
    "\n",
    "Let's train a model to learn both weights and bias from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad476841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data: y = 3x + 7 (weight=3, bias=7)\n",
    "torch.manual_seed(42)\n",
    "X_train = torch.randn(100, 1) * 2  # Random inputs\n",
    "y_train = 3 * X_train + 7 + torch.randn(100, 1) * 0.5  # Add noise\n",
    "\n",
    "print(\"Training data: y = 3x + 7 (with noise)\")\n",
    "print(f\"X shape: {X_train.shape}\")\n",
    "print(f\"y shape: {y_train.shape}\")\n",
    "print()\n",
    "\n",
    "# Create model\n",
    "model = nn.Linear(1, 1)\n",
    "print(\"Initial parameters:\")\n",
    "print(f\"  Weight: {model.weight.item():.4f}\")\n",
    "print(f\"  Bias: {model.bias.item():.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"Training...\\n\")\n",
    "for epoch in [0, 50, 100, 200, 500]:\n",
    "    if epoch > 0:\n",
    "        for _ in range(50 if epoch == 50 else (50 if epoch == 100 else (100 if epoch == 200 else 300))):\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_train)\n",
    "            loss = criterion(predictions, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_train)\n",
    "        loss = criterion(predictions, y_train)\n",
    "    \n",
    "    print(f\"Epoch {epoch:3d}: Weight={model.weight.item():6.3f}, Bias={model.bias.item():6.3f}, Loss={loss.item():.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Target: Weight=3.0, Bias=7.0\")\n",
    "print(f\"Learned: Weight={model.weight.item():.3f}, Bias={model.bias.item():.3f}\")\n",
    "print()\n",
    "print(\"The model successfully learned both weight and bias!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fab960",
   "metadata": {},
   "source": [
    "### When to Use Bias (and When Not To)\n",
    "\n",
    "Most of the time you want bias, but there are exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5122499",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"When to USE bias (most cases):\\n\")\n",
    "print(\"✓ Standard fully-connected layers\")\n",
    "print(\"✓ Convolutional layers\")\n",
    "print(\"✓ When data doesn't naturally pass through origin\")\n",
    "print(\"✓ When you want maximum model flexibility\")\n",
    "print(\"✓ Default choice for most architectures\")\n",
    "print()\n",
    "\n",
    "print(\"When to SKIP bias (special cases):\\n\")\n",
    "print(\"✗ After batch normalization (BN provides its own bias)\")\n",
    "print(\"✗ After layer normalization (LN provides its own bias)\")\n",
    "print(\"✗ When you know the function passes through origin\")\n",
    "print(\"✗ To reduce parameters in very large models\")\n",
    "print(\"✗ Some specific architectures (e.g., certain attention mechanisms)\")\n",
    "print()\n",
    "\n",
    "print(\"Example: Layer followed by BatchNorm\")\n",
    "print(\"  nn.Linear(10, 20, bias=False)  # No bias\")\n",
    "print(\"  nn.BatchNorm1d(20)              # BN has its own bias\")\n",
    "print(\"  # Using bias=False avoids redundant parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec533f6",
   "metadata": {},
   "source": [
    "### Bias Initialization\n",
    "\n",
    "How biases are initialized can affect training. Let's explore common strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f29bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default initialization\n",
    "layer = nn.Linear(10, 5)\n",
    "print(\"Default bias initialization:\")\n",
    "print(f\"Bias values: {layer.bias.data}\")\n",
    "print(f\"Mean: {layer.bias.mean():.4f}\")\n",
    "print(f\"Std: {layer.bias.std():.4f}\")\n",
    "print()\n",
    "print(\"PyTorch initializes biases to small random values (uniform distribution)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common initialization strategies\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Zero initialization (most common)\n",
    "layer1 = nn.Linear(10, 5)\n",
    "init.zeros_(layer1.bias)\n",
    "print(\"Zero initialization:\")\n",
    "print(f\"Bias: {layer1.bias.data}\")\n",
    "print(\"Most common choice - let the model learn bias from scratch\")\n",
    "print()\n",
    "\n",
    "# Constant initialization\n",
    "layer2 = nn.Linear(10, 5)\n",
    "init.constant_(layer2.bias, 0.1)\n",
    "print(\"Constant initialization (0.1):\")\n",
    "print(f\"Bias: {layer2.bias.data}\")\n",
    "print(\"Useful for specific activation functions or architectures\")\n",
    "print()\n",
    "\n",
    "# Custom initialization\n",
    "layer3 = nn.Linear(10, 5)\n",
    "layer3.bias.data = torch.tensor([0.0, 0.1, 0.2, 0.3, 0.4])\n",
    "print(\"Custom initialization:\")\n",
    "print(f\"Bias: {layer3.bias.data}\")\n",
    "print(\"For special cases where you have domain knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034bb21",
   "metadata": {},
   "source": [
    "### Practical Example - Classification with Bias\n",
    "\n",
    "Let's see how bias helps in a real classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa5ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create imbalanced binary classification data\n",
    "# Class 0: 90% of data, Class 1: 10% of data\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate data\n",
    "n_samples = 1000\n",
    "X = torch.randn(n_samples, 5)\n",
    "# Create imbalanced labels\n",
    "y = (torch.rand(n_samples) > 0.9).float().unsqueeze(1)\n",
    "\n",
    "print(f\"Dataset: {n_samples} samples, 5 features\")\n",
    "print(f\"Class 0: {(y == 0).sum().item()} samples ({(y == 0).float().mean()*100:.1f}%)\")\n",
    "print(f\"Class 1: {(y == 1).sum().item()} samples ({(y == 1).float().mean()*100:.1f}%)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d3a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with bias\n",
    "model_with_bias = nn.Sequential(\n",
    "    nn.Linear(5, 1, bias=True),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model_with_bias.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Train\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model_with_bias(X)\n",
    "    loss = criterion(predictions, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Check final bias\n",
    "final_bias = model_with_bias[0].bias.item()\n",
    "print(f\"Final bias: {final_bias:.4f}\")\n",
    "print()\n",
    "print(\"For imbalanced data, the bias learns to shift predictions:\")\n",
    "print(f\"  Negative bias ({final_bias:.2f}) → model predicts class 0 more often\")\n",
    "print(\"  This matches the 90/10 class distribution!\")\n",
    "print()\n",
    "print(\"Without bias, the model couldn't learn this baseline preference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e368835d",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**What is Bias?**\n",
    "- Offset term added to weighted sum: `output = input @ weight + bias`\n",
    "- One bias value per output feature\n",
    "- Learnable parameter, updated during training\n",
    "- The output value when all inputs are zero\n",
    "\n",
    "**Why Bias Matters:**\n",
    "- **Flexibility**: Allows functions that don't pass through origin\n",
    "- **Intercept**: Provides the y-intercept in linear functions\n",
    "- **Threshold**: Controls when neurons activate\n",
    "- **Baseline**: Learns baseline predictions for imbalanced data\n",
    "\n",
    "**With vs Without Bias:**\n",
    "- **With bias**: `y = wx + b` (any line)\n",
    "- **Without bias**: `y = wx` (line through origin)\n",
    "- **Default**: Always use bias unless you have a specific reason not to\n",
    "\n",
    "**When to Skip Bias:**\n",
    "- After batch normalization or layer normalization\n",
    "- When function naturally passes through origin\n",
    "- To reduce parameters in very large models\n",
    "- Specific architectural requirements\n",
    "\n",
    "**Bias Initialization:**\n",
    "- **Zero**: Most common, let model learn from scratch\n",
    "- **Small constant**: For specific activations (e.g., 0.01 for ReLU)\n",
    "- **Custom**: Based on domain knowledge or data statistics\n",
    "\n",
    "**Bias in Training:**\n",
    "- Learned alongside weights via backpropagation\n",
    "- Typically converges faster than weights\n",
    "- Can indicate data properties (e.g., class imbalance)\n",
    "- Monitored during training for debugging\n",
    "\n",
    "**Common Patterns:**\n",
    "```python\n",
    "# Standard layer with bias (default)\n",
    "nn.Linear(10, 5, bias=True)\n",
    "\n",
    "# Layer without bias (before normalization)\n",
    "nn.Linear(10, 5, bias=False)\n",
    "nn.BatchNorm1d(5)\n",
    "\n",
    "# Initialize bias to zero\n",
    "layer = nn.Linear(10, 5)\n",
    "nn.init.zeros_(layer.bias)\n",
    "```\n",
    "\n",
    "**The Big Picture:**\n",
    "\n",
    "Bias parameters are the **baseline** or **default** behavior of neurons:\n",
    "- **Weights** determine how inputs affect outputs\n",
    "- **Bias** determines the output when inputs are neutral\n",
    "\n",
    "Together, weights and bias give neural networks the flexibility to learn any linear transformation. Without bias, you'd be missing half the picture!\n",
    "\n",
    "**Remember**: In `y = wx + b`:\n",
    "- `w` (weight) is the slope - how much y changes with x\n",
    "- `b` (bias) is the intercept - where the line crosses the y-axis\n",
    "\n",
    "Both are essential for learning general functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2557afef",
   "metadata": {},
   "source": [
    "## Linear Regression - Your First Neural Network\n",
    "\n",
    "**What is Linear Regression?**\n",
    "\n",
    "Linear regression is the simplest form of supervised learning - and it's actually a neural network! It learns a linear relationship between inputs and outputs: `y = w₁x₁ + w₂x₂ + ... + b`\n",
    "\n",
    "In PyTorch terms, linear regression is just:\n",
    "- A single `nn.Linear` layer (no activation function)\n",
    "- Mean Squared Error (MSE) loss\n",
    "- An optimizer (like SGD or Adam)\n",
    "\n",
    "**Why does Linear Regression matter?**\n",
    "\n",
    "Linear regression is **the foundation** for understanding neural networks:\n",
    "- **Simplest model**: One layer, easy to understand\n",
    "- **Complete workflow**: Shows all training steps\n",
    "- **Building block**: Deep networks are stacks of linear layers + activations\n",
    "- **Practical**: Used for prediction, forecasting, and analysis\n",
    "\n",
    "**Real-world applications**:\n",
    "- Predicting house prices from features\n",
    "- Forecasting sales from historical data\n",
    "- Estimating energy consumption\n",
    "- Any continuous value prediction task\n",
    "\n",
    "Let's build a complete linear regression model from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df793830",
   "metadata": {},
   "source": [
    "### Simple Linear Regression - One Input, One Output\n",
    "\n",
    "Let's start with the simplest case: predicting one value from one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Generate synthetic data: y = 3x + 2 (with noise)\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(100, 1) * 10  # 100 samples, 1 feature\n",
    "y = 3 * X + 2 + torch.randn(100, 1) * 2  # True relationship + noise\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(f\"X shape: {X.shape}  # (100 samples, 1 feature)\")\n",
    "print(f\"y shape: {y.shape}  # (100 samples, 1 output)\")\n",
    "print()\n",
    "print(\"True relationship: y = 3x + 2\")\n",
    "print(f\"Sample data points:\")\n",
    "for i in range(5):\n",
    "    print(f\"  x={X[i].item():6.2f}, y={y[i].item():6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1871f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # 1 input, 1 output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Create model instance\n",
    "model = LinearRegressionModel()\n",
    "print(\"Linear Regression Model:\")\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# Check initial parameters\n",
    "print(\"Initial parameters:\")\n",
    "print(f\"  Weight: {model.linear.weight.item():.4f}\")\n",
    "print(f\"  Bias: {model.linear.bias.item():.4f}\")\n",
    "print()\n",
    "print(\"Goal: Learn weight ≈ 3.0 and bias ≈ 2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)  # Stochastic Gradient Descent\n",
    "\n",
    "print(\"Training setup:\")\n",
    "print(f\"  Loss function: {criterion}\")\n",
    "print(f\"  Optimizer: {optimizer}\")\n",
    "print(f\"  Learning rate: 0.001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024bf0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "\n",
    "print(\"Training...\\n\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    loss = criterion(predictions, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    loss.backward()         # Compute gradients\n",
    "    optimizer.step()        # Update parameters\n",
    "    \n",
    "    # Record loss\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, \"\n",
    "              f\"Weight: {model.linear.weight.item():.4f}, Bias: {model.linear.bias.item():.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a126f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check final parameters\n",
    "print(\"Final learned parameters:\")\n",
    "print(f\"  Weight: {model.linear.weight.item():.4f} (target: 3.0)\")\n",
    "print(f\"  Bias: {model.linear.bias.item():.4f} (target: 2.0)\")\n",
    "print()\n",
    "\n",
    "# Make predictions\n",
    "test_X = torch.tensor([[0.0], [5.0], [10.0]])\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(test_X)\n",
    "\n",
    "print(\"Test predictions:\")\n",
    "for i in range(len(test_X)):\n",
    "    x_val = test_X[i].item()\n",
    "    pred = test_predictions[i].item()\n",
    "    true = 3 * x_val + 2\n",
    "    print(f\"  x={x_val:4.1f}: predicted={pred:6.2f}, true={true:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c579a",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression - Multiple Inputs\n",
    "\n",
    "Real-world problems usually have multiple input features. Let's build a model that predicts from multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe53aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with multiple features\n",
    "# y = 2*x1 + 3*x2 - 1*x3 + 5\n",
    "torch.manual_seed(42)\n",
    "n_samples = 200\n",
    "n_features = 3\n",
    "\n",
    "X_multi = torch.randn(n_samples, n_features)\n",
    "true_weights = torch.tensor([[2.0], [3.0], [-1.0]])\n",
    "true_bias = 5.0\n",
    "\n",
    "y_multi = X_multi @ true_weights + true_bias + torch.randn(n_samples, 1) * 0.5\n",
    "\n",
    "print(\"Multiple regression data:\")\n",
    "print(f\"X shape: {X_multi.shape}  # (200 samples, 3 features)\")\n",
    "print(f\"y shape: {y_multi.shape}  # (200 samples, 1 output)\")\n",
    "print()\n",
    "print(\"True relationship: y = 2*x1 + 3*x2 - 1*x3 + 5\")\n",
    "print(f\"True weights: {true_weights.squeeze()}\")\n",
    "print(f\"True bias: {true_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model\n",
    "model_multi = nn.Linear(3, 1)  # 3 inputs, 1 output\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_multi.parameters(), lr=0.01)  # Adam often works better\n",
    "\n",
    "print(\"Training multiple regression model...\\n\")\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model_multi(X_multi)\n",
    "    loss = criterion(predictions, y_multi)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Learned parameters:\")\n",
    "print(f\"  Weights: {model_multi.weight.data.squeeze()}\")\n",
    "print(f\"  Bias: {model_multi.bias.item():.4f}\")\n",
    "print()\n",
    "print(\"Target parameters:\")\n",
    "print(f\"  Weights: {true_weights.squeeze()}\")\n",
    "print(f\"  Bias: {true_bias:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675bd033",
   "metadata": {},
   "source": [
    "### Proper Training - Train/Test Split\n",
    "\n",
    "In real machine learning, we split data into training and test sets to evaluate how well the model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate larger dataset\n",
    "torch.manual_seed(42)\n",
    "n_total = 1000\n",
    "X_all = torch.randn(n_total, 5) * 2\n",
    "y_all = (X_all @ torch.randn(5, 1) * 2 + 10 + torch.randn(n_total, 1))\n",
    "\n",
    "# Split into train (80%) and test (20%)\n",
    "n_train = 800\n",
    "X_train = X_all[:n_train]\n",
    "y_train = y_all[:n_train]\n",
    "X_test = X_all[n_train:]\n",
    "y_test = y_all[n_train:]\n",
    "\n",
    "print(\"Dataset split:\")\n",
    "print(f\"  Training: {X_train.shape[0]} samples\")\n",
    "print(f\"  Test: {X_test.shape[0]} samples\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495adeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model\n",
    "model_split = nn.Linear(5, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_split.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop with evaluation\n",
    "print(\"Training with train/test split...\\n\")\n",
    "for epoch in range(200):\n",
    "    # Training\n",
    "    model_split.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_pred = model_split(X_train)\n",
    "    train_loss = criterion(train_pred, y_train)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluation (every 50 epochs)\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        model_split.eval()\n",
    "        with torch.no_grad():\n",
    "            test_pred = model_split(X_test)\n",
    "            test_loss = criterion(test_pred, y_test)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss: {train_loss.item():.4f}\")\n",
    "        print(f\"  Test Loss: {test_loss.item():.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef7119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "model_split.eval()\n",
    "with torch.no_grad():\n",
    "    train_pred = model_split(X_train)\n",
    "    test_pred = model_split(X_test)\n",
    "    train_loss = criterion(train_pred, y_train)\n",
    "    test_loss = criterion(test_pred, y_test)\n",
    "\n",
    "print(\"Final Results:\")\n",
    "print(f\"  Training Loss: {train_loss.item():.4f}\")\n",
    "print(f\"  Test Loss: {test_loss.item():.4f}\")\n",
    "print()\n",
    "if test_loss < train_loss * 1.1:\n",
    "    print(\"✓ Model generalizes well! Test loss is close to train loss.\")\n",
    "else:\n",
    "    print(\"⚠ Model might be overfitting. Test loss is much higher than train loss.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a90301",
   "metadata": {},
   "source": [
    "### Complete Example - House Price Prediction\n",
    "\n",
    "Let's build a realistic linear regression model to predict house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate house price data\n",
    "torch.manual_seed(42)\n",
    "n_houses = 500\n",
    "\n",
    "# Features: [square_feet, bedrooms, bathrooms, age, distance_to_city]\n",
    "square_feet = torch.randn(n_houses, 1) * 500 + 2000  # 1500-2500 sq ft\n",
    "bedrooms = torch.randint(2, 6, (n_houses, 1)).float()  # 2-5 bedrooms\n",
    "bathrooms = torch.randint(1, 4, (n_houses, 1)).float()  # 1-3 bathrooms\n",
    "age = torch.rand(n_houses, 1) * 50  # 0-50 years old\n",
    "distance = torch.rand(n_houses, 1) * 30  # 0-30 miles from city\n",
    "\n",
    "# Combine features\n",
    "X_houses = torch.cat([square_feet, bedrooms, bathrooms, age, distance], dim=1)\n",
    "\n",
    "# Generate prices (realistic formula)\n",
    "# Price = 100*sqft + 20000*bedrooms + 15000*bathrooms - 500*age - 2000*distance + noise\n",
    "y_houses = (100 * square_feet + \n",
    "            20000 * bedrooms + \n",
    "            15000 * bathrooms - \n",
    "            500 * age - \n",
    "            2000 * distance + \n",
    "            torch.randn(n_houses, 1) * 10000)  # Add noise\n",
    "\n",
    "print(\"House Price Dataset:\")\n",
    "print(f\"  Samples: {n_houses}\")\n",
    "print(f\"  Features: {X_houses.shape[1]}\")\n",
    "print(f\"    - Square feet\")\n",
    "print(f\"    - Bedrooms\")\n",
    "print(f\"    - Bathrooms\")\n",
    "print(f\"    - Age (years)\")\n",
    "print(f\"    - Distance to city (miles)\")\n",
    "print()\n",
    "print(f\"Sample house:\")\n",
    "print(f\"  Features: {X_houses[0]}\")\n",
    "print(f\"  Price: ${y_houses[0].item():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817be3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features (important for training stability)\n",
    "X_mean = X_houses.mean(dim=0, keepdim=True)\n",
    "X_std = X_houses.std(dim=0, keepdim=True)\n",
    "X_normalized = (X_houses - X_mean) / X_std\n",
    "\n",
    "y_mean = y_houses.mean()\n",
    "y_std = y_houses.std()\n",
    "y_normalized = (y_houses - y_mean) / y_std\n",
    "\n",
    "print(\"Feature normalization:\")\n",
    "print(f\"  Original X range: [{X_houses.min():.1f}, {X_houses.max():.1f}]\")\n",
    "print(f\"  Normalized X range: [{X_normalized.min():.1f}, {X_normalized.max():.1f}]\")\n",
    "print()\n",
    "print(\"Normalization helps training converge faster and more stably!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57323967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "n_train = 400\n",
    "X_train = X_normalized[:n_train]\n",
    "y_train = y_normalized[:n_train]\n",
    "X_test = X_normalized[n_train:]\n",
    "y_test = y_normalized[n_train:]\n",
    "\n",
    "# Create model\n",
    "house_model = nn.Linear(5, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(house_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train\n",
    "print(\"Training house price model...\\n\")\n",
    "for epoch in range(1000):\n",
    "    house_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = house_model(X_train)\n",
    "    loss = criterion(predictions, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        house_model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_pred = house_model(X_test)\n",
    "            test_loss = criterion(test_pred, y_test)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={loss.item():.4f}, Test Loss={test_loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "house_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred_normalized = house_model(X_test)\n",
    "    # Denormalize predictions\n",
    "    test_pred = test_pred_normalized * y_std + y_mean\n",
    "    test_actual = y_test * y_std + y_mean\n",
    "\n",
    "# Show some predictions\n",
    "print(\"Sample predictions on test set:\\n\")\n",
    "print(\"Predicted Price | Actual Price | Error\")\n",
    "print(\"----------------|--------------|-------\")\n",
    "for i in range(10):\n",
    "    pred = test_pred[i].item()\n",
    "    actual = test_actual[i].item()\n",
    "    error = abs(pred - actual)\n",
    "    print(f\"${pred:>13,.0f} | ${actual:>11,.0f} | ${error:>6,.0f}\")\n",
    "\n",
    "# Calculate metrics\n",
    "mae = (test_pred - test_actual).abs().mean()\n",
    "print(f\"\\nMean Absolute Error: ${mae.item():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7659e803",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**What is Linear Regression?**\n",
    "- Simplest neural network: one linear layer, no activation\n",
    "- Learns linear relationship: `y = w₁x₁ + w₂x₂ + ... + b`\n",
    "- Foundation for understanding more complex models\n",
    "\n",
    "**Components:**\n",
    "```python\n",
    "model = nn.Linear(n_features, 1)  # Model\n",
    "criterion = nn.MSELoss()          # Loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Optimizer\n",
    "```\n",
    "\n",
    "**Training Loop:**\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(X)\n",
    "    loss = criterion(predictions, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "1. **Normalize features**: Scale inputs to similar ranges\n",
    "2. **Train/test split**: Evaluate generalization\n",
    "3. **Use Adam optimizer**: Often works better than SGD\n",
    "4. **Monitor both losses**: Watch for overfitting\n",
    "5. **Denormalize predictions**: Convert back to original scale\n",
    "\n",
    "**When to Use Linear Regression:**\n",
    "- Predicting continuous values\n",
    "- Linear relationships in data\n",
    "- Baseline model before trying complex architectures\n",
    "- Interpretable predictions needed\n",
    "\n",
    "**Limitations:**\n",
    "- Can only learn linear relationships\n",
    "- No interaction between features (without manual feature engineering)\n",
    "- Limited capacity for complex patterns\n",
    "\n",
    "**Next Steps:**\n",
    "- Add activation functions → non-linear models\n",
    "- Stack multiple layers → deep neural networks\n",
    "- Add regularization → prevent overfitting\n",
    "\n",
    "**The Big Picture:**\n",
    "\n",
    "Linear regression is your first complete neural network! It demonstrates:\n",
    "- Forward pass (making predictions)\n",
    "- Loss computation (measuring error)\n",
    "- Backward pass (computing gradients)\n",
    "- Parameter updates (learning)\n",
    "\n",
    "Every complex neural network follows this same pattern. Master linear regression, and you understand the foundation of all deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d4b44f",
   "metadata": {},
   "source": [
    "## Feed-Forward Networks - Building Multi-Layer Architectures\n",
    "\n",
    "**What are Feed-Forward Networks?**\n",
    "\n",
    "A feed-forward network (also called a Multi-Layer Perceptron or MLP) is a neural network where information flows in one direction: from input through hidden layers to output, without any cycles or loops. Each layer transforms its input and passes the result to the next layer.\n",
    "\n",
    "**Structure:**\n",
    "- **Input layer**: Receives the raw data\n",
    "- **Hidden layers**: One or more layers that learn intermediate representations\n",
    "- **Output layer**: Produces the final prediction\n",
    "\n",
    "**Why do Feed-Forward Networks matter?**\n",
    "\n",
    "Feed-forward networks are the foundation of deep learning:\n",
    "- They can approximate any continuous function (universal approximation theorem)\n",
    "- Adding layers allows learning hierarchical features (edges → shapes → objects)\n",
    "- They're the building blocks for more complex architectures (CNNs, Transformers)\n",
    "- With enough layers and neurons, they can solve complex problems\n",
    "\n",
    "**Key insight**: The \"deep\" in deep learning refers to having multiple layers. Each layer learns increasingly abstract representations of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eeffef",
   "metadata": {},
   "source": [
    "### Building a Simple Feed-Forward Network\n",
    "\n",
    "Let's build a network with one hidden layer using `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeba04de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Simple feed-forward network: input → hidden → output\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),   # Input layer: 10 features → 20 hidden units\n",
    "    nn.ReLU(),           # Activation function\n",
    "    nn.Linear(20, 1)     # Output layer: 20 hidden units → 1 output\n",
    ")\n",
    "\n",
    "print(\"Simple Feed-Forward Network:\")\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print()\n",
    "\n",
    "# Test with sample input\n",
    "x = torch.randn(5, 10)  # Batch of 5 samples, 10 features each\n",
    "output = model(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c05e24",
   "metadata": {},
   "source": [
    "### Deeper Networks - Multiple Hidden Layers\n",
    "\n",
    "Adding more layers allows the network to learn more complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper network with multiple hidden layers\n",
    "deep_model = nn.Sequential(\n",
    "    nn.Linear(784, 256),   # Input: 784 (e.g., 28x28 image flattened)\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),   # First hidden layer\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),    # Second hidden layer\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)      # Output: 10 classes\n",
    ")\n",
    "\n",
    "print(\"Deep Feed-Forward Network:\")\n",
    "print(deep_model)\n",
    "print()\n",
    "\n",
    "# Analyze the architecture\n",
    "print(\"Layer-by-layer breakdown:\")\n",
    "for i, layer in enumerate(deep_model):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        params = layer.in_features * layer.out_features + layer.out_features\n",
    "        print(f\"Layer {i}: {layer.in_features} → {layer.out_features} ({params:,} params)\")\n",
    "    else:\n",
    "        print(f\"Layer {i}: {layer.__class__.__name__} (no parameters)\")\n",
    "print()\n",
    "\n",
    "total_params = sum(p.numel() for p in deep_model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c5de8",
   "metadata": {},
   "source": [
    "### Custom Feed-Forward Network Class\n",
    "\n",
    "For more control, we can define a custom class inheriting from `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ff05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_sizes: List of hidden layer sizes\n",
    "            output_size: Number of output units\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build layers dynamically\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Add hidden layers with ReLU activation\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Add output layer (no activation)\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        \n",
    "        # Combine into sequential model\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Create a network with custom architecture\n",
    "custom_model = FeedForwardNetwork(\n",
    "    input_size=100,\n",
    "    hidden_sizes=[64, 32, 16],  # Three hidden layers\n",
    "    output_size=5\n",
    ")\n",
    "\n",
    "print(\"Custom Feed-Forward Network:\")\n",
    "print(custom_model)\n",
    "print()\n",
    "\n",
    "# Test it\n",
    "x = torch.randn(10, 100)  # Batch of 10 samples\n",
    "output = custom_model(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621177e",
   "metadata": {},
   "source": [
    "### Understanding Information Flow\n",
    "\n",
    "Let's trace how data flows through a feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd97349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple network and trace the flow\n",
    "class TracedNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(5, 10)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(10, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        print(f\"After fc1: {x.shape}\")\n",
    "        \n",
    "        x = self.relu1(x)\n",
    "        print(f\"After relu1: {x.shape} (ReLU doesn't change shape)\")\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        print(f\"After fc2 (output): {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "traced_model = TracedNetwork()\n",
    "sample_input = torch.randn(2, 5)  # Batch of 2 samples\n",
    "\n",
    "print(\"Tracing information flow:\")\n",
    "output = traced_model(sample_input)\n",
    "print(f\"\\nFinal output:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8bb378",
   "metadata": {},
   "source": [
    "### Why Multiple Layers? The Power of Depth\n",
    "\n",
    "Multiple layers allow networks to learn hierarchical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dffcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"What each layer learns (conceptually):\\n\")\n",
    "print(\"Example: Image Classification\")\n",
    "print(\"  Layer 1: Detects edges and simple patterns\")\n",
    "print(\"  Layer 2: Combines edges into shapes (circles, squares)\")\n",
    "print(\"  Layer 3: Combines shapes into parts (eyes, wheels, windows)\")\n",
    "print(\"  Layer 4: Combines parts into objects (faces, cars, houses)\")\n",
    "print(\"  Output: Classifies the complete object\\n\")\n",
    "\n",
    "print(\"Example: Text Classification\")\n",
    "print(\"  Layer 1: Learns character patterns\")\n",
    "print(\"  Layer 2: Learns word patterns\")\n",
    "print(\"  Layer 3: Learns phrase patterns\")\n",
    "print(\"  Layer 4: Learns sentence meaning\")\n",
    "print(\"  Output: Classifies sentiment or topic\\n\")\n",
    "\n",
    "print(\"Key insight: Each layer builds on the previous layer's representations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f09277a",
   "metadata": {},
   "source": [
    "### Practical Example - Binary Classification\n",
    "\n",
    "Let's train a feed-forward network on a real task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic binary classification data\n",
    "torch.manual_seed(42)\n",
    "n_samples = 1000\n",
    "n_features = 20\n",
    "\n",
    "# Create data with non-linear decision boundary\n",
    "X = torch.randn(n_samples, n_features)\n",
    "# Complex decision rule (non-linear)\n",
    "y = ((X[:, 0]**2 + X[:, 1]**2) > 1.0).float().unsqueeze(1)\n",
    "\n",
    "print(f\"Dataset: {n_samples} samples, {n_features} features\")\n",
    "print(f\"Class distribution: {y.sum().item():.0f} positive, {(n_samples - y.sum()).item():.0f} negative\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434438e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feed-forward network for classification\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(20, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    "    nn.Sigmoid()  # Output probability between 0 and 1\n",
    ")\n",
    "\n",
    "# Train the network\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    # Forward pass\n",
    "    predictions = classifier(X)\n",
    "    loss = criterion(predictions, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # Calculate accuracy\n",
    "        predicted_classes = (predictions > 0.5).float()\n",
    "        accuracy = (predicted_classes == y).float().mean()\n",
    "        print(f\"Epoch {epoch+1}/50 - Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4b576",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- **Feed-forward networks** pass information in one direction: input → hidden layers → output\n",
    "- **Multiple layers** enable learning hierarchical representations (simple → complex features)\n",
    "- **Architecture choices** matter: number of layers, layer sizes, activation functions\n",
    "- **nn.Sequential** is convenient for simple architectures\n",
    "- **Custom nn.Module** classes provide more flexibility\n",
    "- **Deeper networks** can learn more complex patterns but require more data and training time\n",
    "\n",
    "Feed-forward networks are the workhorses of deep learning. Master them, and you have the foundation for understanding more advanced architectures like CNNs, RNNs, and Transformers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6828c3",
   "metadata": {},
   "source": [
    "## Forward Pass - Computing Outputs from Inputs\n",
    "\n",
    "**What is the Forward Pass?**\n",
    "\n",
    "The forward pass is the process of computing a neural network's output by passing input data through the network's layers sequentially. It's called \"forward\" because data flows forward from input to output, applying each layer's transformation along the way.\n",
    "\n",
    "**The Forward Pass Process:**\n",
    "1. Start with input data\n",
    "2. Apply first layer's transformation (weights, bias)\n",
    "3. Apply activation function\n",
    "4. Pass result to next layer\n",
    "5. Repeat until reaching output layer\n",
    "6. Return final output\n",
    "\n",
    "**Why does the Forward Pass matter?**\n",
    "\n",
    "The forward pass is fundamental to neural networks:\n",
    "- **During training**: Compute predictions to calculate loss\n",
    "- **During inference**: Generate predictions for new data\n",
    "- **For gradients**: The forward pass builds the computation graph needed for backpropagation\n",
    "\n",
    "Every time you use a neural network - whether training or making predictions - you're performing a forward pass!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c758bfb7",
   "metadata": {},
   "source": [
    "### Manual Forward Pass - Step by Step\n",
    "\n",
    "Let's manually perform a forward pass to understand what happens at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a simple network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3, 4),  # Layer 1: 3 inputs → 4 outputs\n",
    "    nn.ReLU(),        # Activation\n",
    "    nn.Linear(4, 2)   # Layer 2: 4 inputs → 2 outputs\n",
    ")\n",
    "\n",
    "# Input data\n",
    "x = torch.tensor([[1.0, 2.0, 3.0]])  # Single sample with 3 features\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual forward pass - step by step\n",
    "print(\"Step-by-step forward pass:\\n\")\n",
    "\n",
    "# Step 1: First linear layer\n",
    "layer1 = model[0]\n",
    "z1 = layer1(x)\n",
    "print(f\"After Linear Layer 1: {z1}\")\n",
    "print(f\"Shape: {z1.shape}\")\n",
    "print(f\"Computation: z1 = x @ W1.T + b1\")\n",
    "print()\n",
    "\n",
    "# Step 2: ReLU activation\n",
    "activation = model[1]\n",
    "a1 = activation(z1)\n",
    "print(f\"After ReLU: {a1}\")\n",
    "print(f\"Shape: {a1.shape}\")\n",
    "print(f\"Computation: a1 = max(0, z1)\")\n",
    "print()\n",
    "\n",
    "# Step 3: Second linear layer\n",
    "layer2 = model[2]\n",
    "z2 = layer2(a1)\n",
    "print(f\"After Linear Layer 2 (output): {z2}\")\n",
    "print(f\"Shape: {z2.shape}\")\n",
    "print(f\"Computation: z2 = a1 @ W2.T + b2\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33093a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with automatic forward pass\n",
    "output_auto = model(x)\n",
    "print(\"Automatic forward pass (calling model(x)):\")\n",
    "print(f\"Output: {output_auto}\")\n",
    "print()\n",
    "\n",
    "# Verify they're the same\n",
    "print(f\"Manual and automatic outputs match: {torch.allclose(z2, output_auto)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88dd417",
   "metadata": {},
   "source": [
    "### Batch Forward Pass - Processing Multiple Samples\n",
    "\n",
    "In practice, we process multiple samples at once (batching) for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac33a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch of inputs\n",
    "batch_size = 5\n",
    "x_batch = torch.randn(batch_size, 3)  # 5 samples, 3 features each\n",
    "\n",
    "print(f\"Batch input shape: {x_batch.shape}\")\n",
    "print(f\"Batch input:\\n{x_batch}\")\n",
    "print()\n",
    "\n",
    "# Forward pass on batch\n",
    "output_batch = model(x_batch)\n",
    "print(f\"Batch output shape: {output_batch.shape}\")\n",
    "print(f\"Batch output:\\n{output_batch}\")\n",
    "print()\n",
    "\n",
    "print(\"Key observation: The batch dimension is preserved!\")\n",
    "print(f\"Input: [{batch_size}, 3] → Output: [{batch_size}, 2]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772eda7a",
   "metadata": {},
   "source": [
    "### The forward() Method - How PyTorch Calls It\n",
    "\n",
    "When you call `model(x)`, PyTorch automatically calls the `forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d71917",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(20, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"This method defines the forward pass.\"\"\"\n",
    "        print(\"  forward() method called!\")\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "custom_model = CustomModel()\n",
    "sample_input = torch.randn(3, 10)\n",
    "\n",
    "print(\"Calling model(x):\")\n",
    "output = custom_model(sample_input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"You can also call forward() directly (but don't do this in practice):\")\n",
    "output_direct = custom_model.forward(sample_input)\n",
    "print(f\"Output shape: {output_direct.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Why use model(x) instead of model.forward(x)?\")\n",
    "print(\"  - model(x) calls hooks and other PyTorch machinery\")\n",
    "print(\"  - model(x) is the standard, idiomatic way\")\n",
    "print(\"  - Always use model(x) in your code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7066d00",
   "metadata": {},
   "source": [
    "### Forward Pass in Training vs Inference\n",
    "\n",
    "The forward pass behaves differently during training and inference for some layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3fed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with dropout (behaves differently in train/eval mode)\n",
    "model_with_dropout = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),  # Drops 50% of neurons during training\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "x = torch.randn(3, 10)\n",
    "\n",
    "# Training mode (default)\n",
    "model_with_dropout.train()\n",
    "print(\"Training mode (dropout active):\")\n",
    "output_train1 = model_with_dropout(x)\n",
    "output_train2 = model_with_dropout(x)\n",
    "print(f\"First forward pass: {output_train1[0]}\")\n",
    "print(f\"Second forward pass: {output_train2[0]}\")\n",
    "print(f\"Outputs are different due to random dropout: {not torch.allclose(output_train1, output_train2)}\")\n",
    "print()\n",
    "\n",
    "# Evaluation mode\n",
    "model_with_dropout.eval()\n",
    "print(\"Evaluation mode (dropout disabled):\")\n",
    "output_eval1 = model_with_dropout(x)\n",
    "output_eval2 = model_with_dropout(x)\n",
    "print(f\"First forward pass: {output_eval1[0]}\")\n",
    "print(f\"Second forward pass: {output_eval2[0]}\")\n",
    "print(f\"Outputs are identical (deterministic): {torch.allclose(output_eval1, output_eval2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e509a",
   "metadata": {},
   "source": [
    "### Complete Example - Forward Pass in Training Loop\n",
    "\n",
    "Let's see how the forward pass fits into a complete training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae952467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and data\n",
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(5, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1)\n",
    ")\n",
    "\n",
    "# Synthetic data\n",
    "X_train = torch.randn(100, 5)\n",
    "y_train = torch.randn(100, 1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Training loop with forward pass:\\n\")\n",
    "\n",
    "for epoch in range(3):\n",
    "    # FORWARD PASS - compute predictions\n",
    "    predictions = model(X_train)  # <-- This is the forward pass!\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(predictions, y_train)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nThe forward pass happens every iteration to compute predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68bb206",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- **Forward pass** computes network output by passing data through layers sequentially\n",
    "- **Calling `model(x)`** automatically invokes the `forward()` method\n",
    "- **Each layer** transforms its input: linear layers apply weights/biases, activations add non-linearity\n",
    "- **Batching** processes multiple samples simultaneously for efficiency\n",
    "- **Training mode** vs **eval mode** affects layers like Dropout and BatchNorm\n",
    "- **During training**: Forward pass → compute loss → backward pass → update parameters\n",
    "- **During inference**: Just forward pass to get predictions\n",
    "\n",
    "The forward pass is the \"prediction\" step of neural networks. Understanding it deeply helps you:\n",
    "- Debug shape mismatches\n",
    "- Design custom architectures\n",
    "- Optimize model performance\n",
    "- Understand what your model is computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6d0e26",
   "metadata": {},
   "source": [
    "## Activation Functions - Introducing Non-Linearity\n",
    "\n",
    "**What are activation functions?**\n",
    "\n",
    "Activation functions are non-linear transformations applied to the outputs of neural network layers. They take a number (or tensor of numbers) as input and transform it according to a specific mathematical function. Without activation functions, neural networks would just be a series of linear transformations, which can only learn linear relationships - no matter how many layers you stack!\n",
    "\n",
    "**Why do activation functions matter?**\n",
    "\n",
    "Activation functions are **essential** for neural networks to learn complex patterns:\n",
    "\n",
    "1. **Enable non-linearity**: They allow networks to learn non-linear relationships in data\n",
    "2. **Create decision boundaries**: Help networks separate different classes in classification tasks\n",
    "3. **Control gradient flow**: Affect how gradients propagate during backpropagation\n",
    "4. **Introduce sparsity**: Some activations (like ReLU) can zero out neurons, creating sparse representations\n",
    "\n",
    "**The Universal Approximation Theorem** states that a neural network with at least one hidden layer and a non-linear activation function can approximate any continuous function. Without activation functions, this wouldn't be possible!\n",
    "\n",
    "**Common activation functions in deep learning:**\n",
    "- **ReLU** (Rectified Linear Unit): Most popular, simple and effective\n",
    "- **GeLU** (Gaussian Error Linear Unit): Smooth alternative to ReLU, used in transformers\n",
    "- **Softmax**: Converts logits to probabilities for classification\n",
    "- **Sigmoid**: Squashes values to (0, 1), used for binary classification\n",
    "- **Tanh**: Squashes values to (-1, 1), centered at zero\n",
    "\n",
    "Let's explore the most important activation functions and understand when to use each one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba5e34",
   "metadata": {},
   "source": [
    "### Why Non-Linearity is Essential\n",
    "\n",
    "Let's demonstrate why activation functions are necessary by comparing networks with and without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example: Stacking linear layers WITHOUT activation functions\n",
    "# This is equivalent to a single linear layer!\n",
    "\n",
    "# Create two linear layers\n",
    "layer1 = nn.Linear(10, 20)\n",
    "layer2 = nn.Linear(20, 5)\n",
    "\n",
    "# Sample input\n",
    "x = torch.randn(3, 10)  # 3 samples, 10 features\n",
    "\n",
    "# Forward pass without activation\n",
    "h = layer1(x)  # Hidden layer output\n",
    "y = layer2(h)  # Final output\n",
    "\n",
    "print(\"Network without activation functions:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Hidden shape: {h.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print()\n",
    "print(\"Mathematical fact: layer2(layer1(x)) = layer2(W1 @ x + b1)\")\n",
    "print(\"                                      = W2 @ (W1 @ x + b1) + b2\")\n",
    "print(\"                                      = (W2 @ W1) @ x + (W2 @ b1 + b2)\")\n",
    "print(\"                                      = W_combined @ x + b_combined\")\n",
    "print()\n",
    "print(\"This is just a SINGLE linear transformation!\")\n",
    "print(\"No matter how many layers you stack, it's still linear.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf647f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's add activation functions\n",
    "# This creates TRUE depth and non-linearity!\n",
    "\n",
    "# Create a network with activations\n",
    "layer1 = nn.Linear(10, 20)\n",
    "activation1 = nn.ReLU()\n",
    "layer2 = nn.Linear(20, 5)\n",
    "\n",
    "# Forward pass WITH activation\n",
    "x = torch.randn(3, 10)\n",
    "h = layer1(x)\n",
    "h_activated = activation1(h)  # Apply non-linear activation!\n",
    "y = layer2(h_activated)\n",
    "\n",
    "print(\"Network WITH activation functions:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Hidden (before activation) shape: {h.shape}\")\n",
    "print(f\"Hidden (after activation) shape: {h_activated.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print()\n",
    "print(\"Now the network can learn non-linear patterns!\")\n",
    "print(\"The ReLU activation breaks the linear chain.\")\n",
    "print()\n",
    "\n",
    "# Show the effect of ReLU\n",
    "print(\"Effect of ReLU on hidden layer:\")\n",
    "print(f\"Before ReLU - negative values present: {(h < 0).sum().item()} negative values\")\n",
    "print(f\"After ReLU - all negative values zeroed: {(h_activated < 0).sum().item()} negative values\")\n",
    "print(f\"ReLU kept {(h_activated > 0).sum().item()} positive values out of {h.numel()} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a811e",
   "metadata": {},
   "source": [
    "### Choosing the Right Activation Function\n",
    "\n",
    "Different activation functions have different properties. Here's a quick guide:\n",
    "\n",
    "| Activation | Formula | Range | Use Case | Pros | Cons |\n",
    "|------------|---------|-------|----------|------|------|\n",
    "| **ReLU** | max(0, x) | [0, ∞) | Hidden layers (default choice) | Fast, simple, reduces vanishing gradients | Dead neurons (outputs always 0) |\n",
    "| **GeLU** | x * Φ(x) | (-∞, ∞) | Transformers, modern architectures | Smooth, better gradients | Slightly slower than ReLU |\n",
    "| **Softmax** | e^xi / Σe^xj | (0, 1), sum=1 | Output layer (multi-class) | Produces probabilities | Only for final layer |\n",
    "| **Sigmoid** | 1/(1+e^-x) | (0, 1) | Binary classification output | Bounded output | Vanishing gradients |\n",
    "| **Tanh** | (e^x-e^-x)/(e^x+e^-x) | (-1, 1) | Hidden layers (less common now) | Zero-centered | Vanishing gradients |\n",
    "\n",
    "**General guidelines:**\n",
    "- **Start with ReLU** for hidden layers - it's the default choice for good reason\n",
    "- **Use GeLU** in transformer architectures or when you want smoother gradients\n",
    "- **Use Softmax** for multi-class classification output layers\n",
    "- **Use Sigmoid** for binary classification or when you need outputs in (0, 1)\n",
    "- **Avoid Sigmoid/Tanh** in deep networks (vanishing gradient problem)\n",
    "\n",
    "Let's explore the most important activation functions in detail!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bf7a77",
   "metadata": {},
   "source": [
    "### ReLU - Rectified Linear Unit\n",
    "\n",
    "**What is ReLU?**\n",
    "\n",
    "ReLU (Rectified Linear Unit) is the most widely used activation function in deep learning. It has an incredibly simple formula:\n",
    "\n",
    "```\n",
    "ReLU(x) = max(0, x)\n",
    "```\n",
    "\n",
    "In other words:\n",
    "- If the input is positive, output the input unchanged\n",
    "- If the input is negative or zero, output zero\n",
    "\n",
    "**Why does ReLU matter?**\n",
    "\n",
    "ReLU revolutionized deep learning when it was popularized around 2012:\n",
    "\n",
    "1. **Solves vanishing gradients**: Unlike sigmoid/tanh, ReLU's gradient is either 0 or 1, not a tiny fraction\n",
    "2. **Computationally efficient**: Just a simple comparison and max operation\n",
    "3. **Sparse activation**: Zeros out negative values, creating sparse representations\n",
    "4. **Biological inspiration**: Similar to how neurons in the brain activate\n",
    "\n",
    "**When to use ReLU:**\n",
    "- Default choice for hidden layers in feedforward networks\n",
    "- Convolutional neural networks (CNNs)\n",
    "- Most deep learning architectures\n",
    "\n",
    "**Potential issues:**\n",
    "- **Dying ReLU problem**: Neurons can get stuck outputting zero for all inputs\n",
    "- **Not zero-centered**: Outputs are always >= 0\n",
    "- **Unbounded**: No upper limit on outputs\n",
    "\n",
    "Let's see ReLU in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e5329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a ReLU activation function\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# Test with various inputs\n",
    "x = torch.tensor([-3.0, -1.5, -0.5, 0.0, 0.5, 1.5, 3.0])\n",
    "\n",
    "print(\"ReLU Activation:\")\n",
    "print(f\"Input:  {x}\")\n",
    "print(f\"Output: {relu(x)}\")\n",
    "print()\n",
    "print(\"Notice:\")\n",
    "print(\"  - All negative values become 0\")\n",
    "print(\"  - Positive values pass through unchanged\")\n",
    "print(\"  - Zero stays zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb38f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ReLU behavior with a range of values\n",
    "x = torch.linspace(-5, 5, 100)  # 100 values from -5 to 5\n",
    "y = relu(x)\n",
    "\n",
    "print(\"ReLU applied to range [-5, 5]:\")\n",
    "print(f\"Input range: [{x.min():.2f}, {x.max():.2f}]\")\n",
    "print(f\"Output range: [{y.min():.2f}, {y.max():.2f}]\")\n",
    "print()\n",
    "print(f\"Number of zeros in output: {(y == 0).sum().item()} out of {len(y)}\")\n",
    "print(f\"Number of positive values: {(y > 0).sum().item()} out of {len(y)}\")\n",
    "print()\n",
    "print(\"This demonstrates ReLU's sparsity - about half the outputs are zero!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e3f28b",
   "metadata": {},
   "source": [
    "### Using ReLU in Neural Networks\n",
    "\n",
    "ReLU is typically applied after linear layers to introduce non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b49d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple network with ReLU activations\n",
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(10, 20)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(20, 10)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(10, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)  # Apply ReLU after first layer\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)  # Apply ReLU after second layer\n",
    "        x = self.layer3(x)  # No ReLU after final layer\n",
    "        return x\n",
    "\n",
    "# Create model and test\n",
    "model = SimpleNetwork()\n",
    "x = torch.randn(3, 10)  # Batch of 3 samples\n",
    "\n",
    "print(\"Network with ReLU activations:\")\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb5004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Using nn.Sequential with ReLU\n",
    "model_sequential = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 5)\n",
    ")\n",
    "\n",
    "print(\"Sequential model with ReLU:\")\n",
    "print(model_sequential)\n",
    "print()\n",
    "\n",
    "# Test it\n",
    "output = model_sequential(x)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da24c1e",
   "metadata": {},
   "source": [
    "### Functional API: torch.nn.functional.relu\n",
    "\n",
    "You can also use ReLU as a function instead of a module. This is useful when you don't want to create a separate layer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69103c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Using functional ReLU\n",
    "x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "# Method 1: Using nn.ReLU() module\n",
    "relu_module = nn.ReLU()\n",
    "output1 = relu_module(x)\n",
    "\n",
    "# Method 2: Using F.relu() function\n",
    "output2 = F.relu(x)\n",
    "\n",
    "print(\"Comparing nn.ReLU() vs F.relu():\")\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"nn.ReLU() output: {output1}\")\n",
    "print(f\"F.relu() output:  {output2}\")\n",
    "print(f\"Same result? {torch.equal(output1, output2)}\")\n",
    "print()\n",
    "print(\"Use nn.ReLU() when defining models (cleaner)\")\n",
    "print(\"Use F.relu() for quick operations or in forward() methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e62789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using F.relu in forward method\n",
    "class NetworkWithFunctionalReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(10, 20)\n",
    "        self.layer2 = nn.Linear(20, 5)\n",
    "        # No need to define ReLU modules!\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)  # Apply ReLU functionally\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "model = NetworkWithFunctionalReLU()\n",
    "print(\"Model using functional ReLU:\")\n",
    "print(model)\n",
    "print()\n",
    "print(\"Notice: No ReLU layers in the model definition!\")\n",
    "print(\"ReLU is applied in the forward() method using F.relu()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692564de",
   "metadata": {},
   "source": [
    "### The Dying ReLU Problem\n",
    "\n",
    "One issue with ReLU is the \"dying ReLU\" problem: if a neuron's output is always negative, ReLU will always output zero, and the gradient will also be zero. This means the neuron stops learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97215176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating the dying ReLU problem\n",
    "# If weights push all activations negative, the neuron \"dies\"\n",
    "\n",
    "# Create a neuron that outputs negative values\n",
    "layer = nn.Linear(5, 1)\n",
    "# Manually set weights to produce negative outputs\n",
    "with torch.no_grad():\n",
    "    layer.weight.fill_(-1.0)\n",
    "    layer.bias.fill_(-10.0)\n",
    "\n",
    "# Test with positive inputs\n",
    "x = torch.randn(10, 5).abs()  # All positive inputs\n",
    "\n",
    "# Forward pass\n",
    "pre_activation = layer(x)\n",
    "post_activation = F.relu(pre_activation)\n",
    "\n",
    "print(\"Dying ReLU demonstration:\")\n",
    "print(f\"Pre-activation values (sample): {pre_activation[:5].squeeze()}\")\n",
    "print(f\"All pre-activation values negative? {(pre_activation < 0).all().item()}\")\n",
    "print()\n",
    "print(f\"Post-activation values (sample): {post_activation[:5].squeeze()}\")\n",
    "print(f\"All post-activation values zero? {(post_activation == 0).all().item()}\")\n",
    "print()\n",
    "print(\"This neuron is 'dead' - it always outputs zero!\")\n",
    "print(\"Gradient will be zero, so it won't learn.\")\n",
    "print()\n",
    "print(\"Solutions:\")\n",
    "print(\"  - Use proper weight initialization\")\n",
    "print(\"  - Use smaller learning rates\")\n",
    "print(\"  - Try Leaky ReLU or other variants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c952f6c",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- **ReLU formula**: max(0, x) - zeros out negative values\n",
    "- **Most popular activation**: Default choice for hidden layers\n",
    "- **Advantages**: Fast, simple, solves vanishing gradients, creates sparsity\n",
    "- **Disadvantages**: Dying ReLU problem, not zero-centered, unbounded\n",
    "- **Two ways to use**: `nn.ReLU()` module or `F.relu()` function\n",
    "- **Typical usage**: Apply after linear layers in hidden layers\n",
    "- **Not for output layers**: Use softmax, sigmoid, or no activation for outputs\n",
    "\n",
    "ReLU's simplicity and effectiveness make it the go-to activation function for most deep learning applications. Despite its simplicity (or perhaps because of it), ReLU has been instrumental in training very deep networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f517d0",
   "metadata": {},
   "source": [
    "### GeLU - Gaussian Error Linear Unit\n",
    "\n",
    "**What is GeLU?**\n",
    "\n",
    "GeLU (Gaussian Error Linear Unit) is a smooth activation function that has become popular in modern architectures, especially transformers (like BERT and GPT). The formula is:\n",
    "\n",
    "```\n",
    "GeLU(x) = x * Φ(x)\n",
    "```\n",
    "\n",
    "Where Φ(x) is the cumulative distribution function of the standard Gaussian distribution. In simpler terms, GeLU weights inputs by their value, with a smooth transition around zero.\n",
    "\n",
    "**Intuition**: GeLU can be thought of as a smooth version of ReLU. Instead of abruptly cutting off at zero, it smoothly transitions from negative to positive values.\n",
    "\n",
    "**Why does GeLU matter?**\n",
    "\n",
    "1. **Smooth gradients**: Unlike ReLU's sharp corner at zero, GeLU is smooth everywhere\n",
    "2. **Better performance**: Often outperforms ReLU in transformer architectures\n",
    "3. **Stochastic regularization**: Can be interpreted as applying dropout that depends on the input\n",
    "4. **Non-monotonic**: Unlike ReLU, GeLU can output small negative values for negative inputs\n",
    "\n",
    "**When to use GeLU:**\n",
    "- Transformer models (BERT, GPT, etc.)\n",
    "- Modern NLP architectures\n",
    "- When you want smoother gradients than ReLU\n",
    "- Computer vision models (increasingly common)\n",
    "\n",
    "Let's explore GeLU and compare it with ReLU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a GeLU activation function\n",
    "gelu = nn.GELU()\n",
    "\n",
    "# Test with various inputs\n",
    "x = torch.tensor([-3.0, -1.5, -0.5, 0.0, 0.5, 1.5, 3.0])\n",
    "\n",
    "print(\"GeLU Activation:\")\n",
    "print(f\"Input:  {x}\")\n",
    "print(f\"Output: {gelu(x)}\")\n",
    "print()\n",
    "print(\"Notice:\")\n",
    "print(\"  - Negative values become small (but not exactly zero)\")\n",
    "print(\"  - Positive values pass through (slightly modified)\")\n",
    "print(\"  - Smooth transition around zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a386447e",
   "metadata": {},
   "source": [
    "### Comparing ReLU and GeLU\n",
    "\n",
    "Let's directly compare how ReLU and GeLU handle the same inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ca9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create both activation functions\n",
    "relu = nn.ReLU()\n",
    "gelu = nn.GELU()\n",
    "\n",
    "# Test with a range of values\n",
    "x = torch.linspace(-3, 3, 13)  # -3 to 3 in steps of 0.5\n",
    "\n",
    "relu_output = relu(x)\n",
    "gelu_output = gelu(x)\n",
    "\n",
    "print(\"ReLU vs GeLU Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Input':>8} | {'ReLU':>10} | {'GeLU':>10} | {'Difference':>12}\")\n",
    "print(\"-\"*60)\n",
    "for i in range(len(x)):\n",
    "    diff = gelu_output[i] - relu_output[i]\n",
    "    print(f\"{x[i]:>8.2f} | {relu_output[i]:>10.4f} | {gelu_output[i]:>10.4f} | {diff:>12.4f}\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"Key observations:\")\n",
    "print(\"  - ReLU: Hard cutoff at 0 (all negatives become exactly 0)\")\n",
    "print(\"  - GeLU: Smooth transition (negatives become small but not exactly 0)\")\n",
    "print(\"  - For large positive values, both are similar\")\n",
    "print(\"  - For negative values, GeLU allows small negative outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77586c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the behavior with more data points\n",
    "x = torch.linspace(-5, 5, 1000)\n",
    "relu_output = relu(x)\n",
    "gelu_output = gelu(x)\n",
    "\n",
    "print(\"Statistical comparison over range [-5, 5]:\")\n",
    "print()\n",
    "print(\"ReLU:\")\n",
    "print(f\"  Min: {relu_output.min():.4f}\")\n",
    "print(f\"  Max: {relu_output.max():.4f}\")\n",
    "print(f\"  Zeros: {(relu_output == 0).sum().item()} out of {len(relu_output)}\")\n",
    "print(f\"  Negative values: {(relu_output < 0).sum().item()}\")\n",
    "print()\n",
    "print(\"GeLU:\")\n",
    "print(f\"  Min: {gelu_output.min():.4f}\")\n",
    "print(f\"  Max: {gelu_output.max():.4f}\")\n",
    "print(f\"  Exact zeros: {(gelu_output == 0).sum().item()}\")\n",
    "print(f\"  Negative values: {(gelu_output < 0).sum().item()}\")\n",
    "print()\n",
    "print(\"GeLU is smoother - no exact zeros, allows small negative values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09e94a9",
   "metadata": {},
   "source": [
    "### Using GeLU in Neural Networks\n",
    "\n",
    "GeLU is used exactly like ReLU - just swap `nn.ReLU()` for `nn.GELU()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6bb17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network with GeLU activations (like in transformers)\n",
    "class TransformerStyleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(512, 2048)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layer2 = nn.Linear(2048, 512)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # This is similar to the feedforward network in transformers\n",
    "        x = self.layer1(x)\n",
    "        x = self.gelu(x)  # GeLU activation\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "model = TransformerStyleNetwork()\n",
    "print(\"Transformer-style network with GeLU:\")\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# Test it\n",
    "x = torch.randn(2, 512)  # Batch of 2, 512 features\n",
    "output = model(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print()\n",
    "print(\"This pattern (Linear -> GeLU -> Linear) is used in transformer feedforward layers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing networks with ReLU vs GeLU\n",
    "model_relu = nn.Sequential(\n",
    "    nn.Linear(100, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10)\n",
    ")\n",
    "\n",
    "model_gelu = nn.Sequential(\n",
    "    nn.Linear(100, 200),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(200, 100),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(100, 10)\n",
    ")\n",
    "\n",
    "# Test with same input\n",
    "x = torch.randn(5, 100)\n",
    "\n",
    "output_relu = model_relu(x)\n",
    "output_gelu = model_gelu(x)\n",
    "\n",
    "print(\"Comparing ReLU vs GeLU networks:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print()\n",
    "print(f\"ReLU network output shape: {output_relu.shape}\")\n",
    "print(f\"ReLU output sample: {output_relu[0, :5]}\")\n",
    "print()\n",
    "print(f\"GeLU network output shape: {output_gelu.shape}\")\n",
    "print(f\"GeLU output sample: {output_gelu[0, :5]}\")\n",
    "print()\n",
    "print(\"Both produce valid outputs, but GeLU often trains better in practice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b84441",
   "metadata": {},
   "source": [
    "### Functional API: torch.nn.functional.gelu\n",
    "\n",
    "Like ReLU, GeLU can also be used as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6bba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "# Method 1: Using nn.GELU() module\n",
    "gelu_module = nn.GELU()\n",
    "output1 = gelu_module(x)\n",
    "\n",
    "# Method 2: Using F.gelu() function\n",
    "output2 = F.gelu(x)\n",
    "\n",
    "print(\"Comparing nn.GELU() vs F.gelu():\")\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"nn.GELU() output: {output1}\")\n",
    "print(f\"F.gelu() output:  {output2}\")\n",
    "print(f\"Same result? {torch.allclose(output1, output2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72866bd",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- **GeLU formula**: x * Φ(x) where Φ is the Gaussian CDF\n",
    "- **Smooth activation**: No sharp corners like ReLU\n",
    "- **Popular in transformers**: Used in BERT, GPT, and other modern architectures\n",
    "- **Advantages**: Smooth gradients, often better performance, allows small negative values\n",
    "- **Disadvantages**: Slightly more expensive to compute than ReLU\n",
    "- **Usage**: Drop-in replacement for ReLU - just use `nn.GELU()` instead of `nn.ReLU()`\n",
    "\n",
    "**When to choose GeLU over ReLU:**\n",
    "- Building transformer models (it's the standard choice)\n",
    "- When you want smoother gradients\n",
    "- When ReLU's hard cutoff at zero is problematic\n",
    "- In modern architectures where it's shown to work better\n",
    "\n",
    "**When to stick with ReLU:**\n",
    "- When computational efficiency is critical\n",
    "- In simpler architectures where ReLU works fine\n",
    "- When following established architectures that use ReLU\n",
    "\n",
    "GeLU represents the evolution of activation functions - it keeps ReLU's benefits while adding smoothness that helps with gradient flow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f0d16c",
   "metadata": {},
   "source": [
    "### Softmax - Converting Logits to Probabilities\n",
    "\n",
    "**What is Softmax?**\n",
    "\n",
    "Softmax is a special activation function that converts a vector of numbers (called logits) into a probability distribution. The formula for each element is:\n",
    "\n",
    "```\n",
    "Softmax(xi) = exp(xi) / Σ exp(xj)\n",
    "```\n",
    "\n",
    "In other words:\n",
    "1. Take the exponential of each element\n",
    "2. Divide each by the sum of all exponentials\n",
    "\n",
    "**Key properties of Softmax output:**\n",
    "- All values are between 0 and 1\n",
    "- All values sum to exactly 1\n",
    "- Larger inputs get larger probabilities\n",
    "- It's a \"soft\" version of argmax (hence the name)\n",
    "\n",
    "**Why does Softmax matter?**\n",
    "\n",
    "Softmax is **essential for multi-class classification**:\n",
    "\n",
    "1. **Produces probabilities**: Converts raw scores to interpretable probabilities\n",
    "2. **Mutually exclusive classes**: Perfect for \"pick one class\" problems\n",
    "3. **Differentiable**: Unlike argmax, softmax has gradients for training\n",
    "4. **Amplifies differences**: Larger logits get exponentially larger probabilities\n",
    "\n",
    "**When to use Softmax:**\n",
    "- **Output layer** of multi-class classification networks\n",
    "- Attention mechanisms in transformers\n",
    "- Any time you need to convert scores to probabilities\n",
    "\n",
    "**Important**: Softmax is typically used ONLY in the output layer, not in hidden layers!\n",
    "\n",
    "Let's see Softmax in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62dbb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a Softmax activation\n",
    "# dim=-1 means apply softmax along the last dimension\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "# Example: Logits for a 3-class classification problem\n",
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "\n",
    "print(\"Softmax Activation:\")\n",
    "print(f\"Input (logits):  {logits}\")\n",
    "print(f\"Output (probabilities): {softmax(logits)}\")\n",
    "print()\n",
    "print(\"Properties:\")\n",
    "probs = softmax(logits)\n",
    "print(f\"  All values in [0, 1]? {(probs >= 0).all() and (probs <= 1).all()}\")\n",
    "print(f\"  Sum to 1? {probs.sum():.6f}\")\n",
    "print(f\"  Largest logit (2.0) gets largest probability: {probs[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea256d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual calculation to understand the formula\n",
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "\n",
    "print(\"Manual Softmax calculation:\")\n",
    "print(f\"Logits: {logits}\")\n",
    "print()\n",
    "\n",
    "# Step 1: Compute exponentials\n",
    "exp_logits = torch.exp(logits)\n",
    "print(f\"Step 1 - Exponentials: {exp_logits}\")\n",
    "print(f\"  exp(2.0) = {exp_logits[0]:.4f}\")\n",
    "print(f\"  exp(1.0) = {exp_logits[1]:.4f}\")\n",
    "print(f\"  exp(0.1) = {exp_logits[2]:.4f}\")\n",
    "print()\n",
    "\n",
    "# Step 2: Sum of exponentials\n",
    "sum_exp = exp_logits.sum()\n",
    "print(f\"Step 2 - Sum of exponentials: {sum_exp:.4f}\")\n",
    "print()\n",
    "\n",
    "# Step 3: Divide each by the sum\n",
    "probabilities = exp_logits / sum_exp\n",
    "print(f\"Step 3 - Probabilities: {probabilities}\")\n",
    "print(f\"  {exp_logits[0]:.4f} / {sum_exp:.4f} = {probabilities[0]:.4f}\")\n",
    "print(f\"  {exp_logits[1]:.4f} / {sum_exp:.4f} = {probabilities[1]:.4f}\")\n",
    "print(f\"  {exp_logits[2]:.4f} / {sum_exp:.4f} = {probabilities[2]:.4f}\")\n",
    "print()\n",
    "\n",
    "# Verify it matches nn.Softmax\n",
    "softmax_result = softmax(logits)\n",
    "print(f\"Using nn.Softmax: {softmax_result}\")\n",
    "print(f\"Match? {torch.allclose(probabilities, softmax_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed7b107",
   "metadata": {},
   "source": [
    "### Softmax with Batches\n",
    "\n",
    "In practice, you'll apply softmax to batches of predictions. The `dim` parameter controls which dimension to normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d70186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch of logits: 4 samples, 5 classes each\n",
    "batch_logits = torch.tensor([\n",
    "    [2.0, 1.0, 0.1, -1.0, -2.0],  # Sample 1\n",
    "    [0.5, 0.5, 0.5, 0.5, 0.5],    # Sample 2 (all equal)\n",
    "    [5.0, 1.0, 1.0, 1.0, 1.0],    # Sample 3 (one dominant)\n",
    "    [-1.0, -1.0, -1.0, -1.0, -1.0] # Sample 4 (all equal, negative)\n",
    "])\n",
    "\n",
    "print(\"Batch of logits (4 samples, 5 classes):\")\n",
    "print(batch_logits)\n",
    "print(f\"Shape: {batch_logits.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf96dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply softmax along the class dimension (dim=1 or dim=-1)\n",
    "softmax = nn.Softmax(dim=1)  # Normalize across classes for each sample\n",
    "probabilities = softmax(batch_logits)\n",
    "\n",
    "print(\"Probabilities after softmax:\")\n",
    "print(probabilities)\n",
    "print()\n",
    "\n",
    "# Verify properties for each sample\n",
    "print(\"Verification:\")\n",
    "for i in range(len(probabilities)):\n",
    "    sample_probs = probabilities[i]\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Probabilities: {sample_probs}\")\n",
    "    print(f\"  Sum: {sample_probs.sum():.6f}\")\n",
    "    print(f\"  Max probability: {sample_probs.max():.4f} (class {sample_probs.argmax().item()})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bcd774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: The dim parameter matters!\n",
    "print(\"Understanding the dim parameter:\")\n",
    "print()\n",
    "\n",
    "# dim=1: Softmax across classes (what we usually want)\n",
    "softmax_dim1 = nn.Softmax(dim=1)\n",
    "probs_dim1 = softmax_dim1(batch_logits)\n",
    "print(\"Softmax with dim=1 (across classes):\")\n",
    "print(f\"Each row sums to 1: {probs_dim1.sum(dim=1)}\")\n",
    "print(\"This is correct for classification!\")\n",
    "print()\n",
    "\n",
    "# dim=0: Softmax across samples (usually NOT what we want)\n",
    "softmax_dim0 = nn.Softmax(dim=0)\n",
    "probs_dim0 = softmax_dim0(batch_logits)\n",
    "print(\"Softmax with dim=0 (across samples):\")\n",
    "print(f\"Each column sums to 1: {probs_dim0.sum(dim=0)}\")\n",
    "print(\"This is usually NOT what you want for classification!\")\n",
    "print()\n",
    "print(\"Rule: For classification, use dim=1 (or dim=-1) to normalize across classes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56669c42",
   "metadata": {},
   "source": [
    "### Softmax in Multi-Class Classification\n",
    "\n",
    "Here's how softmax is used in a complete classification network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015480c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple classification network\n",
    "class ClassificationNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        logits = self.layer2(x)  # Raw scores\n",
    "        probabilities = self.softmax(logits)  # Convert to probabilities\n",
    "        return probabilities\n",
    "\n",
    "# Create model for 10-class classification\n",
    "model = ClassificationNetwork(input_size=20, hidden_size=50, num_classes=10)\n",
    "\n",
    "# Test with a batch of 5 samples\n",
    "x = torch.randn(5, 20)\n",
    "predictions = model(x)\n",
    "\n",
    "print(\"Classification network output:\")\n",
    "print(f\"Input shape: {x.shape}  # (5 samples, 20 features)\")\n",
    "print(f\"Output shape: {predictions.shape}  # (5 samples, 10 class probabilities)\")\n",
    "print()\n",
    "print(\"Sample predictions:\")\n",
    "print(predictions[0])\n",
    "print(f\"Sum: {predictions[0].sum():.6f}\")\n",
    "print()\n",
    "\n",
    "# Get predicted classes\n",
    "predicted_classes = predictions.argmax(dim=1)\n",
    "print(f\"Predicted classes: {predicted_classes}\")\n",
    "print()\n",
    "for i in range(len(predictions)):\n",
    "    pred_class = predicted_classes[i].item()\n",
    "    confidence = predictions[i, pred_class].item()\n",
    "    print(f\"Sample {i}: Predicted class {pred_class} with {confidence:.2%} confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28942de1",
   "metadata": {},
   "source": [
    "### Important: Softmax and Loss Functions\n",
    "\n",
    "**Critical note**: When training classification networks, you typically **don't** include softmax in your model!\n",
    "\n",
    "Why? Because PyTorch's `CrossEntropyLoss` includes softmax internally for numerical stability. If you apply softmax before the loss, you'll get incorrect gradients.\n",
    "\n",
    "**Best practice:**\n",
    "- **During training**: Output raw logits, use `CrossEntropyLoss`\n",
    "- **During inference**: Apply softmax to get probabilities\n",
    "\n",
    "Let's see the correct pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT: Model outputs logits (no softmax)\n",
    "class ClassificationNetworkCorrect(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes)\n",
    "        # No softmax in the model!\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        logits = self.layer2(x)  # Return raw logits\n",
    "        return logits\n",
    "\n",
    "model = ClassificationNetworkCorrect(20, 50, 10)\n",
    "x = torch.randn(5, 20)\n",
    "logits = model(x)\n",
    "\n",
    "print(\"Model outputs (logits):\")\n",
    "print(logits[0])\n",
    "print(f\"Sum: {logits[0].sum():.4f}  # NOT 1.0 - these are raw scores!\")\n",
    "print()\n",
    "\n",
    "# For training: Use CrossEntropyLoss (includes softmax)\n",
    "targets = torch.tensor([3, 7, 2, 9, 1])  # True class labels\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(logits, targets)\n",
    "print(f\"Training loss: {loss.item():.4f}\")\n",
    "print(\"CrossEntropyLoss applies softmax internally!\")\n",
    "print()\n",
    "\n",
    "# For inference: Apply softmax manually\n",
    "softmax = nn.Softmax(dim=1)\n",
    "probabilities = softmax(logits)\n",
    "print(\"Inference probabilities:\")\n",
    "print(probabilities[0])\n",
    "print(f\"Sum: {probabilities[0].sum():.6f}  # Now sums to 1.0!\")\n",
    "print()\n",
    "predicted_classes = probabilities.argmax(dim=1)\n",
    "print(f\"Predicted classes: {predicted_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c941cd0b",
   "metadata": {},
   "source": [
    "### Functional API: torch.nn.functional.softmax\n",
    "\n",
    "Like other activations, softmax can be used as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1], [1.0, 3.0, 0.5]])\n",
    "\n",
    "# Method 1: Using nn.Softmax() module\n",
    "softmax_module = nn.Softmax(dim=1)\n",
    "output1 = softmax_module(logits)\n",
    "\n",
    "# Method 2: Using F.softmax() function\n",
    "output2 = F.softmax(logits, dim=1)\n",
    "\n",
    "print(\"Comparing nn.Softmax() vs F.softmax():\")\n",
    "print(f\"Logits:\\n{logits}\")\n",
    "print()\n",
    "print(f\"nn.Softmax() output:\\n{output1}\")\n",
    "print()\n",
    "print(f\"F.softmax() output:\\n{output2}\")\n",
    "print()\n",
    "print(f\"Same result? {torch.allclose(output1, output2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f2c9e6",
   "metadata": {},
   "source": [
    "### Advanced: Temperature Scaling\n",
    "\n",
    "Softmax can be modified with a \"temperature\" parameter to control how \"sharp\" the probability distribution is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature scaling: softmax(logits / temperature)\n",
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "\n",
    "print(\"Effect of temperature on softmax:\")\n",
    "print(f\"Logits: {logits}\")\n",
    "print()\n",
    "\n",
    "# Low temperature (< 1): Sharper distribution\n",
    "temp_low = 0.5\n",
    "probs_low = F.softmax(logits / temp_low, dim=0)\n",
    "print(f\"Temperature = {temp_low} (sharp):\")\n",
    "print(f\"  Probabilities: {probs_low}\")\n",
    "print(f\"  Max probability: {probs_low.max():.4f}\")\n",
    "print()\n",
    "\n",
    "# Normal temperature (= 1): Standard softmax\n",
    "temp_normal = 1.0\n",
    "probs_normal = F.softmax(logits / temp_normal, dim=0)\n",
    "print(f\"Temperature = {temp_normal} (normal):\")\n",
    "print(f\"  Probabilities: {probs_normal}\")\n",
    "print(f\"  Max probability: {probs_normal.max():.4f}\")\n",
    "print()\n",
    "\n",
    "# High temperature (> 1): Softer distribution\n",
    "temp_high = 2.0\n",
    "probs_high = F.softmax(logits / temp_high, dim=0)\n",
    "print(f\"Temperature = {temp_high} (soft):\")\n",
    "print(f\"  Probabilities: {probs_high}\")\n",
    "print(f\"  Max probability: {probs_high.max():.4f}\")\n",
    "print()\n",
    "print(\"Lower temperature → more confident (peaked) distribution\")\n",
    "print(\"Higher temperature → less confident (uniform) distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db4924",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- **Softmax formula**: exp(xi) / Σ exp(xj) - converts logits to probabilities\n",
    "- **Output properties**: All values in [0, 1], sum to exactly 1\n",
    "- **Primary use**: Output layer of multi-class classification networks\n",
    "- **dim parameter**: Usually dim=1 or dim=-1 to normalize across classes\n",
    "- **Training vs Inference**:\n",
    "  - Training: Output logits, use `CrossEntropyLoss` (includes softmax)\n",
    "  - Inference: Apply softmax to get probabilities\n",
    "- **Don't double-apply**: `CrossEntropyLoss` includes softmax internally!\n",
    "- **Temperature scaling**: Divide logits by temperature to control sharpness\n",
    "\n",
    "**Common mistakes to avoid:**\n",
    "1. ❌ Applying softmax before `CrossEntropyLoss`\n",
    "2. ❌ Using wrong dim parameter (dim=0 instead of dim=1)\n",
    "3. ❌ Using softmax in hidden layers (use ReLU/GeLU instead)\n",
    "4. ❌ Forgetting to apply softmax during inference\n",
    "\n",
    "**Correct pattern:**\n",
    "```python\n",
    "# Model outputs logits (no softmax)\n",
    "logits = model(x)\n",
    "\n",
    "# Training: Use CrossEntropyLoss\n",
    "loss = nn.CrossEntropyLoss()(logits, targets)\n",
    "\n",
    "# Inference: Apply softmax\n",
    "probabilities = F.softmax(logits, dim=1)\n",
    "predicted_class = probabilities.argmax(dim=1)\n",
    "```\n",
    "\n",
    "Softmax is the standard way to convert raw network outputs into interpretable probabilities for classification tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69e8ec",
   "metadata": {},
   "source": [
    "## Embedding - Converting Tokens to Vectors\n",
    "\n",
    "**What is an embedding?**\n",
    "\n",
    "An embedding is a learned mapping from discrete tokens (like words, characters, or categories) to continuous vector representations. Think of it as a lookup table where each token has an associated vector:\n",
    "\n",
    "```\n",
    "Token ID → Dense Vector\n",
    "   0     → [0.2, -0.5, 0.8, ...]\n",
    "   1     → [-0.3, 0.7, -0.1, ...]\n",
    "   2     → [0.9, 0.1, -0.4, ...]\n",
    "```\n",
    "\n",
    "**Why do embeddings matter?**\n",
    "\n",
    "Embeddings are **fundamental** to modern NLP and many other domains:\n",
    "\n",
    "1. **Convert discrete to continuous**: Neural networks need continuous inputs, not discrete tokens\n",
    "2. **Learn semantic relationships**: Similar tokens get similar vectors (e.g., \"cat\" and \"dog\")\n",
    "3. **Dimensionality reduction**: Represent large vocabularies (50k+ words) in compact spaces (256-1024 dims)\n",
    "4. **Trainable**: Embedding vectors are learned during training to capture task-relevant patterns\n",
    "\n",
    "**Common uses:**\n",
    "- **Word embeddings**: Map words to vectors in NLP models\n",
    "- **Token embeddings**: Map subword tokens in transformers (BERT, GPT)\n",
    "- **Categorical embeddings**: Map categories in recommendation systems\n",
    "- **Position embeddings**: Encode positions in sequences\n",
    "\n",
    "Let's explore how embeddings work in PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create an embedding layer\n",
    "# Parameters: num_embeddings (vocabulary size), embedding_dim (vector size)\n",
    "vocab_size = 10  # We have 10 different tokens\n",
    "embedding_dim = 4  # Each token maps to a 4-dimensional vector\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "print(\"Embedding layer:\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "print(f\"Total parameters: {vocab_size * embedding_dim}\")\n",
    "print()\n",
    "\n",
    "# The embedding layer contains a weight matrix\n",
    "print(\"Embedding weight matrix shape:\", embedding.weight.shape)\n",
    "print(\"This is a lookup table with one vector per token!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up embeddings for specific tokens\n",
    "# Token IDs must be integers in range [0, vocab_size)\n",
    "token_ids = torch.tensor([0, 2, 5, 1])\n",
    "\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print()\n",
    "\n",
    "# Look up the embeddings\n",
    "embedded = embedding(token_ids)\n",
    "\n",
    "print(\"Embedded vectors:\")\n",
    "print(embedded)\n",
    "print(f\"Shape: {embedded.shape}  # (4 tokens, 4 dimensions)\")\n",
    "print()\n",
    "\n",
    "# Each token gets its own vector\n",
    "print(\"Token 0 → vector:\", embedded[0])\n",
    "print(\"Token 2 → vector:\", embedded[1])\n",
    "print(\"Token 5 → vector:\", embedded[2])\n",
    "print(\"Token 1 → vector:\", embedded[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64404228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embedding is just a lookup in the weight matrix\n",
    "print(\"Embedding lookup is just indexing:\")\n",
    "print()\n",
    "\n",
    "token_id = 3\n",
    "print(f\"Token ID: {token_id}\")\n",
    "print()\n",
    "\n",
    "# Method 1: Using the embedding layer\n",
    "embedded_vector = embedding(torch.tensor([token_id]))\n",
    "print(f\"Using embedding layer: {embedded_vector.squeeze()}\")\n",
    "print()\n",
    "\n",
    "# Method 2: Direct lookup in weight matrix\n",
    "direct_lookup = embedding.weight[token_id]\n",
    "print(f\"Direct weight lookup:  {direct_lookup}\")\n",
    "print()\n",
    "\n",
    "print(f\"Same result? {torch.allclose(embedded_vector.squeeze(), direct_lookup)}\")\n",
    "print()\n",
    "print(\"Embedding is just a learnable lookup table!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54cf948",
   "metadata": {},
   "source": [
    "### Embedding Batches of Sequences\n",
    "\n",
    "In practice, you'll embed batches of sequences (like sentences). Each sequence is a list of token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of sequences\n",
    "# Shape: (batch_size, sequence_length)\n",
    "# Each element is a token ID\n",
    "batch_sequences = torch.tensor([\n",
    "    [1, 4, 2, 7, 0],  # Sequence 1: 5 tokens\n",
    "    [3, 8, 5, 2, 9],  # Sequence 2: 5 tokens\n",
    "    [0, 1, 1, 4, 6],  # Sequence 3: 5 tokens\n",
    "])\n",
    "\n",
    "print(\"Batch of sequences (token IDs):\")\n",
    "print(batch_sequences)\n",
    "print(f\"Shape: {batch_sequences.shape}  # (3 sequences, 5 tokens each)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fdcbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the entire batch\n",
    "embedded_batch = embedding(batch_sequences)\n",
    "\n",
    "print(\"Embedded batch:\")\n",
    "print(f\"Shape: {embedded_batch.shape}  # (3 sequences, 5 tokens, 4 dimensions)\")\n",
    "print()\n",
    "print(\"First sequence, first token embedding:\")\n",
    "print(embedded_batch[0, 0])\n",
    "print()\n",
    "print(\"This is the embedding for token ID\", batch_sequences[0, 0].item())\n",
    "print()\n",
    "\n",
    "# Verify it matches direct lookup\n",
    "token_id = batch_sequences[0, 0].item()\n",
    "direct = embedding.weight[token_id]\n",
    "print(f\"Direct lookup: {direct}\")\n",
    "print(f\"Match? {torch.allclose(embedded_batch[0, 0], direct)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c28ad9",
   "metadata": {},
   "source": [
    "### Using Embeddings in a Neural Network\n",
    "\n",
    "Embeddings are typically the first layer in NLP models, converting token IDs to vectors that subsequent layers can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb81964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text classification model with embeddings\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        # Embedding layer: converts token IDs to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Process the embedded sequence\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        # token_ids shape: (batch_size, sequence_length)\n",
    "        \n",
    "        # Embed tokens\n",
    "        embedded = self.embedding(token_ids)\n",
    "        # embedded shape: (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        # Average over sequence length (simple pooling)\n",
    "        pooled = embedded.mean(dim=1)\n",
    "        # pooled shape: (batch_size, embedding_dim)\n",
    "        \n",
    "        # Classification layers\n",
    "        x = self.fc1(pooled)\n",
    "        x = self.relu(x)\n",
    "        logits = self.fc2(x)\n",
    "        # logits shape: (batch_size, num_classes)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create model\n",
    "model = TextClassifier(\n",
    "    vocab_size=1000,    # 1000 different words\n",
    "    embedding_dim=128,  # Each word → 128-dim vector\n",
    "    hidden_dim=64,\n",
    "    num_classes=5       # 5 categories\n",
    ")\n",
    "\n",
    "print(\"Text classification model:\")\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# Test with a batch\n",
    "batch = torch.randint(0, 1000, (4, 10))  # 4 sequences, 10 tokens each\n",
    "output = model(batch)\n",
    "\n",
    "print(f\"Input shape: {batch.shape}  # (4 sequences, 10 tokens)\")\n",
    "print(f\"Output shape: {output.shape}  # (4 sequences, 5 class logits)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07676263",
   "metadata": {},
   "source": [
    "### Loading Pre-trained Embeddings\n",
    "\n",
    "You can initialize embeddings with pre-trained vectors (like Word2Vec or GloVe) instead of random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding with custom initialization\n",
    "vocab_size = 5\n",
    "embedding_dim = 3\n",
    "\n",
    "# Create embedding layer\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "print(\"Random initialization:\")\n",
    "print(embedding.weight)\n",
    "print()\n",
    "\n",
    "# Load pre-trained vectors (simulated here)\n",
    "pretrained_vectors = torch.tensor([\n",
    "    [0.1, 0.2, 0.3],  # Token 0\n",
    "    [0.4, 0.5, 0.6],  # Token 1\n",
    "    [0.7, 0.8, 0.9],  # Token 2\n",
    "    [1.0, 1.1, 1.2],  # Token 3\n",
    "    [1.3, 1.4, 1.5],  # Token 4\n",
    "])\n",
    "\n",
    "# Load the pre-trained vectors\n",
    "embedding.weight.data = pretrained_vectors\n",
    "\n",
    "print(\"After loading pre-trained vectors:\")\n",
    "print(embedding.weight)\n",
    "print()\n",
    "print(\"Now the embeddings start with meaningful values!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca8c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing embeddings (preventing updates during training)\n",
    "# Useful when you want to keep pre-trained embeddings fixed\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "print(\"Before freezing:\")\n",
    "print(f\"Requires grad? {embedding.weight.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# Freeze the embeddings\n",
    "embedding.weight.requires_grad = False\n",
    "\n",
    "print(\"After freezing:\")\n",
    "print(f\"Requires grad? {embedding.weight.requires_grad}\")\n",
    "print()\n",
    "print(\"Now the embeddings won't be updated during training!\")\n",
    "print(\"Useful for keeping pre-trained embeddings fixed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5046f",
   "metadata": {},
   "source": [
    "### Padding Index\n",
    "\n",
    "When working with variable-length sequences, you often pad them to the same length. The `padding_idx` parameter tells the embedding layer to always output zeros for that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding with padding index\n",
    "vocab_size = 10\n",
    "embedding_dim = 4\n",
    "padding_idx = 0  # Token ID 0 is reserved for padding\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "\n",
    "print(\"Embedding with padding_idx=0:\")\n",
    "print()\n",
    "\n",
    "# The padding token always has zero embedding\n",
    "print(\"Embedding for padding token (ID 0):\")\n",
    "print(embedding(torch.tensor([0])))\n",
    "print(\"All zeros!\")\n",
    "print()\n",
    "\n",
    "# Other tokens have normal embeddings\n",
    "print(\"Embedding for regular token (ID 5):\")\n",
    "print(embedding(torch.tensor([5])))\n",
    "print(\"Non-zero values!\")\n",
    "print()\n",
    "\n",
    "# Example with padded sequences\n",
    "sequences = torch.tensor([\n",
    "    [3, 5, 7, 2, 0, 0],  # Sequence 1: 4 real tokens, 2 padding\n",
    "    [1, 4, 0, 0, 0, 0],  # Sequence 2: 2 real tokens, 4 padding\n",
    "])\n",
    "\n",
    "embedded = embedding(sequences)\n",
    "print(\"Padded sequences embedded:\")\n",
    "print(f\"Shape: {embedded.shape}\")\n",
    "print()\n",
    "print(\"Last token of first sequence (padding):\")\n",
    "print(embedded[0, -1])  # Should be all zeros\n",
    "print(\"All zeros? {}\".format((embedded[0, -1] == 0).all().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b366e93",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- **Embedding**: Learnable lookup table mapping token IDs to dense vectors\n",
    "- **Creation**: `nn.Embedding(vocab_size, embedding_dim)`\n",
    "- **Input**: Integer tensor of token IDs, shape (batch_size, sequence_length)\n",
    "- **Output**: Float tensor of vectors, shape (batch_size, sequence_length, embedding_dim)\n",
    "- **Trainable**: Embedding vectors are learned during training\n",
    "- **Pre-trained**: Can initialize with pre-trained vectors (Word2Vec, GloVe, etc.)\n",
    "- **Freezing**: Set `requires_grad=False` to prevent updates\n",
    "- **Padding**: Use `padding_idx` to always output zeros for padding tokens\n",
    "\n",
    "**Common patterns:**\n",
    "```python\n",
    "# Basic embedding\n",
    "embedding = nn.Embedding(vocab_size=10000, embedding_dim=256)\n",
    "embedded = embedding(token_ids)  # (batch, seq_len) → (batch, seq_len, 256)\n",
    "\n",
    "# With padding\n",
    "embedding = nn.Embedding(10000, 256, padding_idx=0)\n",
    "\n",
    "# Load pre-trained\n",
    "embedding.weight.data = pretrained_vectors\n",
    "embedding.weight.requires_grad = False  # Freeze\n",
    "```\n",
    "\n",
    "Embeddings are the bridge between discrete tokens and continuous neural networks, enabling models to learn rich representations of words, characters, or any categorical data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6a7b6",
   "metadata": {},
   "source": [
    "## LayerNorm - Layer Normalization\n",
    "\n",
    "**What is Layer Normalization?**\n",
    "\n",
    "Layer Normalization (LayerNorm) is a technique that normalizes activations across the feature dimension for each sample independently. For each sample, it:\n",
    "\n",
    "1. Computes the mean and standard deviation across all features\n",
    "2. Normalizes to have mean=0 and std=1\n",
    "3. Applies learned scale (γ) and shift (β) parameters\n",
    "\n",
    "Formula: `LayerNorm(x) = γ * (x - μ) / σ + β`\n",
    "\n",
    "Where:\n",
    "- μ = mean across features\n",
    "- σ = standard deviation across features\n",
    "- γ, β = learnable parameters\n",
    "\n",
    "**Why does LayerNorm matter?**\n",
    "\n",
    "LayerNorm is **essential** for training deep networks, especially transformers:\n",
    "\n",
    "1. **Training stability**: Prevents activations from exploding or vanishing\n",
    "2. **Faster convergence**: Networks train faster with normalized activations\n",
    "3. **Reduces internal covariate shift**: Stabilizes the distribution of layer inputs\n",
    "4. **Works with small batches**: Unlike BatchNorm, doesn't depend on batch statistics\n",
    "5. **Standard in transformers**: Used in BERT, GPT, and virtually all modern NLP models\n",
    "\n",
    "**LayerNorm vs BatchNorm:**\n",
    "- **BatchNorm**: Normalizes across the batch dimension (for each feature)\n",
    "- **LayerNorm**: Normalizes across the feature dimension (for each sample)\n",
    "- **LayerNorm advantage**: Works with batch size = 1, better for RNNs and transformers\n",
    "\n",
    "Let's explore how LayerNorm works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519df67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a LayerNorm layer\n",
    "# Parameter: normalized_shape - the shape of the features to normalize\n",
    "feature_dim = 5\n",
    "layer_norm = nn.LayerNorm(feature_dim)\n",
    "\n",
    "print(\"LayerNorm layer:\")\n",
    "print(f\"Normalized shape: {feature_dim}\")\n",
    "print(f\"Learnable parameters: γ (scale) and β (shift)\")\n",
    "print()\n",
    "\n",
    "# LayerNorm has two learnable parameters\n",
    "print(\"Initial parameters:\")\n",
    "print(f\"γ (weight): {layer_norm.weight}  # Initialized to 1\")\n",
    "print(f\"β (bias): {layer_norm.bias}  # Initialized to 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f06eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LayerNorm to a single sample\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(f\"Mean: {x.mean():.4f}\")\n",
    "print(f\"Std: {x.std(unbiased=False):.4f}\")\n",
    "print()\n",
    "\n",
    "# Apply LayerNorm\n",
    "normalized = layer_norm(x)\n",
    "\n",
    "print(\"After LayerNorm:\")\n",
    "print(normalized)\n",
    "print(f\"Mean: {normalized.mean():.6f}  # Close to 0\")\n",
    "print(f\"Std: {normalized.std(unbiased=False):.6f}  # Close to 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8fd543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual LayerNorm calculation to understand the formula\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "print(\"Manual LayerNorm calculation:\")\n",
    "print(f\"Input: {x}\")\n",
    "print()\n",
    "\n",
    "# Step 1: Compute mean\n",
    "mean = x.mean()\n",
    "print(f\"Step 1 - Mean: {mean:.4f}\")\n",
    "print()\n",
    "\n",
    "# Step 2: Compute standard deviation\n",
    "std = x.std(unbiased=False)  # Population std, not sample std\n",
    "print(f\"Step 2 - Std: {std:.4f}\")\n",
    "print()\n",
    "\n",
    "# Step 3: Normalize\n",
    "normalized = (x - mean) / std\n",
    "print(f\"Step 3 - Normalized: {normalized}\")\n",
    "print(f\"  Mean: {normalized.mean():.6f}\")\n",
    "print(f\"  Std: {normalized.std(unbiased=False):.6f}\")\n",
    "print()\n",
    "\n",
    "# Step 4: Apply scale (γ) and shift (β)\n",
    "# With default initialization (γ=1, β=0), output = normalized\n",
    "gamma = torch.ones(5)\n",
    "beta = torch.zeros(5)\n",
    "output = gamma * normalized + beta\n",
    "print(f\"Step 4 - After scale and shift: {output}\")\n",
    "print()\n",
    "\n",
    "# Compare with nn.LayerNorm\n",
    "layer_norm = nn.LayerNorm(5)\n",
    "ln_output = layer_norm(x)\n",
    "print(f\"Using nn.LayerNorm: {ln_output}\")\n",
    "print(f\"Match? {torch.allclose(output, ln_output, atol=1e-5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528f079a",
   "metadata": {},
   "source": [
    "### LayerNorm with Batches\n",
    "\n",
    "LayerNorm normalizes each sample independently, which is perfect for batched data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c547a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch of samples\n",
    "batch = torch.tensor([\n",
    "    [1.0, 2.0, 3.0, 4.0, 5.0],    # Sample 1\n",
    "    [10.0, 20.0, 30.0, 40.0, 50.0],  # Sample 2 (larger values)\n",
    "    [-5.0, -3.0, 0.0, 3.0, 5.0],  # Sample 3 (different range)\n",
    "])\n",
    "\n",
    "print(\"Batch of samples:\")\n",
    "print(batch)\n",
    "print(f\"Shape: {batch.shape}  # (3 samples, 5 features)\")\n",
    "print()\n",
    "\n",
    "# Statistics before normalization\n",
    "print(\"Before LayerNorm:\")\n",
    "for i in range(len(batch)):\n",
    "    print(f\"Sample {i}: mean={batch[i].mean():.2f}, std={batch[i].std(unbiased=False):.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LayerNorm\n",
    "layer_norm = nn.LayerNorm(5)\n",
    "normalized_batch = layer_norm(batch)\n",
    "\n",
    "print(\"After LayerNorm:\")\n",
    "print(normalized_batch)\n",
    "print()\n",
    "\n",
    "# Statistics after normalization\n",
    "print(\"Statistics after LayerNorm:\")\n",
    "for i in range(len(normalized_batch)):\n",
    "    sample = normalized_batch[i]\n",
    "    print(f\"Sample {i}: mean={sample.mean():.6f}, std={sample.std(unbiased=False):.6f}\")\n",
    "print()\n",
    "print(\"Each sample is independently normalized to mean≈0, std≈1!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36583036",
   "metadata": {},
   "source": [
    "### LayerNorm in Neural Networks\n",
    "\n",
    "LayerNorm is typically applied after linear layers or attention layers in transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a95fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer-style block with LayerNorm\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        # Feedforward layers\n",
    "        self.ff1 = nn.Linear(d_model, d_ff)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.ff2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Feedforward with residual connection\n",
    "        residual = x\n",
    "        x = self.ff1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.ff2(x)\n",
    "        \n",
    "        # Add residual and normalize\n",
    "        x = self.layer_norm(x + residual)\n",
    "        return x\n",
    "\n",
    "# Create a transformer block\n",
    "block = TransformerBlock(d_model=512, d_ff=2048)\n",
    "\n",
    "print(\"Transformer block with LayerNorm:\")\n",
    "print(block)\n",
    "print()\n",
    "\n",
    "# Test it\n",
    "x = torch.randn(2, 10, 512)  # (batch=2, seq_len=10, d_model=512)\n",
    "output = block(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print()\n",
    "print(\"This pattern (FF + Residual + LayerNorm) is standard in transformers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f839634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing with and without LayerNorm\n",
    "class NetworkWithoutNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class NetworkWithNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(100, 100)\n",
    "        self.ln1 = nn.LayerNorm(100)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.ln2 = nn.LayerNorm(100)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.ln1(x)  # Normalize after linear layer\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.ln2(x)  # Normalize after linear layer\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model_without = NetworkWithoutNorm()\n",
    "model_with = NetworkWithNorm()\n",
    "\n",
    "x = torch.randn(5, 100)\n",
    "\n",
    "output_without = model_without(x)\n",
    "output_with = model_with(x)\n",
    "\n",
    "print(\"Network without LayerNorm:\")\n",
    "print(f\"  Output mean: {output_without.mean():.4f}\")\n",
    "print(f\"  Output std: {output_without.std():.4f}\")\n",
    "print()\n",
    "print(\"Network with LayerNorm:\")\n",
    "print(f\"  Output mean: {output_with.mean():.4f}\")\n",
    "print(f\"  Output std: {output_with.std():.4f}\")\n",
    "print()\n",
    "print(\"LayerNorm helps keep activations in a reasonable range!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a63a61",
   "metadata": {},
   "source": [
    "### Understanding normalized_shape\n",
    "\n",
    "The `normalized_shape` parameter specifies which dimensions to normalize over. It can be an integer or a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c097eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different normalized_shape examples\n",
    "\n",
    "# Example 1: Normalize over last dimension only\n",
    "x = torch.randn(2, 3, 4)  # (batch, seq, features)\n",
    "ln = nn.LayerNorm(4)  # Normalize over the last dimension (features)\n",
    "output = ln(x)\n",
    "\n",
    "print(\"Example 1: normalized_shape=4\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Normalizes over dimension of size 4\")\n",
    "print()\n",
    "\n",
    "# Example 2: Normalize over last two dimensions\n",
    "x = torch.randn(2, 3, 4)  # (batch, seq, features)\n",
    "ln = nn.LayerNorm([3, 4])  # Normalize over last two dimensions\n",
    "output = ln(x)\n",
    "\n",
    "print(\"Example 2: normalized_shape=[3, 4]\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Normalizes over dimensions of size (3, 4)\")\n",
    "print()\n",
    "\n",
    "# Most common: Just normalize over feature dimension\n",
    "print(\"Most common usage: nn.LayerNorm(feature_dim)\")\n",
    "print(\"This normalizes each sample's features independently.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc820944",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- **LayerNorm**: Normalizes activations across features for each sample\n",
    "- **Formula**: γ * (x - μ) / σ + β, where μ and σ are computed per sample\n",
    "- **Parameters**: γ (scale) and β (shift) are learned during training\n",
    "- **Benefits**: Training stability, faster convergence, works with any batch size\n",
    "- **Standard in transformers**: Used in BERT, GPT, and all modern NLP models\n",
    "- **Placement**: Typically after linear/attention layers, often with residual connections\n",
    "\n",
    "**LayerNorm vs BatchNorm:**\n",
    "| Aspect | LayerNorm | BatchNorm |\n",
    "|--------|-----------|----------|\n",
    "| Normalizes over | Features (per sample) | Batch (per feature) |\n",
    "| Batch size dependency | No | Yes |\n",
    "| Best for | Transformers, RNNs, small batches | CNNs, large batches |\n",
    "| Train/eval difference | No | Yes (uses running stats) |\n",
    "\n",
    "**Common pattern:**\n",
    "```python\n",
    "# Transformer-style: Linear → Residual → LayerNorm\n",
    "residual = x\n",
    "x = linear_layer(x)\n",
    "x = layer_norm(x + residual)\n",
    "\n",
    "# Or: LayerNorm → Linear (pre-norm)\n",
    "x = layer_norm(x)\n",
    "x = linear_layer(x)\n",
    "```\n",
    "\n",
    "LayerNorm is a key ingredient in training deep networks, especially transformers. It's one of the reasons modern NLP models can be trained so effectively!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcf73eb",
   "metadata": {},
   "source": [
    "## Dropout - Regularization Through Random Deactivation\n",
    "\n",
    "**What is Dropout?**\n",
    "\n",
    "Dropout is a regularization technique that randomly \"drops out\" (sets to zero) a fraction of neurons during training. For each training step:\n",
    "\n",
    "1. Randomly select a fraction `p` of neurons to drop\n",
    "2. Set their outputs to zero\n",
    "3. Scale the remaining outputs by 1/(1-p) to maintain expected value\n",
    "\n",
    "During inference (evaluation), dropout is turned off - all neurons are active.\n",
    "\n",
    "**Why does Dropout matter?**\n",
    "\n",
    "Dropout is one of the most effective regularization techniques in deep learning:\n",
    "\n",
    "1. **Prevents overfitting**: Forces the network to not rely on any single neuron\n",
    "2. **Ensemble effect**: Training with dropout is like training many sub-networks\n",
    "3. **Improves generalization**: Models perform better on unseen data\n",
    "4. **Simple to implement**: Just add `nn.Dropout()` layers\n",
    "5. **Widely used**: Standard in most deep learning architectures\n",
    "\n",
    "**When to use Dropout:**\n",
    "- After fully connected layers (common: p=0.5)\n",
    "- After convolutional layers (common: p=0.2-0.3)\n",
    "- In transformer feedforward layers\n",
    "- When your model is overfitting (training accuracy >> validation accuracy)\n",
    "\n",
    "**When NOT to use Dropout:**\n",
    "- In very small networks (may hurt performance)\n",
    "- When you have very little data (other regularization may be better)\n",
    "- In the output layer\n",
    "\n",
    "Let's see Dropout in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33fe771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a Dropout layer\n",
    "# p=0.5 means 50% of neurons will be dropped during training\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "print(\"Dropout layer with p=0.5:\")\n",
    "print(f\"Drop probability: 50%\")\n",
    "print(f\"Keep probability: 50%\")\n",
    "print()\n",
    "\n",
    "# Test input\n",
    "x = torch.ones(10)  # All ones for easy visualization\n",
    "print(f\"Input: {x}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3ecdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dropout in training mode\n",
    "dropout.train()  # Set to training mode\n",
    "\n",
    "print(\"Dropout in TRAINING mode:\")\n",
    "print()\n",
    "\n",
    "# Apply dropout multiple times to see randomness\n",
    "for i in range(3):\n",
    "    output = dropout(x)\n",
    "    num_zeros = (output == 0).sum().item()\n",
    "    print(f\"Trial {i+1}: {output}\")\n",
    "    print(f\"  Zeros: {num_zeros}/10, Non-zeros: {10-num_zeros}/10\")\n",
    "    print()\n",
    "\n",
    "print(\"Notice:\")\n",
    "print(\"  - About 50% of values are zero (dropped)\")\n",
    "print(\"  - Non-zero values are scaled up (≈2.0 instead of 1.0)\")\n",
    "print(\"  - Different neurons are dropped each time (random)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae604fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dropout in evaluation mode\n",
    "dropout.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"Dropout in EVALUATION mode:\")\n",
    "print()\n",
    "\n",
    "# Apply dropout multiple times\n",
    "for i in range(3):\n",
    "    output = dropout(x)\n",
    "    num_zeros = (output == 0).sum().item()\n",
    "    print(f\"Trial {i+1}: {output}\")\n",
    "    print(f\"  Zeros: {num_zeros}/10\")\n",
    "    print()\n",
    "\n",
    "print(\"Notice:\")\n",
    "print(\"  - NO values are dropped (all neurons active)\")\n",
    "print(\"  - Values pass through unchanged\")\n",
    "print(\"  - Same output every time (deterministic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf0e967",
   "metadata": {},
   "source": [
    "### Understanding Dropout Scaling\n",
    "\n",
    "Dropout scales the remaining activations by 1/(1-p) during training. This ensures the expected value stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43733fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the scaling\n",
    "x = torch.ones(1000)  # Large sample for statistics\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "dropout.train()\n",
    "\n",
    "output = dropout(x)\n",
    "\n",
    "print(\"Dropout scaling demonstration:\")\n",
    "print(f\"Input mean: {x.mean():.4f}\")\n",
    "print(f\"Output mean: {output.mean():.4f}\")\n",
    "print()\n",
    "print(f\"Fraction dropped: {(output == 0).float().mean():.2%}\")\n",
    "print(f\"Fraction kept: {(output != 0).float().mean():.2%}\")\n",
    "print()\n",
    "print(f\"Non-zero values: {output[output != 0][:5]}\")\n",
    "print(f\"Scaling factor: 1/(1-0.5) = {1/(1-0.5):.1f}\")\n",
    "print()\n",
    "print(\"The scaling ensures the expected value is preserved!\")\n",
    "print(\"E[output] = (1-p) * input * (1/(1-p)) = input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2f27cc",
   "metadata": {},
   "source": [
    "### Using Dropout in Neural Networks\n",
    "\n",
    "Dropout is typically placed after activation functions in fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0df629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network with dropout\n",
    "class NetworkWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(100, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)  # 50% dropout\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.5)  # 50% dropout\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 10)  # No dropout on output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)  # Apply dropout after activation\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)  # Apply dropout after activation\n",
    "        \n",
    "        x = self.fc3(x)  # No dropout on output\n",
    "        return x\n",
    "\n",
    "model = NetworkWithDropout()\n",
    "print(\"Network with Dropout:\")\n",
    "print(model)\n",
    "print()\n",
    "print(\"Dropout is applied after ReLU activations in hidden layers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1fd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing training vs evaluation mode\n",
    "model = NetworkWithDropout()\n",
    "x = torch.randn(5, 100)  # Batch of 5 samples\n",
    "\n",
    "# Training mode\n",
    "model.train()\n",
    "output_train1 = model(x)\n",
    "output_train2 = model(x)  # Same input, different output\n",
    "\n",
    "print(\"Training mode (dropout active):\")\n",
    "print(f\"Output 1: {output_train1[0, :5]}\")\n",
    "print(f\"Output 2: {output_train2[0, :5]}\")\n",
    "print(f\"Same? {torch.allclose(output_train1, output_train2)}\")\n",
    "print(\"Different outputs due to random dropout!\")\n",
    "print()\n",
    "\n",
    "# Evaluation mode\n",
    "model.eval()\n",
    "output_eval1 = model(x)\n",
    "output_eval2 = model(x)  # Same input, same output\n",
    "\n",
    "print(\"Evaluation mode (dropout inactive):\")\n",
    "print(f\"Output 1: {output_eval1[0, :5]}\")\n",
    "print(f\"Output 2: {output_eval2[0, :5]}\")\n",
    "print(f\"Same? {torch.allclose(output_eval1, output_eval2)}\")\n",
    "print(\"Identical outputs - dropout is off!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def817a0",
   "metadata": {},
   "source": [
    "### Choosing the Right Dropout Rate\n",
    "\n",
    "The dropout probability `p` is a hyperparameter you need to tune. Common values and guidelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ed90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing different dropout rates\n",
    "x = torch.ones(1000)\n",
    "\n",
    "dropout_rates = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "print(\"Effect of different dropout rates:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'p':>5} | {'Kept':>8} | {'Dropped':>8} | {'Scale':>8} | {'Mean':>8}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for p in dropout_rates:\n",
    "    dropout = nn.Dropout(p=p)\n",
    "    dropout.train()\n",
    "    output = dropout(x)\n",
    "    \n",
    "    kept = (output != 0).float().mean()\n",
    "    dropped = (output == 0).float().mean()\n",
    "    scale = 1 / (1 - p)\n",
    "    mean = output.mean()\n",
    "    \n",
    "    print(f\"{p:>5.1f} | {kept:>7.1%} | {dropped:>7.1%} | {scale:>8.2f} | {mean:>8.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"Guidelines:\")\n",
    "print(\"  p=0.1-0.2: Light regularization (CNNs)\")\n",
    "print(\"  p=0.3-0.5: Moderate regularization (most common)\")\n",
    "print(\"  p=0.5-0.7: Heavy regularization (fully connected layers)\")\n",
    "print(\"  p>0.7: Very aggressive (rarely used)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a44e04",
   "metadata": {},
   "source": [
    "### Functional API: torch.nn.functional.dropout\n",
    "\n",
    "Dropout can also be applied as a function, giving you more control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.ones(10)\n",
    "\n",
    "# Using F.dropout - must specify training mode manually\n",
    "output_train = F.dropout(x, p=0.5, training=True)\n",
    "output_eval = F.dropout(x, p=0.5, training=False)\n",
    "\n",
    "print(\"Functional dropout:\")\n",
    "print(f\"Input: {x}\")\n",
    "print()\n",
    "print(f\"F.dropout(training=True):  {output_train}\")\n",
    "print(f\"F.dropout(training=False): {output_eval}\")\n",
    "print()\n",
    "print(\"With F.dropout, you must manually specify training=True/False!\")\n",
    "print(\"nn.Dropout() automatically uses model.train()/model.eval() state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6982587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using F.dropout in forward method\n",
    "class NetworkWithFunctionalDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(100, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        # No dropout modules defined!\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)  # Use self.training\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = NetworkWithFunctionalDropout()\n",
    "print(\"Model using functional dropout:\")\n",
    "print(model)\n",
    "print()\n",
    "print(\"No Dropout layers in the model definition!\")\n",
    "print(\"Dropout is applied in forward() using F.dropout()\")\n",
    "print(\"self.training automatically tracks train/eval mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f689ccea",
   "metadata": {},
   "source": [
    "### Dropout Best Practices\n",
    "\n",
    "Here are some important guidelines for using dropout effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ded9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practice example\n",
    "class WellRegularizedNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input layer - no dropout\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)  # Can combine with BatchNorm\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)  # Heavy dropout after first layer\n",
    "        \n",
    "        # Hidden layer\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)  # Lighter dropout\n",
    "        \n",
    "        # Output layer - no dropout\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)  # Dropout after activation\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)  # Dropout after activation\n",
    "        \n",
    "        x = self.fc3(x)  # No dropout on output\n",
    "        return x\n",
    "\n",
    "print(\"Best practices demonstrated:\")\n",
    "print(\"✓ Dropout after activation functions\")\n",
    "print(\"✓ Higher dropout (0.5) in early layers\")\n",
    "print(\"✓ Lower dropout (0.3) in later layers\")\n",
    "print(\"✓ No dropout on output layer\")\n",
    "print(\"✓ Can combine with BatchNorm\")\n",
    "print(\"✓ Order: Linear → BatchNorm → ReLU → Dropout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861d826",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- **Dropout**: Randomly drops neurons during training to prevent overfitting\n",
    "- **Parameter p**: Probability of dropping a neuron (common: 0.3-0.5)\n",
    "- **Scaling**: Remaining activations are scaled by 1/(1-p) during training\n",
    "- **Training vs Eval**: Active during training, inactive during evaluation\n",
    "- **Placement**: After activation functions in hidden layers\n",
    "- **Not on output**: Never apply dropout to the output layer\n",
    "\n",
    "**Common dropout rates:**\n",
    "- **0.1-0.2**: Light regularization (CNNs, when overfitting is mild)\n",
    "- **0.3-0.5**: Standard regularization (most common)\n",
    "- **0.5-0.7**: Heavy regularization (fully connected layers, severe overfitting)\n",
    "\n",
    "**Important reminders:**\n",
    "1. ✓ Always call `model.train()` during training\n",
    "2. ✓ Always call `model.eval()` during evaluation/inference\n",
    "3. ✓ Apply dropout AFTER activation functions\n",
    "4. ✗ Don't use dropout on output layers\n",
    "5. ✗ Don't forget to switch between train/eval modes\n",
    "\n",
    "**Typical pattern:**\n",
    "```python\n",
    "# Training\n",
    "model.train()\n",
    "for batch in train_loader:\n",
    "    output = model(batch)  # Dropout active\n",
    "    loss = criterion(output, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        output = model(batch)  # Dropout inactive\n",
    "        # Evaluate...\n",
    "```\n",
    "\n",
    "Dropout is one of the simplest yet most effective regularization techniques. It's a standard tool in every deep learning practitioner's toolkit!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f105afd",
   "metadata": {},
   "source": [
    "## torch.gather - Advanced Indexing Operation\n",
    "\n",
    "**What is torch.gather?**\n",
    "\n",
    "`torch.gather` is a powerful indexing operation that selects elements from a tensor along a specified dimension using an index tensor. Think of it as \"fancy indexing\" that lets you pick specific elements from each row or column.\n",
    "\n",
    "**Signature**: `torch.gather(input, dim, index)`\n",
    "\n",
    "- **input**: The source tensor to gather from\n",
    "- **dim**: The dimension along which to index\n",
    "- **index**: Tensor of indices specifying which elements to select\n",
    "\n",
    "**Why does torch.gather matter?**\n",
    "\n",
    "`torch.gather` is essential for many deep learning operations:\n",
    "\n",
    "1. **Selecting predictions**: Get the predicted values for specific classes\n",
    "2. **Attention mechanisms**: Select attended values in transformers\n",
    "3. **Reinforcement learning**: Extract Q-values for taken actions\n",
    "4. **Loss computation**: Select logits corresponding to true labels\n",
    "5. **Custom operations**: Any time you need to select different elements from each sample\n",
    "\n",
    "**Common use cases:**\n",
    "- Extracting model predictions for ground truth classes\n",
    "- Implementing attention mechanisms\n",
    "- Gathering embeddings for specific tokens\n",
    "- Selecting actions in RL\n",
    "\n",
    "Let's understand torch.gather with clear examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a73e29b",
   "metadata": {},
   "source": [
    "### Basic Example: 1D Tensor\n",
    "\n",
    "Let's start with a simple 1D example to understand the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6343fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Simple 1D tensor\n",
    "values = torch.tensor([10, 20, 30, 40, 50])\n",
    "print(\"Values:\", values)\n",
    "print()\n",
    "\n",
    "# Indices to gather\n",
    "indices = torch.tensor([0, 2, 4, 1])  # Select elements at positions 0, 2, 4, 1\n",
    "print(\"Indices:\", indices)\n",
    "print()\n",
    "\n",
    "# Gather elements\n",
    "gathered = torch.gather(values, dim=0, index=indices)\n",
    "print(\"Gathered:\", gathered)\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(f\"  Index 0 → values[0] = {values[0]}\")\n",
    "print(f\"  Index 2 → values[2] = {values[2]}\")\n",
    "print(f\"  Index 4 → values[4] = {values[4]}\")\n",
    "print(f\"  Index 1 → values[1] = {values[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69201134",
   "metadata": {},
   "source": [
    "### 2D Tensor: Understanding Dimensions\n",
    "\n",
    "With 2D tensors, the `dim` parameter determines whether we gather along rows or columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6653df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D tensor (3 rows, 5 columns)\n",
    "matrix = torch.tensor([\n",
    "    [10, 11, 12, 13, 14],  # Row 0\n",
    "    [20, 21, 22, 23, 24],  # Row 1\n",
    "    [30, 31, 32, 33, 34],  # Row 2\n",
    "])\n",
    "\n",
    "print(\"Matrix:\")\n",
    "print(matrix)\n",
    "print(f\"Shape: {matrix.shape}  # (3 rows, 5 columns)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da4fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather along dim=1 (columns) - most common\n",
    "# For each row, select specific columns\n",
    "indices = torch.tensor([\n",
    "    [0, 2, 4],  # Row 0: select columns 0, 2, 4\n",
    "    [1, 3, 4],  # Row 1: select columns 1, 3, 4\n",
    "    [2, 2, 2],  # Row 2: select column 2 three times\n",
    "])\n",
    "\n",
    "print(\"Indices (which columns to select for each row):\")\n",
    "print(indices)\n",
    "print()\n",
    "\n",
    "gathered = torch.gather(matrix, dim=1, index=indices)\n",
    "print(\"Gathered (dim=1):\")\n",
    "print(gathered)\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(f\"  Row 0: columns [0,2,4] → [{matrix[0,0]}, {matrix[0,2]}, {matrix[0,4]}]\")\n",
    "print(f\"  Row 1: columns [1,3,4] → [{matrix[1,1]}, {matrix[1,3]}, {matrix[1,4]}]\")\n",
    "print(f\"  Row 2: columns [2,2,2] → [{matrix[2,2]}, {matrix[2,2]}, {matrix[2,2]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c287ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather along dim=0 (rows) - less common\n",
    "# For each column, select specific rows\n",
    "indices = torch.tensor([\n",
    "    [0, 1, 2, 0, 1],  # For each column, which row to select\n",
    "])\n",
    "\n",
    "print(\"Indices (which row to select for each column):\")\n",
    "print(indices)\n",
    "print()\n",
    "\n",
    "gathered = torch.gather(matrix, dim=0, index=indices)\n",
    "print(\"Gathered (dim=0):\")\n",
    "print(gathered)\n",
    "print()\n",
    "print(\"Explanation:\")\n",
    "print(f\"  Column 0: row 0 → {matrix[0,0]}\")\n",
    "print(f\"  Column 1: row 1 → {matrix[1,1]}\")\n",
    "print(f\"  Column 2: row 2 → {matrix[2,2]}\")\n",
    "print(f\"  Column 3: row 0 → {matrix[0,3]}\")\n",
    "print(f\"  Column 4: row 1 → {matrix[1,4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8edce",
   "metadata": {},
   "source": [
    "### Practical Example: Extracting Class Predictions\n",
    "\n",
    "A common use case: given model logits and true labels, extract the logit values for the correct classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd28b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated model output: 4 samples, 5 classes\n",
    "logits = torch.tensor([\n",
    "    [2.1, 0.5, 1.3, 0.8, 1.9],  # Sample 0\n",
    "    [0.3, 3.2, 1.1, 0.9, 1.5],  # Sample 1\n",
    "    [1.8, 1.2, 0.7, 2.5, 1.0],  # Sample 2\n",
    "    [0.9, 1.4, 2.8, 1.1, 0.6],  # Sample 3\n",
    "])\n",
    "\n",
    "# True class labels\n",
    "true_labels = torch.tensor([0, 1, 3, 2])  # Correct class for each sample\n",
    "\n",
    "print(\"Logits (model predictions):\")\n",
    "print(logits)\n",
    "print(f\"Shape: {logits.shape}  # (4 samples, 5 classes)\")\n",
    "print()\n",
    "print(f\"True labels: {true_labels}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca4e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract logits for the true classes using gather\n",
    "# Need to reshape indices to match gather requirements\n",
    "indices = true_labels.unsqueeze(1)  # Shape: (4, 1)\n",
    "\n",
    "print(f\"Indices shape: {indices.shape}\")\n",
    "print(f\"Indices: {indices.squeeze()}\")\n",
    "print()\n",
    "\n",
    "# Gather the logits for true classes\n",
    "true_class_logits = torch.gather(logits, dim=1, index=indices)\n",
    "\n",
    "print(\"True class logits:\")\n",
    "print(true_class_logits.squeeze())\n",
    "print()\n",
    "print(\"Verification:\")\n",
    "for i in range(len(true_labels)):\n",
    "    label = true_labels[i].item()\n",
    "    logit = logits[i, label].item()\n",
    "    gathered_logit = true_class_logits[i].item()\n",
    "    print(f\"  Sample {i}: true class={label}, logit={logit:.2f}, gathered={gathered_logit:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187d7ea",
   "metadata": {},
   "source": [
    "### Advanced Example: Attention-like Selection\n",
    "\n",
    "Gather is used in attention mechanisms to select values based on attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated attention scenario\n",
    "# We have a sequence and want to select specific positions for each sample\n",
    "\n",
    "# Sequence of values: (batch=2, seq_len=5, features=3)\n",
    "sequence = torch.tensor([\n",
    "    # Sample 0\n",
    "    [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15]],\n",
    "    # Sample 1\n",
    "    [[16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26, 27], [28, 29, 30]],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(\"Sequence:\")\n",
    "print(f\"Shape: {sequence.shape}  # (2 samples, 5 positions, 3 features)\")\n",
    "print()\n",
    "\n",
    "# Indices: which positions to select for each sample\n",
    "# Sample 0: select positions 1 and 3\n",
    "# Sample 1: select positions 0 and 4\n",
    "indices = torch.tensor([\n",
    "    [1, 3],  # Sample 0\n",
    "    [0, 4],  # Sample 1\n",
    "])\n",
    "\n",
    "print(f\"Indices: {indices}\")\n",
    "print(f\"Shape: {indices.shape}  # (2 samples, 2 selections)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f862d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To gather, we need to expand indices to match the feature dimension\n",
    "# indices shape: (2, 2) → need (2, 2, 3)\n",
    "indices_expanded = indices.unsqueeze(-1).expand(-1, -1, 3)\n",
    "\n",
    "print(f\"Expanded indices shape: {indices_expanded.shape}  # (2, 2, 3)\")\n",
    "print(\"Expanded indices:\")\n",
    "print(indices_expanded)\n",
    "print()\n",
    "\n",
    "# Gather along the sequence dimension (dim=1)\n",
    "selected = torch.gather(sequence, dim=1, index=indices_expanded)\n",
    "\n",
    "print(\"Selected values:\")\n",
    "print(selected)\n",
    "print(f\"Shape: {selected.shape}  # (2 samples, 2 selections, 3 features)\")\n",
    "print()\n",
    "print(\"Verification:\")\n",
    "print(f\"Sample 0, position 1: {sequence[0, 1]}\")\n",
    "print(f\"Sample 0, position 3: {sequence[0, 3]}\")\n",
    "print(f\"Sample 1, position 0: {sequence[1, 0]}\")\n",
    "print(f\"Sample 1, position 4: {sequence[1, 4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd68c613",
   "metadata": {},
   "source": [
    "### torch.gather vs Regular Indexing\n",
    "\n",
    "When should you use `torch.gather` vs regular indexing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b71b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Selecting one element per row\n",
    "matrix = torch.tensor([\n",
    "    [10, 20, 30],\n",
    "    [40, 50, 60],\n",
    "    [70, 80, 90],\n",
    "])\n",
    "indices = torch.tensor([1, 0, 2])  # Column to select for each row\n",
    "\n",
    "print(\"Matrix:\")\n",
    "print(matrix)\n",
    "print()\n",
    "print(f\"Indices: {indices} (column to select for each row)\")\n",
    "print()\n",
    "\n",
    "# Method 1: Using torch.gather\n",
    "gathered = torch.gather(matrix, dim=1, index=indices.unsqueeze(1))\n",
    "print(f\"Using torch.gather: {gathered.squeeze()}\")\n",
    "print()\n",
    "\n",
    "# Method 2: Using advanced indexing\n",
    "row_indices = torch.arange(len(matrix))\n",
    "indexed = matrix[row_indices, indices]\n",
    "print(f\"Using indexing: {indexed}\")\n",
    "print()\n",
    "\n",
    "print(f\"Same result? {torch.equal(gathered.squeeze(), indexed)}\")\n",
    "print()\n",
    "print(\"When to use gather:\")\n",
    "print(\"  - When working with batched operations\")\n",
    "print(\"  - When indices have complex shapes\")\n",
    "print(\"  - When you need to gather multiple elements per sample\")\n",
    "print()\n",
    "print(\"When to use regular indexing:\")\n",
    "print(\"  - When selecting single elements\")\n",
    "print(\"  - When the pattern is simple\")\n",
    "print(\"  - When readability is more important\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255ff3be",
   "metadata": {},
   "source": [
    "### Common Patterns with torch.gather\n",
    "\n",
    "Here are some frequently used patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd473ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Selecting top-k elements\n",
    "values = torch.tensor([[3.2, 1.5, 4.8, 2.1, 5.3],\n",
    "                       [2.7, 4.2, 1.9, 5.1, 3.5]])\n",
    "\n",
    "print(\"Values:\")\n",
    "print(values)\n",
    "print()\n",
    "\n",
    "# Get indices of top-3 values for each row\n",
    "k = 3\n",
    "top_k_values, top_k_indices = torch.topk(values, k, dim=1)\n",
    "\n",
    "print(f\"Top-{k} indices:\")\n",
    "print(top_k_indices)\n",
    "print()\n",
    "print(f\"Top-{k} values (from topk):\")\n",
    "print(top_k_values)\n",
    "print()\n",
    "\n",
    "# Verify using gather\n",
    "gathered_top_k = torch.gather(values, dim=1, index=top_k_indices)\n",
    "print(f\"Top-{k} values (using gather):\")\n",
    "print(gathered_top_k)\n",
    "print()\n",
    "print(f\"Match? {torch.equal(top_k_values, gathered_top_k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba76e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 2: Gathering embeddings for specific tokens\n",
    "# Simulated embedding table\n",
    "embeddings = torch.randn(10, 4)  # 10 tokens, 4-dim embeddings\n",
    "\n",
    "# Token IDs for a batch of sequences\n",
    "token_ids = torch.tensor([\n",
    "    [2, 5, 7],  # Sequence 1\n",
    "    [1, 3, 9],  # Sequence 2\n",
    "])\n",
    "\n",
    "print(f\"Embedding table shape: {embeddings.shape}  # (10 tokens, 4 dims)\")\n",
    "print(f\"Token IDs shape: {token_ids.shape}  # (2 sequences, 3 tokens)\")\n",
    "print()\n",
    "\n",
    "# Expand indices to match embedding dimension\n",
    "indices = token_ids.unsqueeze(-1).expand(-1, -1, 4)\n",
    "print(f\"Expanded indices shape: {indices.shape}  # (2, 3, 4)\")\n",
    "print()\n",
    "\n",
    "# Gather embeddings\n",
    "# Note: This is what nn.Embedding does internally!\n",
    "gathered_embeddings = torch.gather(\n",
    "    embeddings.unsqueeze(0).expand(2, -1, -1),  # Expand for batch\n",
    "    dim=1,\n",
    "    index=indices\n",
    ")\n",
    "\n",
    "print(f\"Gathered embeddings shape: {gathered_embeddings.shape}  # (2, 3, 4)\")\n",
    "print()\n",
    "print(\"This is similar to how nn.Embedding works!\")\n",
    "print(\"(Though nn.Embedding uses optimized indexing, not gather)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1078fa9",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- **torch.gather**: Selects elements from a tensor using an index tensor\n",
    "- **Signature**: `torch.gather(input, dim, index)`\n",
    "- **dim parameter**: Specifies which dimension to index along\n",
    "- **Index shape**: Must match input shape except along the gather dimension\n",
    "\n",
    "**Common use cases:**\n",
    "1. Extracting predictions for true classes in classification\n",
    "2. Selecting attended values in attention mechanisms\n",
    "3. Gathering Q-values for actions in reinforcement learning\n",
    "4. Selecting top-k elements\n",
    "5. Custom indexing operations in batched computations\n",
    "\n",
    "**Key rules:**\n",
    "- Index values must be in range [0, input.size(dim))\n",
    "- Index shape must match input shape except along dim\n",
    "- Output shape matches index shape\n",
    "- Can select the same element multiple times\n",
    "\n",
    "**Common patterns:**\n",
    "```python\n",
    "# Extract logits for true classes\n",
    "true_logits = torch.gather(logits, 1, labels.unsqueeze(1))\n",
    "\n",
    "# Select top-k values\n",
    "_, indices = torch.topk(values, k, dim=1)\n",
    "top_k = torch.gather(values, 1, indices)\n",
    "\n",
    "# Gather with feature dimension\n",
    "indices_expanded = indices.unsqueeze(-1).expand(-1, -1, feature_dim)\n",
    "selected = torch.gather(input, 1, indices_expanded)\n",
    "```\n",
    "\n",
    "**torch.gather vs alternatives:**\n",
    "- **Simple indexing**: Use when pattern is straightforward\n",
    "- **torch.gather**: Use for batched operations with complex index patterns\n",
    "- **torch.index_select**: Use when selecting entire rows/columns\n",
    "- **Boolean indexing**: Use when selecting based on conditions\n",
    "\n",
    "`torch.gather` is a powerful tool for advanced indexing operations. While it can be tricky at first, it's essential for implementing many deep learning operations efficiently!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74ccfce",
   "metadata": {},
   "source": [
    "## Loss Functions - Measuring Prediction Error\n",
    "\n",
    "**What are loss functions?**\n",
    "\n",
    "A loss function (also called a cost function or objective function) measures how wrong your model's predictions are. It takes the model's predictions and the true labels, and outputs a single number representing the error. The goal of training is to minimize this loss.\n",
    "\n",
    "**Why do loss functions matter?**\n",
    "\n",
    "Loss functions are **essential for training neural networks**:\n",
    "- They provide a **quantitative measure** of model performance\n",
    "- They define **what \"better\" means** for your specific task\n",
    "- They enable **gradient-based optimization** through backpropagation\n",
    "- Different tasks require different loss functions\n",
    "\n",
    "**Common loss functions:**\n",
    "- **Mean Squared Error (MSE)**: For regression tasks (predicting continuous values)\n",
    "- **Cross-Entropy Loss**: For classification tasks (predicting categories)\n",
    "- **Binary Cross-Entropy**: For binary classification (yes/no, 0/1)\n",
    "- **L1 Loss (MAE)**: For regression with outliers\n",
    "\n",
    "In this section, we'll focus on **MSE Loss**, which is perfect for understanding the basics because it's simple and intuitive.\n",
    "\n",
    "**The training workflow:**\n",
    "```\n",
    "1. Forward pass: predictions = model(inputs)\n",
    "2. Compute loss: loss = loss_function(predictions, targets)\n",
    "3. Backward pass: loss.backward()  # Compute gradients\n",
    "4. Update weights: optimizer.step()  # Adjust parameters\n",
    "```\n",
    "\n",
    "Let's explore how loss functions work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d650a8fe",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE) Loss\n",
    "\n",
    "MSE Loss is one of the most common loss functions for regression tasks. It measures the average squared difference between predictions and targets:\n",
    "\n",
    "**Formula**: `MSE = (1/n) * Σ(prediction - target)²`\n",
    "\n",
    "**Why square the differences?**\n",
    "- Makes all errors positive (no cancellation)\n",
    "- Penalizes large errors more heavily than small ones\n",
    "- Creates smooth gradients for optimization\n",
    "\n",
    "**When to use MSE:**\n",
    "- Predicting continuous values (prices, temperatures, distances)\n",
    "- When you want to penalize large errors more\n",
    "- When your target values are roughly normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb49580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create some predictions and targets\n",
    "predictions = torch.tensor([2.5, 3.0, 4.5, 5.0])\n",
    "targets = torch.tensor([3.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Targets:    \", targets)\n",
    "print()\n",
    "\n",
    "# Calculate MSE manually\n",
    "squared_errors = (predictions - targets) ** 2\n",
    "mse_manual = squared_errors.mean()\n",
    "\n",
    "print(\"Squared errors:\", squared_errors)\n",
    "print(f\"Manual MSE: {mse_manual.item():.4f}\")\n",
    "print()\n",
    "\n",
    "# Calculate using PyTorch's MSELoss\n",
    "mse_loss = nn.MSELoss()\n",
    "loss = mse_loss(predictions, targets)\n",
    "\n",
    "print(f\"PyTorch MSE: {loss.item():.4f}\")\n",
    "print()\n",
    "print(\"Calculation breakdown:\")\n",
    "print(\"  (2.5 - 3.0)² = 0.25\")\n",
    "print(\"  (3.0 - 3.0)² = 0.00\")\n",
    "print(\"  (4.5 - 4.0)² = 0.25\")\n",
    "print(\"  (5.0 - 5.0)² = 0.00\")\n",
    "print(\"  Average = (0.25 + 0.00 + 0.25 + 0.00) / 4 = 0.125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3401d",
   "metadata": {},
   "source": [
    "### MSE Loss with Batches\n",
    "\n",
    "In practice, you'll compute loss over batches of data. MSE Loss automatically handles multi-dimensional tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a8bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch of predictions: 8 samples, each with 3 output values\n",
    "batch_predictions = torch.randn(8, 3)\n",
    "batch_targets = torch.randn(8, 3)\n",
    "\n",
    "print(f\"Predictions shape: {batch_predictions.shape}  # (batch_size, num_outputs)\")\n",
    "print(f\"Targets shape: {batch_targets.shape}\")\n",
    "print()\n",
    "\n",
    "# Compute MSE loss\n",
    "mse_loss = nn.MSELoss()\n",
    "loss = mse_loss(batch_predictions, batch_targets)\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(f\"Loss shape: {loss.shape}  # Scalar (single number)\")\n",
    "print()\n",
    "print(\"The loss is averaged over ALL elements (8 samples × 3 outputs = 24 values)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0949ea",
   "metadata": {},
   "source": [
    "### Loss Reduction Modes\n",
    "\n",
    "PyTorch loss functions have a `reduction` parameter that controls how the loss is aggregated:\n",
    "- **'mean'** (default): Average loss over all elements\n",
    "- **'sum'**: Sum of all losses\n",
    "- **'none'**: Return individual losses (no reduction)\n",
    "\n",
    "Different reduction modes are useful for different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "targets = torch.tensor([[1.5, 2.5], [3.5, 4.5], [5.5, 6.5]])\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(predictions)\n",
    "print(\"\\nTargets:\")\n",
    "print(targets)\n",
    "print()\n",
    "\n",
    "# Mean reduction (default)\n",
    "loss_mean = nn.MSELoss(reduction='mean')\n",
    "print(f\"MSE with reduction='mean': {loss_mean(predictions, targets).item():.4f}\")\n",
    "print()\n",
    "\n",
    "# Sum reduction\n",
    "loss_sum = nn.MSELoss(reduction='sum')\n",
    "print(f\"MSE with reduction='sum': {loss_sum(predictions, targets).item():.4f}\")\n",
    "print()\n",
    "\n",
    "# No reduction - returns loss for each element\n",
    "loss_none = nn.MSELoss(reduction='none')\n",
    "losses = loss_none(predictions, targets)\n",
    "print(\"MSE with reduction='none':\")\n",
    "print(losses)\n",
    "print(f\"Shape: {losses.shape}  # Same as input shape\")\n",
    "print()\n",
    "print(\"Verification:\")\n",
    "print(f\"  Mean of individual losses: {losses.mean().item():.4f}\")\n",
    "print(f\"  Sum of individual losses: {losses.sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26367bf5",
   "metadata": {},
   "source": [
    "### Using Loss Functions in Training\n",
    "\n",
    "Here's how loss functions fit into the training workflow. We'll create a simple model and compute its loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be02e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple linear model\n",
    "model = nn.Linear(5, 1)  # 5 input features, 1 output\n",
    "\n",
    "# Create some fake data\n",
    "batch_size = 16\n",
    "inputs = torch.randn(batch_size, 5)  # 16 samples, 5 features each\n",
    "targets = torch.randn(batch_size, 1)  # 16 target values\n",
    "\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")\n",
    "print()\n",
    "\n",
    "# Forward pass: get predictions\n",
    "predictions = model(inputs)\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print()\n",
    "\n",
    "# Compute loss\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(predictions, targets)\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print()\n",
    "print(\"This loss value tells us how far off our predictions are from the targets.\")\n",
    "print(\"During training, we'll use this loss to compute gradients and update the model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a203ec0a",
   "metadata": {},
   "source": [
    "### Interpreting Loss Values\n",
    "\n",
    "Understanding what loss values mean helps you monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97893e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfect predictions (loss = 0)\n",
    "perfect_pred = torch.tensor([1.0, 2.0, 3.0])\n",
    "perfect_target = torch.tensor([1.0, 2.0, 3.0])\n",
    "perfect_loss = nn.MSELoss()(perfect_pred, perfect_target)\n",
    "print(f\"Perfect predictions - Loss: {perfect_loss.item():.4f}\")\n",
    "print()\n",
    "\n",
    "# Small errors\n",
    "good_pred = torch.tensor([1.1, 2.05, 2.95])\n",
    "good_target = torch.tensor([1.0, 2.0, 3.0])\n",
    "good_loss = nn.MSELoss()(good_pred, good_target)\n",
    "print(f\"Small errors - Loss: {good_loss.item():.4f}\")\n",
    "print()\n",
    "\n",
    "# Large errors\n",
    "bad_pred = torch.tensor([2.0, 4.0, 1.0])\n",
    "bad_target = torch.tensor([1.0, 2.0, 3.0])\n",
    "bad_loss = nn.MSELoss()(bad_pred, bad_target)\n",
    "print(f\"Large errors - Loss: {bad_loss.item():.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Key insight: Lower loss = better predictions!\")\n",
    "print(\"During training, we want to see the loss decrease over time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40cfec7",
   "metadata": {},
   "source": [
    "### Other Common Loss Functions\n",
    "\n",
    "While we focused on MSE, PyTorch provides many other loss functions. Here's a quick overview of when to use each:\n",
    "\n",
    "**For Regression (predicting continuous values):**\n",
    "- `nn.MSELoss()`: Mean Squared Error - penalizes large errors heavily\n",
    "- `nn.L1Loss()`: Mean Absolute Error - more robust to outliers\n",
    "- `nn.SmoothL1Loss()`: Huber loss - combines benefits of MSE and L1\n",
    "\n",
    "**For Classification (predicting categories):**\n",
    "- `nn.CrossEntropyLoss()`: For multi-class classification (most common)\n",
    "- `nn.BCELoss()`: Binary Cross-Entropy for binary classification\n",
    "- `nn.BCEWithLogitsLoss()`: BCE with built-in sigmoid (more numerically stable)\n",
    "- `nn.NLLLoss()`: Negative Log-Likelihood (used with log_softmax)\n",
    "\n",
    "**For Other Tasks:**\n",
    "- `nn.KLDivLoss()`: KL Divergence for comparing probability distributions\n",
    "- `nn.CosineEmbeddingLoss()`: For learning embeddings\n",
    "- `nn.TripletMarginLoss()`: For metric learning\n",
    "\n",
    "**Choosing the right loss function:**\n",
    "- Match the loss to your task (regression vs classification)\n",
    "- Consider your data distribution (outliers, class imbalance)\n",
    "- Some losses work better with specific architectures\n",
    "- Experiment and validate on a held-out set!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07538b52",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**What are loss functions?**\n",
    "- Measure how wrong your model's predictions are\n",
    "- Output a single number representing the error\n",
    "- Essential for training neural networks through gradient descent\n",
    "\n",
    "**Mean Squared Error (MSE):**\n",
    "- Formula: `MSE = (1/n) * Σ(prediction - target)²`\n",
    "- Best for regression tasks (predicting continuous values)\n",
    "- Penalizes large errors more than small ones\n",
    "- Created in PyTorch: `criterion = nn.MSELoss()`\n",
    "\n",
    "**Using Loss Functions:**\n",
    "```python\n",
    "# 1. Create loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 2. Get predictions from model\n",
    "predictions = model(inputs)\n",
    "\n",
    "# 3. Compute loss\n",
    "loss = criterion(predictions, targets)\n",
    "\n",
    "# 4. Use loss for backpropagation\n",
    "loss.backward()  # Compute gradients\n",
    "```\n",
    "\n",
    "**Reduction Modes:**\n",
    "- `reduction='mean'`: Average loss (default, most common)\n",
    "- `reduction='sum'`: Sum of all losses\n",
    "- `reduction='none'`: Individual losses per element\n",
    "\n",
    "**Important Concepts:**\n",
    "- **Lower loss = better predictions**\n",
    "- Loss should decrease during training\n",
    "- Different tasks need different loss functions\n",
    "- Loss provides the signal for gradient-based optimization\n",
    "\n",
    "**The Training Loop Connection:**\n",
    "\n",
    "Loss functions are the bridge between your model's predictions and learning:\n",
    "1. Model makes predictions (forward pass)\n",
    "2. Loss function measures error\n",
    "3. Backpropagation computes gradients from loss\n",
    "4. Optimizer uses gradients to improve model\n",
    "5. Repeat until loss is minimized!\n",
    "\n",
    "Understanding loss functions is crucial because they define what your model is trying to achieve. Choose the right loss function, and your model will learn the right thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac5afd1",
   "metadata": {},
   "source": [
    "## Logits - Raw Model Outputs\n",
    "\n",
    "**What are logits?**\n",
    "\n",
    "Logits are the **raw, unnormalized scores** that come directly out of a neural network before any activation function is applied. The term comes from \"logistic units\" and is commonly used in classification tasks.\n",
    "\n",
    "Think of logits as the model's \"confidence scores\" in their raw form:\n",
    "- Higher logit value = model thinks this class is more likely\n",
    "- Logits can be any real number (positive, negative, or zero)\n",
    "- They're not probabilities yet - they need to be transformed (usually with softmax)\n",
    "\n",
    "**Why do logits matter?**\n",
    "\n",
    "Understanding logits is important because:\n",
    "- **Numerical stability**: Many loss functions work better with logits than probabilities\n",
    "- **Flexibility**: You can apply different transformations (softmax, sigmoid, etc.)\n",
    "- **Interpretation**: Logits show the model's raw confidence before normalization\n",
    "- **Common terminology**: You'll see \"logits\" everywhere in deep learning papers and code\n",
    "\n",
    "**The typical flow:**\n",
    "```\n",
    "Input → Neural Network → Logits → Activation (softmax/sigmoid) → Probabilities\n",
    "```\n",
    "\n",
    "Let's explore logits in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd34fec",
   "metadata": {},
   "source": [
    "### Logits in Classification\n",
    "\n",
    "In classification tasks, logits represent the model's raw scores for each class. Let's see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a37f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Simulate a model's output (logits) for a 3-class classification\n",
    "# These are raw scores before any activation\n",
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "\n",
    "print(\"Logits (raw model outputs):\")\n",
    "print(logits)\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\"  Class 0: score = 2.0 (highest - model is most confident about this)\")\n",
    "print(\"  Class 1: score = 1.0 (medium confidence)\")\n",
    "print(\"  Class 2: score = 0.1 (lowest - model is least confident)\")\n",
    "print()\n",
    "print(\"Note: These are NOT probabilities!\")\n",
    "print(f\"  Sum of logits: {logits.sum().item():.2f} (not 1.0)\")\n",
    "print(f\"  Logits can be negative, and don't sum to 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aca7490",
   "metadata": {},
   "source": [
    "### Converting Logits to Probabilities\n",
    "\n",
    "To get probabilities from logits, we apply the **softmax** function. Softmax transforms logits into a probability distribution:\n",
    "- All values become positive\n",
    "- All values sum to 1.0\n",
    "- Relative ordering is preserved (highest logit → highest probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf42bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply softmax to convert logits to probabilities\n",
    "softmax = nn.Softmax(dim=0)\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "print(\"Logits:\")\n",
    "print(logits)\n",
    "print()\n",
    "print(\"Probabilities (after softmax):\")\n",
    "print(probabilities)\n",
    "print()\n",
    "print(\"Properties of probabilities:\")\n",
    "print(f\"  All positive: {(probabilities > 0).all().item()}\")\n",
    "print(f\"  Sum to 1.0: {probabilities.sum().item():.6f}\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(f\"  Class 0: {probabilities[0].item():.1%} probability\")\n",
    "print(f\"  Class 1: {probabilities[1].item():.1%} probability\")\n",
    "print(f\"  Class 2: {probabilities[2].item():.1%} probability\")\n",
    "print()\n",
    "print(\"The model predicts Class 0 with highest confidence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb637a67",
   "metadata": {},
   "source": [
    "### Logits for a Batch of Samples\n",
    "\n",
    "In practice, models process batches of data and output logits for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e65e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple classifier\n",
    "num_classes = 5\n",
    "model = nn.Linear(10, num_classes)  # 10 input features, 5 classes\n",
    "\n",
    "# Batch of 8 samples, each with 10 features\n",
    "batch_inputs = torch.randn(8, 10)\n",
    "\n",
    "# Get logits from the model\n",
    "logits = model(batch_inputs)\n",
    "\n",
    "print(f\"Input shape: {batch_inputs.shape}  # (batch_size, features)\")\n",
    "print(f\"Logits shape: {logits.shape}  # (batch_size, num_classes)\")\n",
    "print()\n",
    "print(\"Logits for first 3 samples:\")\n",
    "print(logits[:3])\n",
    "print()\n",
    "print(\"Each row contains logits for one sample across all 5 classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68733539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert batch of logits to probabilities\n",
    "softmax = nn.Softmax(dim=1)  # Apply softmax across classes (dim=1)\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "print(\"Probabilities for first 3 samples:\")\n",
    "print(probabilities[:3])\n",
    "print()\n",
    "print(\"Verify each row sums to 1.0:\")\n",
    "print(probabilities[:3].sum(dim=1))\n",
    "print()\n",
    "\n",
    "# Get predicted class (highest logit/probability)\n",
    "predicted_classes = torch.argmax(logits, dim=1)\n",
    "print(f\"Predicted classes: {predicted_classes}\")\n",
    "print()\n",
    "print(\"Each number is the predicted class (0-4) for each sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b50ace7",
   "metadata": {},
   "source": [
    "### Why Work with Logits Instead of Probabilities?\n",
    "\n",
    "You might wonder: why not just output probabilities directly? There are several important reasons to work with logits:\n",
    "\n",
    "**1. Numerical Stability**\n",
    "- Computing softmax and then log (for cross-entropy loss) can cause numerical issues\n",
    "- PyTorch's `CrossEntropyLoss` combines these operations efficiently\n",
    "- Working with logits avoids intermediate probability calculations\n",
    "\n",
    "**2. Flexibility**\n",
    "- Same logits can be used with different activation functions\n",
    "- Can apply temperature scaling to logits for calibration\n",
    "- Easier to combine with different loss functions\n",
    "\n",
    "**3. Efficiency**\n",
    "- One less activation function to compute during forward pass\n",
    "- Loss functions that accept logits are optimized for this\n",
    "\n",
    "**4. Convention**\n",
    "- Standard practice in deep learning\n",
    "- Most pre-trained models output logits\n",
    "- Makes code more compatible with existing tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: CrossEntropyLoss expects logits, not probabilities\n",
    "\n",
    "# Create some logits and targets\n",
    "logits = torch.tensor([\n",
    "    [2.0, 1.0, 0.1],  # Sample 1\n",
    "    [0.5, 2.5, 1.0],  # Sample 2\n",
    "    [1.0, 0.5, 2.0]   # Sample 3\n",
    "])\n",
    "targets = torch.tensor([0, 1, 2])  # True class indices\n",
    "\n",
    "print(\"Logits:\")\n",
    "print(logits)\n",
    "print(f\"\\nTargets: {targets}\")\n",
    "print()\n",
    "\n",
    "# CrossEntropyLoss works directly with logits\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(logits, targets)\n",
    "\n",
    "print(f\"Cross-Entropy Loss: {loss.item():.4f}\")\n",
    "print()\n",
    "print(\"Note: CrossEntropyLoss internally:\")\n",
    "print(\"  1. Applies softmax to logits\")\n",
    "print(\"  2. Takes log of probabilities\")\n",
    "print(\"  3. Computes negative log-likelihood\")\n",
    "print(\"  All in one numerically stable operation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e40de3",
   "metadata": {},
   "source": [
    "### Understanding Logit Magnitudes\n",
    "\n",
    "The magnitude of logits tells you about the model's confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66140121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confident predictions (large differences in logits)\n",
    "confident_logits = torch.tensor([10.0, 1.0, 0.5])\n",
    "confident_probs = nn.Softmax(dim=0)(confident_logits)\n",
    "\n",
    "print(\"Confident predictions (large logit differences):\")\n",
    "print(f\"Logits: {confident_logits}\")\n",
    "print(f\"Probabilities: {confident_probs}\")\n",
    "print(f\"Max probability: {confident_probs.max().item():.1%}\")\n",
    "print()\n",
    "\n",
    "# Uncertain predictions (small differences in logits)\n",
    "uncertain_logits = torch.tensor([1.0, 0.9, 0.8])\n",
    "uncertain_probs = nn.Softmax(dim=0)(uncertain_logits)\n",
    "\n",
    "print(\"Uncertain predictions (small logit differences):\")\n",
    "print(f\"Logits: {uncertain_logits}\")\n",
    "print(f\"Probabilities: {uncertain_probs}\")\n",
    "print(f\"Max probability: {uncertain_probs.max().item():.1%}\")\n",
    "print()\n",
    "\n",
    "print(\"Key insight:\")\n",
    "print(\"  Large differences in logits → High confidence (one class dominates)\")\n",
    "print(\"  Small differences in logits → Low confidence (classes are similar)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5144d234",
   "metadata": {},
   "source": [
    "### Negative Logits Are Normal!\n",
    "\n",
    "Unlike probabilities (which must be between 0 and 1), logits can be any real number, including negative values. This is perfectly fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f04550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits with negative values\n",
    "logits_with_negatives = torch.tensor([-2.0, -1.0, 3.0, -0.5])\n",
    "\n",
    "print(\"Logits (including negative values):\")\n",
    "print(logits_with_negatives)\n",
    "print()\n",
    "\n",
    "# Convert to probabilities\n",
    "probs = nn.Softmax(dim=0)(logits_with_negatives)\n",
    "\n",
    "print(\"Probabilities (after softmax):\")\n",
    "print(probs)\n",
    "print()\n",
    "print(\"Observations:\")\n",
    "print(\"  - Negative logits become small (but positive) probabilities\")\n",
    "print(\"  - The highest logit (3.0) gets the highest probability\")\n",
    "print(\"  - All probabilities are positive and sum to 1.0\")\n",
    "print(f\"  - Sum: {probs.sum().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2bc0f5",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**What are logits?**\n",
    "- Raw, unnormalized scores output by a neural network\n",
    "- Can be any real number (positive, negative, or zero)\n",
    "- Represent the model's confidence before normalization\n",
    "- Higher logit = model thinks this option is more likely\n",
    "\n",
    "**Logits vs Probabilities:**\n",
    "```\n",
    "Logits:         [-2.0, 1.0, 3.0]  # Raw scores, any range\n",
    "                      ↓ softmax\n",
    "Probabilities:  [0.02, 0.12, 0.86]  # Normalized, sum to 1.0\n",
    "```\n",
    "\n",
    "**Converting Logits to Probabilities:**\n",
    "```python\n",
    "# For multi-class classification\n",
    "probabilities = nn.Softmax(dim=1)(logits)\n",
    "\n",
    "# For binary classification\n",
    "probabilities = torch.sigmoid(logits)\n",
    "```\n",
    "\n",
    "**Why Use Logits?**\n",
    "1. **Numerical stability**: Loss functions work better with logits\n",
    "2. **Efficiency**: One less computation in forward pass\n",
    "3. **Flexibility**: Can apply different transformations\n",
    "4. **Convention**: Standard practice in deep learning\n",
    "\n",
    "**Important Points:**\n",
    "- `nn.CrossEntropyLoss()` expects logits, not probabilities\n",
    "- Negative logits are perfectly normal\n",
    "- Large differences in logits = high confidence\n",
    "- Small differences in logits = low confidence\n",
    "- Use `torch.argmax(logits, dim=1)` to get predicted classes\n",
    "\n",
    "**Common Pattern:**\n",
    "```python\n",
    "# During training\n",
    "logits = model(inputs)  # Get raw scores\n",
    "loss = criterion(logits, targets)  # Loss function handles conversion\n",
    "\n",
    "# During inference (when you need probabilities)\n",
    "logits = model(inputs)\n",
    "probabilities = nn.Softmax(dim=1)(logits)\n",
    "predicted_class = torch.argmax(logits, dim=1)\n",
    "```\n",
    "\n",
    "**The Big Picture:**\n",
    "\n",
    "Logits are the \"raw thoughts\" of your neural network before they're converted into human-interpretable probabilities. Understanding logits helps you:\n",
    "- Debug model outputs\n",
    "- Choose appropriate loss functions\n",
    "- Interpret model confidence\n",
    "- Write efficient training code\n",
    "\n",
    "Most PyTorch models output logits by default, and most loss functions expect logits as input. This is by design for numerical stability and efficiency!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5fa3e9",
   "metadata": {},
   "source": [
    "## Y-hat (ŷ) - Model Predictions\n",
    "\n",
    "**What is Y-hat (ŷ)?**\n",
    "\n",
    "In machine learning notation, **ŷ** (pronounced \"y-hat\") represents the **predicted values** from your model. It's a standard mathematical notation:\n",
    "- **y** = actual/true values (ground truth, targets, labels)\n",
    "- **ŷ** = predicted values (model outputs, predictions)\n",
    "- The \"hat\" symbol (^) indicates an estimated or predicted value\n",
    "\n",
    "**Why does Y-hat matter?**\n",
    "\n",
    "Y-hat is central to machine learning because:\n",
    "- It's what your model produces after training\n",
    "- Comparing ŷ to y tells you how well your model performs\n",
    "- The difference (y - ŷ) is the error/loss you're trying to minimize\n",
    "- In code, ŷ is often called `predictions`, `outputs`, `y_pred`, or `y_hat`\n",
    "\n",
    "**The fundamental equation of supervised learning:**\n",
    "```\n",
    "ŷ = f(x)  where:\n",
    "  x = input features\n",
    "  f = your model (neural network)\n",
    "  ŷ = predictions\n",
    "```\n",
    "\n",
    "**The training goal:**\n",
    "```\n",
    "Minimize: Loss(ŷ, y) = Loss(f(x), y)\n",
    "Make ŷ as close to y as possible!\n",
    "```\n",
    "\n",
    "Let's explore predictions in different contexts!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ab144",
   "metadata": {},
   "source": [
    "### Making Predictions with a Model\n",
    "\n",
    "Let's see how to generate predictions (ŷ) from a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679f3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a simple linear model\n",
    "model = nn.Linear(3, 1)  # 3 input features, 1 output\n",
    "\n",
    "# Input data (x)\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0],\n",
    "                  [7.0, 8.0, 9.0]])\n",
    "\n",
    "# True values (y) - what we're trying to predict\n",
    "y = torch.tensor([[10.0],\n",
    "                  [20.0],\n",
    "                  [30.0]])\n",
    "\n",
    "print(\"Input (x):\")\n",
    "print(x)\n",
    "print(f\"Shape: {x.shape}  # (3 samples, 3 features)\")\n",
    "print()\n",
    "\n",
    "print(\"True values (y):\")\n",
    "print(y)\n",
    "print(f\"Shape: {y.shape}  # (3 samples, 1 output)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8aef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions (ŷ)\n",
    "y_hat = model(x)\n",
    "\n",
    "print(\"Predictions (ŷ):\")\n",
    "print(y_hat)\n",
    "print(f\"Shape: {y_hat.shape}  # (3 samples, 1 output)\")\n",
    "print()\n",
    "\n",
    "# Compare predictions to true values\n",
    "print(\"Comparison:\")\n",
    "print(\"Sample | True (y) | Predicted (ŷ) | Error (y - ŷ)\")\n",
    "print(\"-\" * 55)\n",
    "for i in range(len(y)):\n",
    "    error = y[i].item() - y_hat[i].item()\n",
    "    print(f\"  {i}    | {y[i].item():7.2f}  | {y_hat[i].item():13.2f}  | {error:13.2f}\")\n",
    "print()\n",
    "print(\"Note: The model hasn't been trained yet, so predictions are random!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a3a7b",
   "metadata": {},
   "source": [
    "### Predictions in Regression Tasks\n",
    "\n",
    "In regression, ŷ represents continuous predicted values. Let's see a more realistic example with a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ae8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset: y = 2x + 3 + noise\n",
    "torch.manual_seed(42)\n",
    "x_train = torch.linspace(0, 10, 50).reshape(-1, 1)\n",
    "y_train = 2 * x_train + 3 + torch.randn(50, 1) * 0.5\n",
    "\n",
    "# Create and train a simple model\n",
    "model = nn.Linear(1, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Quick training loop\n",
    "for epoch in range(100):\n",
    "    # Forward pass: compute predictions\n",
    "    y_hat = model(x_train)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(y_hat, y_train)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Model trained!\")\n",
    "print(f\"Final loss: {loss.item():.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc8e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on new data\n",
    "x_test = torch.tensor([[2.0], [5.0], [8.0]])\n",
    "y_hat = model(x_test)\n",
    "\n",
    "print(\"Test predictions:\")\n",
    "print(\"Input (x) | Predicted (ŷ)\")\n",
    "print(\"-\" * 30)\n",
    "for i in range(len(x_test)):\n",
    "    print(f\"  {x_test[i].item():5.1f}   | {y_hat[i].item():13.2f}\")\n",
    "print()\n",
    "\n",
    "# Expected values (y = 2x + 3)\n",
    "print(\"Expected values (y = 2x + 3):\")\n",
    "for i in range(len(x_test)):\n",
    "    expected = 2 * x_test[i].item() + 3\n",
    "    print(f\"  x={x_test[i].item():.1f}: y = {expected:.1f}\")\n",
    "print()\n",
    "print(\"The model learned to approximate the relationship!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe361a2",
   "metadata": {},
   "source": [
    "### Predictions in Classification Tasks\n",
    "\n",
    "In classification, the process is slightly different:\n",
    "1. Model outputs logits (raw scores)\n",
    "2. Apply softmax to get probabilities\n",
    "3. Take argmax to get predicted class\n",
    "\n",
    "So ŷ can refer to either the probabilities or the predicted class labels, depending on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699526df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple classifier\n",
    "num_classes = 4\n",
    "model = nn.Linear(5, num_classes)\n",
    "\n",
    "# Sample input\n",
    "x = torch.randn(3, 5)  # 3 samples, 5 features\n",
    "\n",
    "# Get logits (raw predictions)\n",
    "logits = model(x)\n",
    "print(\"Logits (raw model outputs):\")\n",
    "print(logits)\n",
    "print(f\"Shape: {logits.shape}  # (3 samples, 4 classes)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87cef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to probabilities (ŷ as probabilities)\n",
    "y_hat_probs = nn.Softmax(dim=1)(logits)\n",
    "\n",
    "print(\"Predictions as probabilities (ŷ):\")\n",
    "print(y_hat_probs)\n",
    "print()\n",
    "print(\"Each row sums to 1.0:\")\n",
    "print(y_hat_probs.sum(dim=1))\n",
    "print()\n",
    "\n",
    "# Get predicted classes (ŷ as class labels)\n",
    "y_hat_classes = torch.argmax(logits, dim=1)\n",
    "\n",
    "print(\"Predictions as class labels (ŷ):\")\n",
    "print(y_hat_classes)\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "for i in range(len(y_hat_classes)):\n",
    "    predicted_class = y_hat_classes[i].item()\n",
    "    confidence = y_hat_probs[i, predicted_class].item()\n",
    "    print(f\"  Sample {i}: Predicted class {predicted_class} with {confidence:.1%} confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dcb179",
   "metadata": {},
   "source": [
    "### Batch Predictions\n",
    "\n",
    "In practice, you'll make predictions on batches of data for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08329873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model and batch of data\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ")\n",
    "\n",
    "# Batch of 32 samples\n",
    "batch_size = 32\n",
    "x_batch = torch.randn(batch_size, 10)\n",
    "\n",
    "# Generate predictions for the entire batch at once\n",
    "y_hat_batch = model(x_batch)\n",
    "\n",
    "print(f\"Input batch shape: {x_batch.shape}  # (32 samples, 10 features)\")\n",
    "print(f\"Predictions shape: {y_hat_batch.shape}  # (32 samples, 1 output)\")\n",
    "print()\n",
    "print(\"First 5 predictions:\")\n",
    "print(y_hat_batch[:5])\n",
    "print()\n",
    "print(\"Batch processing is much faster than predicting one sample at a time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2dc922",
   "metadata": {},
   "source": [
    "### Using Predictions for Evaluation\n",
    "\n",
    "Once you have predictions (ŷ) and true values (y), you can compute various metrics to evaluate your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression example: compute metrics\n",
    "y_true = torch.tensor([3.0, 5.0, 7.0, 9.0, 11.0])\n",
    "y_pred = torch.tensor([2.8, 5.2, 6.9, 9.3, 10.8])\n",
    "\n",
    "print(\"True values (y):    \", y_true.tolist())\n",
    "print(\"Predictions (ŷ):    \", y_pred.tolist())\n",
    "print()\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = ((y_pred - y_true) ** 2).mean()\n",
    "print(f\"Mean Squared Error: {mse.item():.4f}\")\n",
    "print()\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = (y_pred - y_true).abs().mean()\n",
    "print(f\"Mean Absolute Error: {mae.item():.4f}\")\n",
    "print()\n",
    "\n",
    "# R² Score (coefficient of determination)\n",
    "ss_res = ((y_true - y_pred) ** 2).sum()\n",
    "ss_tot = ((y_true - y_true.mean()) ** 2).sum()\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "print(f\"R² Score: {r2.item():.4f}\")\n",
    "print(\"(1.0 = perfect predictions, 0.0 = as good as predicting the mean)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification example: compute accuracy\n",
    "y_true_classes = torch.tensor([0, 1, 2, 1, 0, 2, 1, 0])\n",
    "y_pred_classes = torch.tensor([0, 1, 2, 2, 0, 2, 1, 1])\n",
    "\n",
    "print(\"True classes (y):      \", y_true_classes.tolist())\n",
    "print(\"Predicted classes (ŷ): \", y_pred_classes.tolist())\n",
    "print()\n",
    "\n",
    "# Accuracy: fraction of correct predictions\n",
    "correct = (y_pred_classes == y_true_classes).sum().item()\n",
    "total = len(y_true_classes)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"Correct predictions: {correct}/{total}\")\n",
    "print(f\"Accuracy: {accuracy:.1%}\")\n",
    "print()\n",
    "\n",
    "# Show which predictions were correct\n",
    "print(\"Prediction details:\")\n",
    "for i in range(len(y_true_classes)):\n",
    "    true_class = y_true_classes[i].item()\n",
    "    pred_class = y_pred_classes[i].item()\n",
    "    status = \"✓\" if true_class == pred_class else \"✗\"\n",
    "    print(f\"  Sample {i}: True={true_class}, Pred={pred_class} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed1fa72",
   "metadata": {},
   "source": [
    "### Predictions During Training vs Inference\n",
    "\n",
    "The way you use predictions differs between training and inference:\n",
    "\n",
    "**During Training:**\n",
    "- Generate ŷ to compute loss: `loss = criterion(y_hat, y)`\n",
    "- Use loss for backpropagation: `loss.backward()`\n",
    "- Update model parameters: `optimizer.step()`\n",
    "- Model is in training mode: `model.train()`\n",
    "\n",
    "**During Inference (Evaluation/Prediction):**\n",
    "- Generate ŷ for new, unseen data\n",
    "- No gradient computation needed: `with torch.no_grad()`\n",
    "- Model is in evaluation mode: `model.eval()`\n",
    "- Use predictions for decision-making or analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training mode\n",
    "model = nn.Linear(5, 1)\n",
    "model.train()  # Set to training mode\n",
    "\n",
    "x_train = torch.randn(10, 5)\n",
    "y_train = torch.randn(10, 1)\n",
    "\n",
    "# Predictions during training (gradients tracked)\n",
    "y_hat_train = model(x_train)\n",
    "print(\"Training predictions:\")\n",
    "print(f\"  Shape: {y_hat_train.shape}\")\n",
    "print(f\"  Requires grad: {y_hat_train.requires_grad}\")\n",
    "print(f\"  Can compute loss and backprop: Yes\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference mode\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "x_test = torch.randn(5, 5)\n",
    "\n",
    "# Predictions during inference (no gradients)\n",
    "with torch.no_grad():\n",
    "    y_hat_test = model(x_test)\n",
    "\n",
    "print(\"Inference predictions:\")\n",
    "print(f\"  Shape: {y_hat_test.shape}\")\n",
    "print(f\"  Requires grad: {y_hat_test.requires_grad}\")\n",
    "print(f\"  Memory efficient: Yes (no gradient tracking)\")\n",
    "print(f\"  Faster: Yes (no computation graph)\")\n",
    "print()\n",
    "print(\"Use torch.no_grad() during inference to save memory and speed up computation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a957b09",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**What is Y-hat (ŷ)?**\n",
    "- Mathematical notation for predicted values\n",
    "- The output of your model: `y_hat = model(x)`\n",
    "- Contrasts with y (true values/targets)\n",
    "- In code: `predictions`, `outputs`, `y_pred`, `y_hat`\n",
    "\n",
    "**The Fundamental Relationship:**\n",
    "```\n",
    "Input (x) → Model (f) → Predictions (ŷ)\n",
    "Compare ŷ with y → Compute Loss → Update Model\n",
    "```\n",
    "\n",
    "**Generating Predictions:**\n",
    "```python\n",
    "# Simple prediction\n",
    "y_hat = model(x)\n",
    "\n",
    "# Batch prediction\n",
    "y_hat_batch = model(x_batch)\n",
    "\n",
    "# Classification (get class labels)\n",
    "logits = model(x)\n",
    "y_hat_classes = torch.argmax(logits, dim=1)\n",
    "```\n",
    "\n",
    "**Types of Predictions:**\n",
    "- **Regression**: Continuous values (prices, temperatures)\n",
    "- **Classification**: Class labels or probabilities\n",
    "- **Logits**: Raw scores before activation\n",
    "- **Probabilities**: After softmax/sigmoid\n",
    "\n",
    "**Using Predictions:**\n",
    "1. **Training**: Compute loss, backpropagate, update weights\n",
    "2. **Evaluation**: Calculate metrics (accuracy, MSE, etc.)\n",
    "3. **Inference**: Make predictions on new data\n",
    "4. **Analysis**: Understand model behavior\n",
    "\n",
    "**Best Practices:**\n",
    "```python\n",
    "# During training\n",
    "model.train()\n",
    "y_hat = model(x)\n",
    "loss = criterion(y_hat, y)\n",
    "loss.backward()\n",
    "\n",
    "# During inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model(x)\n",
    "```\n",
    "\n",
    "**Common Evaluation Metrics:**\n",
    "- **Regression**: MSE, MAE, R², RMSE\n",
    "- **Classification**: Accuracy, Precision, Recall, F1-Score\n",
    "- **Probabilistic**: Cross-Entropy, Log-Likelihood\n",
    "\n",
    "**The Big Picture:**\n",
    "\n",
    "Y-hat (ŷ) is the bridge between your model and the real world:\n",
    "- During training: ŷ helps the model learn by comparing to y\n",
    "- During evaluation: ŷ tells you how well the model performs\n",
    "- During deployment: ŷ is what you deliver to users\n",
    "\n",
    "Understanding predictions means understanding:\n",
    "- What your model is trying to learn\n",
    "- How to measure success\n",
    "- When your model is ready for production\n",
    "- How to debug and improve performance\n",
    "\n",
    "Every machine learning workflow revolves around generating good predictions (ŷ) that match the true values (y) as closely as possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae30d09",
   "metadata": {},
   "source": [
    "## Gradient Descent - The Optimization Algorithm\n",
    "\n",
    "**What is gradient descent?**\n",
    "\n",
    "Gradient descent is the fundamental optimization algorithm used to train neural networks. It's a method for finding the minimum of a function by iteratively moving in the direction of steepest descent.\n",
    "\n",
    "**The core idea:**\n",
    "1. Compute the gradient (slope) of the loss with respect to each parameter\n",
    "2. Move parameters in the opposite direction of the gradient\n",
    "3. Repeat until the loss stops decreasing\n",
    "\n",
    "**Why does gradient descent matter?**\n",
    "\n",
    "Gradient descent is how neural networks learn:\n",
    "- It tells us **which direction** to adjust parameters\n",
    "- It tells us **how much** to adjust them\n",
    "- It enables learning from data through backpropagation\n",
    "- All modern deep learning optimizers are variants of gradient descent\n",
    "\n",
    "**The update rule:**\n",
    "```\n",
    "θ_new = θ_old - learning_rate × gradient\n",
    "```\n",
    "Where θ (theta) represents model parameters (weights and biases).\n",
    "\n",
    "Let's see gradient descent in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c38b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Simple example: minimize f(x) = (x - 3)²\n",
    "# The minimum is at x = 3\n",
    "\n",
    "# Start with a random guess\n",
    "x = torch.tensor([0.0], requires_grad=True)\n",
    "learning_rate = 0.1\n",
    "\n",
    "print(\"Minimizing f(x) = (x - 3)² using gradient descent\")\n",
    "print(\"Starting point: x = 0.0\")\n",
    "print(\"Target: x = 3.0 (where f(x) is minimum)\")\n",
    "print()\n",
    "\n",
    "# Perform gradient descent steps\n",
    "for step in range(20):\n",
    "    # Compute function value\n",
    "    f = (x - 3) ** 2\n",
    "    \n",
    "    # Compute gradient\n",
    "    f.backward()\n",
    "    \n",
    "    # Manual gradient descent update\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad\n",
    "    \n",
    "    # Clear gradients for next iteration\n",
    "    x.grad.zero_()\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step:2d}: x = {x.item():.4f}, f(x) = {f.item():.4f}\")\n",
    "\n",
    "print()\n",
    "print(f\"Final: x = {x.item():.4f} (close to 3.0!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac2587",
   "metadata": {},
   "source": [
    "### Gradient Descent for Neural Networks\n",
    "\n",
    "In neural networks, we apply gradient descent to all parameters (weights and biases) simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a420c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model\n",
    "model = nn.Linear(2, 1)\n",
    "\n",
    "# Sample data\n",
    "x = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "y = torch.tensor([[5.0], [11.0], [17.0]])\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(\"Training with manual gradient descent\")\n",
    "print()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Manual gradient descent update\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    \n",
    "    # Clear gradients\n",
    "    model.zero_grad()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Model trained! Loss decreased through gradient descent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e38ecdd",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- Gradient descent finds minimum by following the negative gradient\n",
    "- Update rule: `parameter = parameter - learning_rate × gradient`\n",
    "- Applied to all model parameters simultaneously\n",
    "- Forms the basis of all neural network training\n",
    "- In practice, we use optimizers (like Adam) that implement improved versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de15f8a",
   "metadata": {},
   "source": [
    "## Learning Rate - Controlling the Step Size\n",
    "\n",
    "**What is the learning rate?**\n",
    "\n",
    "The learning rate (often denoted as α or lr) is a hyperparameter that controls **how big of a step** we take when updating model parameters during training. It's the multiplier in the gradient descent update:\n",
    "\n",
    "```\n",
    "parameter_new = parameter_old - learning_rate × gradient\n",
    "```\n",
    "\n",
    "**Why does learning rate matter?**\n",
    "\n",
    "The learning rate is **one of the most important hyperparameters** in deep learning:\n",
    "- **Too high**: Training becomes unstable, loss explodes, model diverges\n",
    "- **Too low**: Training is very slow, may get stuck in local minima\n",
    "- **Just right**: Fast, stable convergence to good solutions\n",
    "\n",
    "Choosing the right learning rate can make the difference between a model that trains in minutes versus hours, or between a model that works versus one that fails completely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d4efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a simple dataset\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(100, 5)\n",
    "y = torch.randn(100, 1)\n",
    "\n",
    "def train_with_lr(learning_rate, epochs=50):\n",
    "    \"\"\"Train a model with a specific learning rate.\"\"\"\n",
    "    model = nn.Linear(5, 1)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Try different learning rates\n",
    "lr_too_low = 0.0001\n",
    "lr_good = 0.01\n",
    "lr_too_high = 1.0\n",
    "\n",
    "print(\"Comparing different learning rates...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70619a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate too low\n",
    "losses_low = train_with_lr(lr_too_low)\n",
    "print(f\"Learning rate = {lr_too_low} (TOO LOW)\")\n",
    "print(f\"  Initial loss: {losses_low[0]:.4f}\")\n",
    "print(f\"  Final loss:   {losses_low[-1]:.4f}\")\n",
    "print(f\"  Improvement:  {losses_low[0] - losses_low[-1]:.4f}\")\n",
    "print(\"  → Very slow progress!\")\n",
    "print()\n",
    "\n",
    "# Good learning rate\n",
    "losses_good = train_with_lr(lr_good)\n",
    "print(f\"Learning rate = {lr_good} (GOOD)\")\n",
    "print(f\"  Initial loss: {losses_good[0]:.4f}\")\n",
    "print(f\"  Final loss:   {losses_good[-1]:.4f}\")\n",
    "print(f\"  Improvement:  {losses_good[0] - losses_good[-1]:.4f}\")\n",
    "print(\"  → Fast, stable convergence!\")\n",
    "print()\n",
    "\n",
    "# Learning rate too high\n",
    "losses_high = train_with_lr(lr_too_high)\n",
    "print(f\"Learning rate = {lr_too_high} (TOO HIGH)\")\n",
    "print(f\"  Initial loss: {losses_high[0]:.4f}\")\n",
    "print(f\"  Final loss:   {losses_high[-1]:.4f}\")\n",
    "print(f\"  Improvement:  {losses_high[0] - losses_high[-1]:.4f}\")\n",
    "print(\"  → Unstable, may diverge!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b12de05",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- Learning rate controls step size in parameter updates\n",
    "- Too high → unstable training, divergence\n",
    "- Too low → very slow training\n",
    "- Typical values: 0.001 to 0.1 (depends on optimizer and problem)\n",
    "- Modern optimizers (like Adam) adapt the learning rate automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c95d6",
   "metadata": {},
   "source": [
    "## Adam Optimizer - Adaptive Learning Rates\n",
    "\n",
    "**What is Adam?**\n",
    "\n",
    "Adam (Adaptive Moment Estimation) is one of the most popular optimization algorithms in deep learning. It's an advanced version of gradient descent that:\n",
    "- Adapts the learning rate for each parameter individually\n",
    "- Uses momentum to smooth out updates\n",
    "- Combines the best of RMSprop and momentum methods\n",
    "\n",
    "**Why use Adam?**\n",
    "\n",
    "Adam is the **default choice** for most deep learning tasks because:\n",
    "- Works well with minimal hyperparameter tuning\n",
    "- Handles sparse gradients effectively\n",
    "- Adapts to different parameter scales automatically\n",
    "- Fast convergence in practice\n",
    "- Robust across different architectures and datasets\n",
    "\n",
    "**Adam vs SGD:**\n",
    "- **SGD**: Simple, same learning rate for all parameters\n",
    "- **Adam**: Adaptive, different effective learning rate per parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a94bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create a model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ")\n",
    "\n",
    "# Create Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Adam optimizer created!\")\n",
    "print(f\"Learning rate: 0.001\")\n",
    "print(f\"Number of parameter groups: {len(optimizer.param_groups)}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SGD vs Adam\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(100, 10)\n",
    "y = torch.randn(100, 1)\n",
    "\n",
    "def train_model(optimizer_name, lr=0.01, epochs=100):\n",
    "    model = nn.Linear(10, 1)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "sgd_loss = train_model('SGD', lr=0.01)\n",
    "adam_loss = train_model('Adam', lr=0.01)\n",
    "\n",
    "print(f\"Final loss with SGD:  {sgd_loss:.4f}\")\n",
    "print(f\"Final loss with Adam: {adam_loss:.4f}\")\n",
    "print()\n",
    "print(\"Adam often converges faster and to better solutions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44233a7",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- Adam is the most popular optimizer in deep learning\n",
    "- Adapts learning rate for each parameter automatically\n",
    "- Created with: `optimizer = optim.Adam(model.parameters(), lr=0.001)`\n",
    "- Default learning rate: 0.001 (works well for most cases)\n",
    "- Use Adam unless you have a specific reason to use something else"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4184e54",
   "metadata": {},
   "source": [
    "## optimizer.zero_grad() - Clearing Previous Gradients\n",
    "\n",
    "**What is optimizer.zero_grad()?**\n",
    "\n",
    "`optimizer.zero_grad()` clears (zeros out) the gradients of all parameters tracked by the optimizer. This is a crucial step in the training loop that must be called before each backward pass.\n",
    "\n",
    "**Why is this necessary?**\n",
    "\n",
    "By default, PyTorch **accumulates gradients** - when you call `.backward()`, new gradients are **added** to existing gradients rather than replacing them. This is useful for some advanced techniques, but for standard training, you want fresh gradients for each batch.\n",
    "\n",
    "**What happens if you forget?**\n",
    "- Gradients keep accumulating across iterations\n",
    "- Parameter updates become incorrect\n",
    "- Loss may increase instead of decrease\n",
    "- Training fails or becomes unstable\n",
    "\n",
    "**The training loop pattern:**\n",
    "```python\n",
    "for batch in dataloader:\n",
    "    optimizer.zero_grad()  # 1. Clear old gradients\n",
    "    output = model(batch)  # 2. Forward pass\n",
    "    loss = criterion(output, target)  # 3. Compute loss\n",
    "    loss.backward()  # 4. Compute gradients\n",
    "    optimizer.step()  # 5. Update parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a26a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a simple model\n",
    "model = nn.Linear(3, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Sample data\n",
    "x = torch.randn(5, 3)\n",
    "y = torch.randn(5, 1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"Initial gradients (should be None):\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.grad}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a23338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First backward pass\n",
    "output = model(x)\n",
    "loss = criterion(output, y)\n",
    "loss.backward()\n",
    "\n",
    "print(\"After first backward pass:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name} gradient: {param.grad.abs().mean().item():.6f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second backward pass WITHOUT zero_grad() - gradients accumulate!\n",
    "output = model(x)\n",
    "loss = criterion(output, y)\n",
    "loss.backward()\n",
    "\n",
    "print(\"After second backward pass (WITHOUT zero_grad):\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name} gradient: {param.grad.abs().mean().item():.6f}\")\n",
    "print(\"Notice: Gradients are roughly 2x larger (accumulated!)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15231344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now clear gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(\"After optimizer.zero_grad():\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name} gradient: {param.grad.abs().mean().item():.6f}\")\n",
    "print(\"Gradients are now zero!\")\n",
    "print()\n",
    "\n",
    "# Third backward pass with fresh gradients\n",
    "output = model(x)\n",
    "loss = criterion(output, y)\n",
    "loss.backward()\n",
    "\n",
    "print(\"After third backward pass (WITH zero_grad):\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name} gradient: {param.grad.abs().mean().item():.6f}\")\n",
    "print(\"Gradients are fresh and correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf6f3f",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- Call `optimizer.zero_grad()` at the start of each training iteration\n",
    "- Clears accumulated gradients from previous iterations\n",
    "- Forgetting this causes incorrect gradient accumulation\n",
    "- Alternative: `model.zero_grad()` does the same thing\n",
    "- Essential for correct training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c93e6",
   "metadata": {},
   "source": [
    "## optimizer.step() - Updating Parameters\n",
    "\n",
    "**What is optimizer.step()?**\n",
    "\n",
    "`optimizer.step()` performs a single optimization step - it updates all model parameters based on their gradients. This is where the actual learning happens!\n",
    "\n",
    "**What does it do?**\n",
    "- Reads the gradients computed by `.backward()`\n",
    "- Applies the optimization algorithm (SGD, Adam, etc.)\n",
    "- Updates each parameter: `param = param - lr × gradient` (for SGD)\n",
    "- Makes the model better at its task\n",
    "\n",
    "**The training cycle:**\n",
    "```\n",
    "1. optimizer.zero_grad()  → Clear old gradients\n",
    "2. loss.backward()        → Compute new gradients\n",
    "3. optimizer.step()       → Update parameters using gradients\n",
    "```\n",
    "\n",
    "Without `optimizer.step()`, gradients are computed but parameters never change - the model doesn't learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189cdb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create model and optimizer\n",
    "model = nn.Linear(2, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Save initial parameters\n",
    "initial_weight = model.weight.data.clone()\n",
    "initial_bias = model.bias.data.clone()\n",
    "\n",
    "print(\"Initial parameters:\")\n",
    "print(f\"  Weight: {initial_weight}\")\n",
    "print(f\"  Bias: {initial_bias}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e870ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one training step\n",
    "x = torch.tensor([[1.0, 2.0]])\n",
    "y = torch.tensor([[5.0]])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "loss = criterion(output, y)\n",
    "print(f\"Loss before update: {loss.item():.4f}\")\n",
    "print()\n",
    "\n",
    "# Backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "print(\"Gradients computed:\")\n",
    "print(f\"  Weight gradient: {model.weight.grad}\")\n",
    "print(f\"  Bias gradient: {model.bias.grad}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update parameters\n",
    "optimizer.step()\n",
    "\n",
    "print(\"After optimizer.step():\")\n",
    "print(f\"  Weight: {model.weight.data}\")\n",
    "print(f\"  Bias: {model.bias.data}\")\n",
    "print()\n",
    "\n",
    "# Check that parameters changed\n",
    "weight_change = (model.weight.data - initial_weight).abs().sum().item()\n",
    "bias_change = (model.bias.data - initial_bias).abs().sum().item()\n",
    "\n",
    "print(f\"Weight changed by: {weight_change:.6f}\")\n",
    "print(f\"Bias changed by: {bias_change:.6f}\")\n",
    "print()\n",
    "\n",
    "# Compute loss again\n",
    "output = model(x)\n",
    "new_loss = criterion(output, y)\n",
    "print(f\"Loss after update: {new_loss.item():.4f}\")\n",
    "print(\"Loss decreased - the model improved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b111eb2",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- `optimizer.step()` updates all model parameters\n",
    "- Must be called after `loss.backward()`\n",
    "- This is where learning actually happens\n",
    "- Each step makes the model slightly better\n",
    "- Different optimizers (SGD, Adam) use different update rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beea30e",
   "metadata": {},
   "source": [
    "## Epochs - Complete Passes Through Data\n",
    "\n",
    "**What is an epoch?**\n",
    "\n",
    "An **epoch** is one complete pass through the entire training dataset. If you have 1000 training samples, one epoch means the model has seen all 1000 samples once.\n",
    "\n",
    "**Why multiple epochs?**\n",
    "\n",
    "Models rarely learn perfectly from seeing data just once. Training for multiple epochs allows:\n",
    "- The model to see patterns multiple times\n",
    "- Gradual refinement of parameters\n",
    "- Better generalization to unseen data\n",
    "- Convergence to better solutions\n",
    "\n",
    "**How many epochs?**\n",
    "- Too few: Underfitting (model hasn't learned enough)\n",
    "- Too many: Overfitting (model memorizes training data)\n",
    "- Just right: Model learns patterns without memorizing\n",
    "\n",
    "Typical range: 10-1000 epochs, depending on:\n",
    "- Dataset size (smaller datasets need more epochs)\n",
    "- Model complexity\n",
    "- Learning rate\n",
    "- Task difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3598a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create dataset\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(100, 5)\n",
    "y = torch.randn(100, 1)\n",
    "\n",
    "# Create model\n",
    "model = nn.Linear(5, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train for multiple epochs\n",
    "num_epochs = 50\n",
    "print(f\"Training for {num_epochs} epochs...\")\n",
    "print()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Training complete! Loss decreased over epochs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07205300",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- One epoch = one complete pass through all training data\n",
    "- Multiple epochs allow gradual learning\n",
    "- Monitor loss across epochs to track progress\n",
    "- Stop when loss plateaus or starts increasing (overfitting)\n",
    "- Use validation data to determine optimal number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd09056",
   "metadata": {},
   "source": [
    "## Training Loop - Putting It All Together\n",
    "\n",
    "**What is a training loop?**\n",
    "\n",
    "The training loop is the complete workflow that trains a neural network. It combines all the concepts we've learned into a structured process that:\n",
    "- Iterates through the dataset multiple times (epochs)\n",
    "- Processes data in batches\n",
    "- Computes predictions and loss\n",
    "- Updates model parameters\n",
    "- Monitors progress\n",
    "\n",
    "**The standard training loop structure:**\n",
    "```python\n",
    "for epoch in range(num_epochs):  # Multiple passes through data\n",
    "    for batch in dataloader:  # Process data in batches\n",
    "        # 1. Forward pass\n",
    "        predictions = model(inputs)\n",
    "        loss = criterion(predictions, targets)\n",
    "        \n",
    "        # 2. Backward pass\n",
    "        optimizer.zero_grad()  # Clear old gradients\n",
    "        loss.backward()  # Compute new gradients\n",
    "        \n",
    "        # 3. Update parameters\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "This is the heart of deep learning - everything else supports this loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Prepare data\n",
    "torch.manual_seed(42)\n",
    "x_train = torch.randn(200, 10)  # 200 samples, 10 features\n",
    "y_train = torch.randn(200, 1)  # 200 target values\n",
    "\n",
    "# 2. Define model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1)\n",
    ")\n",
    "\n",
    "# 3. Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Model: {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee8c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training loop\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Process data in batches\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        # Get batch\n",
    "        batch_x = x_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(batch_x)\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b8c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluate the trained model\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "with torch.no_grad():  # No gradients needed for evaluation\n",
    "    # Make predictions on training data\n",
    "    train_predictions = model(x_train)\n",
    "    train_loss = criterion(train_predictions, y_train)\n",
    "    \n",
    "    print(\"Final evaluation:\")\n",
    "    print(f\"  Training loss: {train_loss.item():.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Show some predictions\n",
    "    print(\"Sample predictions:\")\n",
    "    for i in range(5):\n",
    "        print(f\"  True: {y_train[i].item():7.3f}, Predicted: {train_predictions[i].item():7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bfa0da",
   "metadata": {},
   "source": [
    "### Complete Training Loop Template\n",
    "\n",
    "Here's a complete, production-ready training loop template you can use for your own projects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4bc84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device='cpu'):\n",
    "    \"\"\"\n",
    "    Complete training loop template.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        train_loader: DataLoader for training data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer (Adam, SGD, etc.)\n",
    "        num_epochs: Number of training epochs\n",
    "        device: 'cpu' or 'cuda'\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()  # Set to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Training loop template defined!\")\n",
    "print(\"Use this as a starting point for your own projects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8145f88",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "**The Training Loop Structure:**\n",
    "```python\n",
    "for epoch in range(num_epochs):  # Outer loop: epochs\n",
    "    for batch in dataloader:  # Inner loop: batches\n",
    "        # Forward pass\n",
    "        predictions = model(inputs)\n",
    "        loss = criterion(predictions, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "**Essential Components:**\n",
    "1. **Model**: Neural network architecture\n",
    "2. **Data**: Training dataset (usually in batches)\n",
    "3. **Loss function**: Measures prediction error\n",
    "4. **Optimizer**: Updates parameters (Adam, SGD, etc.)\n",
    "5. **Epochs**: Number of complete passes through data\n",
    "\n",
    "**The Three Steps (repeated for each batch):**\n",
    "1. **Forward pass**: Compute predictions and loss\n",
    "2. **Backward pass**: Compute gradients via backpropagation\n",
    "3. **Optimization**: Update parameters using gradients\n",
    "\n",
    "**Best Practices:**\n",
    "- Always call `optimizer.zero_grad()` before `loss.backward()`\n",
    "- Use `model.train()` during training, `model.eval()` during evaluation\n",
    "- Move model and data to the same device (CPU or GPU)\n",
    "- Monitor loss to track training progress\n",
    "- Use `torch.no_grad()` during evaluation to save memory\n",
    "- Process data in batches for efficiency\n",
    "\n",
    "**Common Enhancements:**\n",
    "- **Validation**: Evaluate on separate validation set each epoch\n",
    "- **Early stopping**: Stop if validation loss stops improving\n",
    "- **Learning rate scheduling**: Adjust learning rate during training\n",
    "- **Checkpointing**: Save model periodically\n",
    "- **Logging**: Track metrics (loss, accuracy, etc.)\n",
    "- **Progress bars**: Visual feedback (tqdm library)\n",
    "\n",
    "**The Big Picture:**\n",
    "\n",
    "The training loop is where everything comes together:\n",
    "- **Tensors** hold your data\n",
    "- **Models** make predictions\n",
    "- **Loss functions** measure errors\n",
    "- **Autograd** computes gradients\n",
    "- **Optimizers** update parameters\n",
    "- **Epochs** repeat the process\n",
    "\n",
    "Master the training loop, and you can train any neural network! This pattern works for:\n",
    "- Image classification (CNNs)\n",
    "- Natural language processing (Transformers)\n",
    "- Reinforcement learning (Policy networks)\n",
    "- Generative models (GANs, VAEs)\n",
    "- Any deep learning task!\n",
    "\n",
    "**Congratulations!** You now understand the complete PyTorch training workflow. You have all the tools you need to build and train your own neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bcab42",
   "metadata": {},
   "source": [
    "## Glossary - Quick Reference for PyTorch Terms\n",
    "\n",
    "This glossary provides concise definitions for all the key PyTorch concepts covered in this notebook. Use it as a quick reference when you need to recall what a term means!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227850c2",
   "metadata": {},
   "source": [
    "**Activation Functions**: Non-linear functions applied to neuron outputs that enable neural networks to learn complex patterns. Without activation functions, stacking multiple layers would be equivalent to a single linear transformation. Common examples include ReLU, GeLU, and Softmax.\n",
    "\n",
    "**Adam Optimizer**: An adaptive learning rate optimization algorithm that combines the benefits of AdaGrad and RMSProp. Adam computes individual learning rates for each parameter and is widely used because it works well with minimal tuning. It maintains moving averages of both gradients and squared gradients.\n",
    "\n",
    "**Argmax**: An operation that returns the index (position) of the maximum value in a tensor. In classification tasks, argmax is used to convert model probabilities into predicted class labels. For example, if a model outputs [0.1, 0.7, 0.2], argmax returns 1 (the index of 0.7).\n",
    "\n",
    "**Autograd**: PyTorch's automatic differentiation system that automatically computes gradients for tensor operations. Autograd builds a computation graph as you perform operations, then uses the chain rule to compute derivatives during backpropagation. This eliminates the need to manually calculate gradients.\n",
    "\n",
    "**Backward Pass**: The process of computing gradients by propagating errors backward through the network, from output to input. During the backward pass, PyTorch uses the chain rule to calculate how much each parameter contributed to the loss. This is triggered by calling `.backward()` on a loss tensor.\n",
    "\n",
    "**Bias Parameters**: Learnable offset terms added to layer outputs that allow the model to shift activation functions. In a linear layer computing `y = Wx + b`, the bias `b` lets the model learn patterns that don't pass through the origin. Biases give the model more flexibility to fit the data.\n",
    "\n",
    "**Computation Graph**: A directed acyclic graph (DAG) that PyTorch builds to track operations performed on tensors with `requires_grad=True`. Each node represents an operation, and edges represent data flow. This graph enables automatic differentiation by recording the sequence of operations needed to compute gradients.\n",
    "\n",
    "**Device Management**: The practice of controlling whether tensors and models run on CPU or GPU. GPUs provide 10-100x speedups for deep learning workloads through massive parallelization. PyTorch requires all tensors in an operation to be on the same device.\n",
    "\n",
    "**Dimension Arguments**: Parameters (like `dim` or `axis`) that specify which dimension of a tensor an operation should work along. For example, `tensor.sum(dim=0)` sums along dimension 0 (rows), while `tensor.sum(dim=1)` sums along dimension 1 (columns). Understanding dimensions is crucial for correct tensor operations.\n",
    "\n",
    "**Dropout**: A regularization technique that randomly sets a fraction of neuron outputs to zero during training. This prevents overfitting by forcing the network to learn robust features that don't rely on specific neurons. Dropout is disabled during evaluation using `model.eval()`.\n",
    "\n",
    "**Embedding**: A learnable lookup table that converts discrete tokens (like words or IDs) into continuous vector representations. Embeddings allow neural networks to process categorical data by mapping each category to a dense vector. The embedding vectors are learned during training to capture semantic relationships.\n",
    "\n",
    "**Epochs**: Complete passes through the entire training dataset. Training typically involves multiple epochs, allowing the model to see each example multiple times. The number of epochs is a hyperparameter that balances training time with model performance.\n",
    "\n",
    "**Feed-Forward Networks**: Neural network architectures where information flows in one direction from input to output, without cycles. Also called Multi-Layer Perceptrons (MLPs), these networks consist of layers of neurons where each layer's output becomes the next layer's input. They're the foundation for more complex architectures.\n",
    "\n",
    "**Forward Pass**: The process of computing a neural network's output by passing input data through the network's layers sequentially. During the forward pass, each layer applies its transformation (weights, biases, activations) to produce outputs. This is the inference or prediction phase of the network.\n",
    "\n",
    "**GeLU (Gaussian Error Linear Unit)**: A smooth activation function that approximates `x * Φ(x)` where Φ is the cumulative distribution function of the standard normal distribution. GeLU is used in modern architectures like BERT and GPT because it provides smoother gradients than ReLU. Unlike ReLU, GeLU allows small negative values to pass through.\n",
    "\n",
    "**Gradient Attributes**: The `.grad` attribute of a tensor that stores computed gradients after calling `.backward()`. Gradients indicate how much changing each parameter would affect the loss. Gradients accumulate by default, so they must be zeroed before each training step.\n",
    "\n",
    "**Gradient Descent**: An optimization algorithm that iteratively adjusts parameters in the direction that reduces the loss. The algorithm computes gradients (slopes) of the loss with respect to parameters, then updates parameters by moving in the opposite direction. The learning rate controls the step size.\n",
    "\n",
    "**LayerNorm (Layer Normalization)**: A normalization technique that standardizes activations across features for each sample independently. LayerNorm improves training stability and speed by keeping activations in a reasonable range. It's particularly effective in recurrent and transformer architectures.\n",
    "\n",
    "**Learning Rate**: A hyperparameter that controls how much to adjust parameters during each optimization step. A learning rate that's too high causes unstable training, while one that's too low makes training extremely slow. Finding the right learning rate is crucial for successful training.\n",
    "\n",
    "**Linear Regression**: A simple model that learns a linear relationship between inputs and outputs: `y = Wx + b`. Despite its simplicity, linear regression demonstrates all the key concepts of neural network training: forward pass, loss computation, backward pass, and parameter updates.\n",
    "\n",
    "**Logits**: Raw, unnormalized scores output by a model before applying an activation function like softmax. In classification, logits represent the model's confidence for each class before conversion to probabilities. Many loss functions (like CrossEntropyLoss) expect logits rather than probabilities for numerical stability.\n",
    "\n",
    "**Loss Functions**: Functions that measure how far a model's predictions are from the true values. The loss provides a single number that the optimizer tries to minimize. Common examples include Mean Squared Error (MSE) for regression and Cross-Entropy for classification.\n",
    "\n",
    "**loss.backward()**: The method call that triggers backpropagation, computing gradients for all parameters with `requires_grad=True`. This method traverses the computation graph backward, applying the chain rule to calculate how each parameter affects the loss. After calling `loss.backward()`, gradients are stored in each parameter's `.grad` attribute.\n",
    "\n",
    "**Matrix Multiplication**: A fundamental linear algebra operation that combines two matrices to produce a new matrix. In PyTorch, matrix multiplication can be performed using `torch.mm()`, `torch.matmul()`, or the `@` operator. This operation is the core computation in neural network layers.\n",
    "\n",
    "**nn.Module**: The base class for all neural network components in PyTorch. Subclassing `nn.Module` provides automatic parameter tracking, device management, and train/eval mode switching. Every custom layer or model should inherit from `nn.Module`.\n",
    "\n",
    "**optimizer.step()**: The method that updates model parameters using computed gradients. After calling `loss.backward()` to compute gradients, `optimizer.step()` applies the optimization algorithm (like SGD or Adam) to adjust parameters in the direction that reduces loss.\n",
    "\n",
    "**optimizer.zero_grad()**: The method that clears (zeros out) all parameter gradients. This must be called before each backward pass because PyTorch accumulates gradients by default. Without zeroing gradients, new gradients would add to old ones, causing incorrect updates.\n",
    "\n",
    "**Parameters**: Learnable tensors in a neural network that are adjusted during training. Parameters are typically weights and biases in layers. PyTorch's `nn.Parameter` class marks tensors as learnable and ensures they're tracked by the module and optimizer.\n",
    "\n",
    "**Reduction Operations**: Operations that reduce tensor dimensions by aggregating values, such as sum, mean, max, or min. Reductions can operate across all elements or along specific dimensions. These operations are essential for computing losses, metrics, and pooling operations.\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**: The most common activation function, defined as `max(0, x)`. ReLU outputs the input if positive, otherwise outputs zero. It's popular because it's computationally efficient, helps avoid vanishing gradients, and works well in practice.\n",
    "\n",
    "**requires_grad**: A boolean flag on tensors that tells PyTorch whether to track operations for automatic differentiation. When `requires_grad=True`, PyTorch builds a computation graph to enable gradient computation. Model parameters have `requires_grad=True` by default, while input data typically doesn't.\n",
    "\n",
    "**Shape**: The dimensions of a tensor, describing how many elements it has along each axis. Shape is accessed via `.shape` or `.size()` and is crucial for ensuring operations have compatible dimensions. Common shapes include 1D vectors, 2D matrices, 3D sequences, and 4D image batches.\n",
    "\n",
    "**Softmax**: An activation function that converts a vector of logits into a probability distribution. Softmax ensures outputs are positive and sum to 1, making them interpretable as probabilities. It's commonly used as the final layer in classification networks.\n",
    "\n",
    "**torch.gather**: An advanced indexing operation that selects elements from a tensor based on indices. Gather is useful for selecting specific elements along a dimension, such as extracting predicted class scores. It's commonly used in reinforcement learning and sequence modeling.\n",
    "\n",
    "**torch.tensor**: The fundamental data structure in PyTorch - a multi-dimensional array that can hold numbers. Tensors are similar to NumPy arrays but can run on GPUs and support automatic differentiation. All data in PyTorch (inputs, outputs, parameters) is represented as tensors.\n",
    "\n",
    "**Training Loop**: The complete workflow for training a neural network, typically consisting of: iterate over epochs, iterate over batches, forward pass, compute loss, backward pass (compute gradients), optimizer step (update parameters), and zero gradients. This loop is repeated until the model converges.\n",
    "\n",
    "**Weights**: The learnable parameters in neural network layers that determine connection strengths between neurons. In a linear layer computing `y = Wx + b`, W is the weight matrix. During training, weights are adjusted to minimize the loss function.\n",
    "\n",
    "**Y-hat (ŷ)**: Mathematical notation for predicted values output by a model, as opposed to true values (y). The difference between y-hat and y is the prediction error, which the loss function quantifies. The goal of training is to make y-hat as close to y as possible.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed this comprehensive PyTorch learning notebook. You now understand the fundamental concepts needed to build, train, and deploy neural networks with PyTorch. Keep practicing, experiment with different architectures, and happy learning! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_prep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
