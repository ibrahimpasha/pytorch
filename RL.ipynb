{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• Reinforcement Learning with PyTorch: Zero to Hero\n",
    "\n",
    "Welcome! This notebook will teach you Reinforcement Learning from the ground up, using **PyTorch** for all neural network implementations.\n",
    "\n",
    "**What you'll learn:**\n",
    "- What RL is and how it differs from other ML approaches\n",
    "- The exploration vs exploitation dilemma\n",
    "- Core algorithms: Bandits, Q-Learning, DQN, Policy Gradients, and more\n",
    "- How to implement everything in PyTorch\n",
    "\n",
    "**Prerequisites:** Basic Python, some familiarity with probability, and basic PyTorch knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [What is Reinforcement Learning?](#section1)\n",
    "2. [Multi-Armed Bandits](#section2)\n",
    "3. [Markov Decision Processes](#section3)\n",
    "4. [Temporal Difference Learning & Q-Learning](#section4)\n",
    "5. [Deep Q-Networks (DQN)](#section5)\n",
    "6. [Advanced DQN: Double & Dueling](#section6)\n",
    "7. [Policy Gradient Methods (REINFORCE)](#section7)\n",
    "8. [Actor-Critic Methods](#section8)\n",
    "9. [Proximal Policy Optimization (PPO)](#section9)\n",
    "10. [Summary & Next Steps](#section10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import the libraries we need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üì± Using device: {device}\")\n",
    "print(\"‚úÖ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section1'></a>\n",
    "# 1. What is Reinforcement Learning?\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "Imagine teaching a dog to fetch. You don't show it thousands of labeled examples of \"correct fetching\" (that would be supervised learning). Instead:\n",
    "\n",
    "1. The dog **tries something** (runs toward the ball)\n",
    "2. You give **feedback** (\"Good boy!\" + treat, or nothing)\n",
    "3. The dog **learns** which actions lead to treats\n",
    "\n",
    "This is **Reinforcement Learning (RL)**: learning through trial and error by receiving rewards or penalties.\n",
    "\n",
    "## How RL Differs from Other Machine Learning\n",
    "\n",
    "| Approach | What it needs | How it learns | Example |\n",
    "|----------|--------------|---------------|--------|\n",
    "| **Supervised Learning** | Labeled data (input ‚Üí correct output) | Learns to map inputs to outputs | Email spam filter: \"This email is spam\" |\n",
    "| **Unsupervised Learning** | Unlabeled data | Finds patterns/structure | Customer segmentation: group similar buyers |\n",
    "| **Reinforcement Learning** | Environment + rewards | Trial and error | Game AI: +1 for winning, -1 for losing |\n",
    "\n",
    "## The RL Framework: Agent and Environment\n",
    "\n",
    "Every RL problem has two main components:\n",
    "\n",
    "- **Agent**: The learner/decision-maker (like the dog, or a game-playing AI)\n",
    "- **Environment**: Everything the agent interacts with (the world, the game)\n",
    "\n",
    "They interact in a loop:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ    Agent ‚îÄ‚îÄaction‚îÄ‚îÄ> Environment        ‚îÇ\n",
    "‚îÇ      ‚Üë                    ‚îÇ             ‚îÇ\n",
    "‚îÇ      ‚îî‚îÄ‚îÄreward + state‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "1. Agent observes the current **state** of the environment\n",
    "2. Agent chooses an **action**\n",
    "3. Environment returns a **reward** and the new **state**\n",
    "4. Repeat!\n",
    "\n",
    "Let's see this in code with a super simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Understanding RL: A Simple Analogy\n",
    "\n",
    "**Think of RL like learning to ride a bike:**\n",
    "\n",
    "1. **You (the Agent)** try different things - lean left, lean right, pedal faster\n",
    "2. **The World (Environment)** responds - you stay balanced or you fall\n",
    "3. **Feedback (Reward)** tells you how you did - staying up = good, falling = bad\n",
    "4. **You Learn** - next time, you remember what worked!\n",
    "\n",
    "**The RL Loop in Plain English:**\n",
    "```\n",
    "1. Look at the situation (observe STATE)\n",
    "2. Decide what to do (choose ACTION)\n",
    "3. See what happens (get REWARD + new STATE)\n",
    "4. Learn from it (update your strategy)\n",
    "5. Repeat!\n",
    "```\n",
    "\n",
    "**Key Terms (Don't worry, we'll explain each one!):**\n",
    "- **Agent**: The learner (like you learning to bike)\n",
    "- **Environment**: Everything else (the bike, the road, gravity)\n",
    "- **State**: The current situation (your balance, speed, position)\n",
    "- **Action**: What you can do (lean, pedal, brake)\n",
    "- **Reward**: Feedback signal (+1 for staying up, -1 for falling)\n",
    "- **Policy**: Your strategy (\"when tilting left, lean right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE: A Simple Guessing Game\n",
    "# The agent must learn to guess the secret number (0 or 1)\n",
    "\n",
    "class SimpleEnvironment:\n",
    "    \"\"\"\n",
    "    A tiny environment where the agent must guess a secret number.\n",
    "    - State: None (no state in this simple game)\n",
    "    - Actions: 0 or 1 (the guess)\n",
    "    - Reward: +1 if correct, -1 if wrong\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.secret = 1  # The correct answer\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Agent takes an action, environment returns reward\"\"\"\n",
    "        if action == self.secret:\n",
    "            return +1  # Correct! Positive reward\n",
    "        else:\n",
    "            return -1  # Wrong! Negative reward\n",
    "\n",
    "# Let's test it manually\n",
    "env = SimpleEnvironment()\n",
    "\n",
    "print(\"Testing the environment:\")\n",
    "print(f\"  Guess 0 ‚Üí reward: {env.step(0)}\")\n",
    "print(f\"  Guess 1 ‚Üí reward: {env.step(1)}\")\n",
    "print(\"\\nThe agent needs to learn that action=1 gives positive reward!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Learning Agent with PyTorch\n",
    "\n",
    "Now let's create an agent that **learns** which action is better.\n",
    "\n",
    "**The agent's strategy:**\n",
    "1. Try each action at least once\n",
    "2. Keep track of the average reward for each action\n",
    "3. Pick the action with the highest average reward\n",
    "\n",
    "This is called **value estimation** - the agent estimates how valuable each action is.\n",
    "\n",
    "**Why PyTorch?** Even for this simple example, we'll use PyTorch tensors. This builds good habits for when we need neural networks later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    \"\"\"\n",
    "    An agent that learns by tracking average rewards.\n",
    "    Uses PyTorch tensors for Q-value storage.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_actions=2):\n",
    "        # Track Q-values (estimated value of each action) using PyTorch\n",
    "        self.q_values = torch.zeros(n_actions)\n",
    "        self.action_count = torch.zeros(n_actions)\n",
    "    \n",
    "    def choose_action(self, epsilon=0.1):\n",
    "        \"\"\"Epsilon-greedy: explore with probability epsilon\"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, 1)  # Random exploration\n",
    "        return self.q_values.argmax().item()  # Exploit best action\n",
    "    \n",
    "    def learn(self, action, reward):\n",
    "        \"\"\"Update Q-value using incremental mean\"\"\"\n",
    "        self.action_count[action] += 1\n",
    "        # Incremental mean update: Q = Q + (1/n)(r - Q)\n",
    "        self.q_values[action] += (reward - self.q_values[action]) / self.action_count[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch the agent learn!\n",
    "agent = SimpleAgent()\n",
    "env = SimpleEnvironment()\n",
    "\n",
    "print(\"The RL Loop in action (using PyTorch tensors):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for step in range(5):\n",
    "    # 1. Agent chooses an action\n",
    "    action = agent.choose_action(epsilon=0.5)  # High exploration at first\n",
    "    \n",
    "    # 2. Environment returns reward\n",
    "    reward = env.step(action)\n",
    "    \n",
    "    # 3. Agent learns from the reward\n",
    "    agent.learn(action, reward)\n",
    "    \n",
    "    # Show what happened\n",
    "    print(f\"Step {step + 1}:\")\n",
    "    print(f\"  Action chosen: {action}\")\n",
    "    print(f\"  Reward received: {reward}\")\n",
    "    print(f\"  Q-values: {agent.q_values.tolist()}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"üéØ Best action learned: {agent.q_values.argmax().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section2'></a>\n",
    "# 2. Multi-Armed Bandits\n",
    "\n",
    "## üé∞ Why Start with Bandits?\n",
    "\n",
    "Before we tackle complex RL, let's understand the **core dilemma** with a simple problem.\n",
    "\n",
    "**Imagine this scenario:**\n",
    "\n",
    "You walk into a casino with 3 slot machines. Each machine has a different (hidden) win rate:\n",
    "- Machine A: Wins 20% of the time\n",
    "- Machine B: Wins 50% of the time  \n",
    "- Machine C: Wins 75% of the time (the best!)\n",
    "\n",
    "**But you don't know these rates!** You only find out by playing.\n",
    "\n",
    "**The Dilemma:**\n",
    "- If you always play the machine that *seems* best so far ‚Üí you might miss a better one!\n",
    "- If you keep trying new machines ‚Üí you waste plays on bad ones!\n",
    "\n",
    "This is the **Exploration vs Exploitation** tradeoff:\n",
    "- **Explore**: Try new things to learn more\n",
    "- **Exploit**: Use what you already know works\n",
    "\n",
    "**Real-world examples:**\n",
    "- üçï Restaurant: Try new places (explore) or go to your favorite (exploit)?\n",
    "- üì∫ Netflix: Watch something new (explore) or rewatch a favorite (exploit)?\n",
    "- üíº Career: Learn new skills (explore) or deepen existing ones (exploit)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Slot Machines\n",
    "\n",
    "Imagine you're in a casino with **K different slot machines** (called \"one-armed bandits\"). Each machine has a different probability of paying out, but you don't know what those probabilities are.\n",
    "\n",
    "**Your goal:** Maximize your total winnings over N plays.\n",
    "\n",
    "**The challenge:** You face a fundamental dilemma:\n",
    "\n",
    "- **Exploration**: Try different machines to discover which ones pay better\n",
    "- **Exploitation**: Stick with the machine that seems best so far\n",
    "\n",
    "If you only exploit, you might miss a better machine. If you only explore, you waste plays on bad machines.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "This isn't just about casinos! The exploration-exploitation tradeoff appears everywhere:\n",
    "\n",
    "- **A/B testing**: Which website design converts better?\n",
    "- **Clinical trials**: Which treatment is most effective?\n",
    "- **Recommendations**: Show content you know the user likes, or try something new?\n",
    "\n",
    "Let's build a bandit environment and try different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    \"\"\"\n",
    "    K slot machines, each with a hidden win probability.\n",
    "    \n",
    "    When you pull arm i:\n",
    "    - You win (reward=1) with probability p[i]\n",
    "    - You lose (reward=0) with probability 1-p[i]\n",
    "    \"\"\"\n",
    "    def __init__(self, probabilities=[0.2, 0.5, 0.75]):\n",
    "        # Hidden probabilities - the agent doesn't know these!\n",
    "        self.probabilities = torch.tensor(probabilities)\n",
    "        self.n_arms = len(probabilities)\n",
    "    \n",
    "    def pull(self, arm):\n",
    "        \"\"\"Pull an arm and get a reward (0 or 1)\"\"\"\n",
    "        if random.random() < self.probabilities[arm].item():\n",
    "            return 1  # Win!\n",
    "        else:\n",
    "            return 0  # Lose\n",
    "\n",
    "# Test the bandit\n",
    "bandit = MultiArmedBandit([0.2, 0.5, 0.75])\n",
    "\n",
    "print(\"Testing each arm 10 times:\")\n",
    "print(\"(Remember: arm 2 has 75% win rate, so it should win most often)\\n\")\n",
    "\n",
    "for arm in range(3):\n",
    "    wins = sum([bandit.pull(arm) for _ in range(10)])\n",
    "    print(f\"Arm {arm}: {wins}/10 wins (true probability: {bandit.probabilities[arm].item()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 1: Pure Greedy (Always Exploit)\n",
    "\n",
    "**The idea:** Always pick the arm with the highest estimated value.\n",
    "\n",
    "**How we estimate value:** Track the average reward for each arm:\n",
    "\n",
    "$Q(a) = \\frac{\\text{total reward from arm } a}{\\text{number of times we pulled arm } a}$\n",
    "\n",
    "**The problem:** If we get unlucky early (a good arm gives a bad result), we might never try it again!\n",
    "\n",
    "Let's see this failure in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_greedy_strategy(bandit, n_steps=200):\n",
    "    \"\"\"\n",
    "    Pure greedy: always pick the arm with highest estimated value.\n",
    "    Returns the history of rewards and which arms were pulled.\n",
    "    \"\"\"\n",
    "    n_arms = bandit.n_arms\n",
    "    \n",
    "    # Track estimates for each arm using PyTorch tensors\n",
    "    Q = torch.zeros(n_arms)        # Estimated value of each arm\n",
    "    N = torch.zeros(n_arms)        # Number of times each arm was pulled\n",
    "    \n",
    "    rewards = []                # History of rewards\n",
    "    arms_pulled = []            # History of which arm was pulled\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        # GREEDY: Pick arm with highest Q value\n",
    "        arm = Q.argmax().item()\n",
    "        \n",
    "        # Pull the arm and get reward\n",
    "        reward = bandit.pull(arm)\n",
    "        \n",
    "        # Update our estimate using incremental average\n",
    "        N[arm] += 1\n",
    "        Q[arm] = Q[arm] + (reward - Q[arm]) / N[arm]\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        arms_pulled.append(arm)\n",
    "    \n",
    "    return rewards, arms_pulled, N\n",
    "\n",
    "# Run with a specific seed to show the problem\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "bandit = MultiArmedBandit([0.2, 0.5, 0.75])\n",
    "\n",
    "rewards, arms, counts = run_greedy_strategy(bandit, n_steps=200)\n",
    "\n",
    "print(\"Greedy Strategy Results:\")\n",
    "print(f\"  Times each arm was pulled: {counts.tolist()}\")\n",
    "print(f\"  Total reward: {sum(rewards)} out of 200 possible\")\n",
    "print(f\"\\n‚ö†Ô∏è Problem: The agent might get stuck on a suboptimal arm!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2: Epsilon-Greedy (Explore + Exploit)\n",
    "\n",
    "**The fix:** Sometimes explore randomly!\n",
    "\n",
    "**The algorithm:**\n",
    "- With probability **Œµ** (epsilon): pick a **random** arm (explore)\n",
    "- With probability **1-Œµ**: pick the **best** arm (exploit)\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$\n",
    "\\text{action} = \n",
    "\\begin{cases}\n",
    "\\text{random arm} & \\text{with probability } \\epsilon \\\\\n",
    "\\arg\\max_a Q(a) & \\text{with probability } 1-\\epsilon\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "**Common values for Œµ:**\n",
    "- Œµ = 0.1 means 10% exploration, 90% exploitation\n",
    "- Œµ = 0.01 means 1% exploration (more exploitation)\n",
    "- Œµ = 0 is pure greedy (no exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epsilon_greedy(bandit, epsilon=0.1, n_steps=200):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy: explore with probability epsilon,\n",
    "    otherwise exploit the best arm.\n",
    "    \"\"\"\n",
    "    n_arms = bandit.n_arms\n",
    "    Q = torch.zeros(n_arms)  # Estimated values\n",
    "    N = torch.zeros(n_arms)  # Pull counts\n",
    "    rewards = []\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        # Flip a coin: explore or exploit?\n",
    "        if random.random() < epsilon:\n",
    "            # EXPLORE: pick a random arm\n",
    "            arm = random.randint(0, n_arms - 1)\n",
    "        else:\n",
    "            # EXPLOIT: pick the best arm\n",
    "            arm = Q.argmax().item()\n",
    "        \n",
    "        # Pull and update\n",
    "        reward = bandit.pull(arm)\n",
    "        N[arm] += 1\n",
    "        Q[arm] = Q[arm] + (reward - Q[arm]) / N[arm]\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards, N, Q\n",
    "\n",
    "# Compare greedy vs epsilon-greedy\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "bandit = MultiArmedBandit([0.2, 0.5, 0.75])\n",
    "\n",
    "greedy_rewards, _, greedy_counts = run_greedy_strategy(bandit, 1000)\n",
    "eps_rewards, eps_counts, eps_Q = run_epsilon_greedy(bandit, epsilon=0.1, n_steps=1000)\n",
    "\n",
    "print(\"Results over 1000 steps:\")\n",
    "print(\"\\nGreedy:\")\n",
    "print(f\"  Arm pulls: {greedy_counts.tolist()}\")\n",
    "print(f\"  Total reward: {sum(greedy_rewards)}\")\n",
    "\n",
    "print(\"\\nEpsilon-Greedy (Œµ=0.1):\")\n",
    "print(f\"  Arm pulls: {eps_counts.tolist()}\")\n",
    "print(f\"  Total reward: {sum(eps_rewards)}\")\n",
    "print(f\"  Learned Q-values: {eps_Q.tolist()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Epsilon-greedy explores and finds the best arm!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 3: Upper Confidence Bound (UCB)\n",
    "\n",
    "**The smartest approach:** Explore arms we're **uncertain** about!\n",
    "\n",
    "**The intuition:**\n",
    "- If we've pulled an arm many times, we're confident about its value\n",
    "- If we've barely tried an arm, we're uncertain - it might be great!\n",
    "- UCB adds a \"bonus\" for uncertainty\n",
    "\n",
    "**The formula:**\n",
    "\n",
    "$A_t = \\arg\\max_a \\left[ Q(a) + c \\sqrt{\\frac{\\ln t}{N(a)}} \\right]$\n",
    "\n",
    "Where:\n",
    "- $Q(a)$ = estimated value of arm a\n",
    "- $N(a)$ = number of times we pulled arm a  \n",
    "- $t$ = total number of steps so far\n",
    "- $c$ = exploration parameter (typically 2)\n",
    "\n",
    "**The bonus term** $\\sqrt{\\frac{\\ln t}{N(a)}}$:\n",
    "- Gets smaller as we pull arm a more (less uncertainty)\n",
    "- Gets larger as total time increases (encourages trying neglected arms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ucb(bandit, c=2.0, n_steps=200):\n",
    "    \"\"\"\n",
    "    Upper Confidence Bound: balance exploitation and exploration\n",
    "    using uncertainty estimates.\n",
    "    \"\"\"\n",
    "    n_arms = bandit.n_arms\n",
    "    Q = torch.zeros(n_arms)\n",
    "    N = torch.zeros(n_arms)\n",
    "    rewards = []\n",
    "    \n",
    "    for t in range(1, n_steps + 1):\n",
    "        # First, try each arm once\n",
    "        if t <= n_arms:\n",
    "            arm = t - 1\n",
    "        else:\n",
    "            # UCB selection: Q(a) + exploration bonus\n",
    "            ucb_values = Q + c * torch.sqrt(torch.log(torch.tensor(float(t))) / (N + 1e-5))\n",
    "            arm = ucb_values.argmax().item()\n",
    "        \n",
    "        reward = bandit.pull(arm)\n",
    "        N[arm] += 1\n",
    "        Q[arm] = Q[arm] + (reward - Q[arm]) / N[arm]\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards, N, Q\n",
    "\n",
    "# Test UCB\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "bandit = MultiArmedBandit([0.2, 0.5, 0.75])\n",
    "\n",
    "ucb_rewards, ucb_counts, ucb_Q = run_ucb(bandit, c=2.0, n_steps=1000)\n",
    "\n",
    "print(\"UCB Results:\")\n",
    "print(f\"  Arm pulls: {ucb_counts.tolist()}\")\n",
    "print(f\"  Total reward: {sum(ucb_rewards)}\")\n",
    "print(f\"  Learned Q-values: {[round(q, 3) for q in ucb_Q.tolist()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ALL strategies\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "bandit = MultiArmedBandit([0.2, 0.5, 0.75])\n",
    "n_steps = 1000\n",
    "\n",
    "greedy_r, _, _ = run_greedy_strategy(bandit, n_steps)\n",
    "eps_r, _, _ = run_epsilon_greedy(bandit, epsilon=0.1, n_steps=n_steps)\n",
    "ucb_r, _, _ = run_ucb(bandit, c=2.0, n_steps=n_steps)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(np.cumsum(greedy_r), label='Greedy', alpha=0.8)\n",
    "plt.plot(np.cumsum(eps_r), label='Œµ-Greedy (Œµ=0.1)', alpha=0.8)\n",
    "plt.plot(np.cumsum(ucb_r), label='UCB (c=2)', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Comparison of Bandit Strategies')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Rewards:\")\n",
    "print(f\"  Greedy:     {sum(greedy_r)}\")\n",
    "print(f\"  Œµ-Greedy:   {sum(eps_r)}\")\n",
    "print(f\"  UCB:        {sum(ucb_r)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section3'></a>\n",
    "# 3. Markov Decision Processes (MDPs)\n",
    "\n",
    "## Beyond Bandits: Adding States\n",
    "\n",
    "In bandits, there's no \"state\" - every decision is independent. But most real problems have **states**:\n",
    "\n",
    "- Chess: The board position is the state\n",
    "- Robot navigation: The robot's location is the state\n",
    "- Game: Health, inventory, position are all part of the state\n",
    "\n",
    "**Markov Decision Process (MDP)** is the mathematical framework for RL with states.\n",
    "\n",
    "## MDP Components\n",
    "\n",
    "An MDP is defined by:\n",
    "\n",
    "- **S**: Set of states (where can the agent be?)\n",
    "- **A**: Set of actions (what can the agent do?)\n",
    "- **P(s'|s,a)**: Transition probabilities (if I do action a in state s, where do I end up?)\n",
    "- **R(s,a,s')**: Reward function (what reward do I get?)\n",
    "- **Œ≥** (gamma): Discount factor (how much do we care about future rewards?)\n",
    "\n",
    "## The Markov Property\n",
    "\n",
    "**Key assumption:** The future only depends on the current state, not the history.\n",
    "\n",
    "$P(S_{t+1} | S_t, A_t) = P(S_{t+1} | S_0, A_0, S_1, A_1, ..., S_t, A_t)$\n",
    "\n",
    "In plain English: \"Where I go next depends only on where I am now, not how I got here.\"\n",
    "\n",
    "## Value Functions\n",
    "\n",
    "**Key idea:** Some states are better than others. We want to quantify this.\n",
    "\n",
    "### State-Value Function V(s)\n",
    "\n",
    "\"How good is it to be in state s?\"\n",
    "\n",
    "$V^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]$\n",
    "\n",
    "The expected return starting from state s and following policy œÄ.\n",
    "\n",
    "### Action-Value Function Q(s, a)\n",
    "\n",
    "\"How good is it to take action a in state s?\"\n",
    "\n",
    "$Q^\\pi(s, a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]$\n",
    "\n",
    "The expected return starting from state s, taking action a, then following policy œÄ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bellman Equation\n",
    "\n",
    "The **Bellman equation** is the foundation of RL. It says:\n",
    "\n",
    "$V(s) = \\mathbb{E}[R + \\gamma V(s')]$\n",
    "\n",
    "In words: \"The value of a state = immediate reward + discounted value of next state\"\n",
    "\n",
    "This creates a **recursive relationship** between state values.\n",
    "\n",
    "### The Bellman Optimality Equation\n",
    "\n",
    "For the optimal policy:\n",
    "\n",
    "$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]$\n",
    "\n",
    "\"The optimal value = best action's expected (reward + discounted future value)\"\n",
    "\n",
    "## Discounted Return\n",
    "\n",
    "**Question:** How do we measure how good a sequence of actions is?\n",
    "\n",
    "**Answer:** Sum up the rewards, but discount future rewards:\n",
    "\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$\n",
    "\n",
    "Where **Œ≥ (gamma)** is the discount factor (0 ‚â§ Œ≥ ‚â§ 1):\n",
    "- Œ≥ = 0: Only care about immediate reward\n",
    "- Œ≥ = 1: Care equally about all future rewards\n",
    "- Œ≥ = 0.9: Typical value - future rewards matter but less than immediate\n",
    "\n",
    "**Why discount?**\n",
    "1. Uncertainty: Future is less certain\n",
    "2. Math: Makes infinite sums converge\n",
    "3. Economics: Money now is worth more than money later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Simple Grid World MDP\n",
    "# The agent moves in a 4x4 grid trying to reach the goal\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    A 4x4 grid world:\n",
    "    \n",
    "    [0 ][1 ][2 ][3 ]\n",
    "    [4 ][5 ][6 ][7 ]\n",
    "    [8 ][9 ][10][11]\n",
    "    [12][13][14][G ]  <- 15 is the goal!\n",
    "    \n",
    "    Actions: 0=up, 1=right, 2=down, 3=left\n",
    "    Reward: +10 for reaching goal, -1 for each step\n",
    "    \"\"\"\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.n_states = size * size\n",
    "        self.n_actions = 4\n",
    "        self.goal = self.n_states - 1\n",
    "        self.state = 0  # Start at top-left\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to starting position\"\"\"\n",
    "        self.state = 0\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action, return (new_state, reward, done)\"\"\"\n",
    "        row, col = self.state // self.size, self.state % self.size\n",
    "        \n",
    "        # Move based on action\n",
    "        if action == 0 and row > 0:              # Up\n",
    "            row -= 1\n",
    "        elif action == 1 and col < self.size - 1:  # Right\n",
    "            col += 1\n",
    "        elif action == 2 and row < self.size - 1:  # Down\n",
    "            row += 1\n",
    "        elif action == 3 and col > 0:              # Left\n",
    "            col -= 1\n",
    "        \n",
    "        self.state = row * self.size + col\n",
    "        \n",
    "        # Check if we reached the goal\n",
    "        if self.state == self.goal:\n",
    "            return self.state, +10, True  # Big reward, episode done\n",
    "        else:\n",
    "            return self.state, -1, False  # Small penalty, continue\n",
    "    \n",
    "    def get_state_tensor(self, state=None):\n",
    "        \"\"\"One-hot encode state for neural networks\"\"\"\n",
    "        if state is None:\n",
    "            state = self.state\n",
    "        tensor = torch.zeros(self.n_states)\n",
    "        tensor[state] = 1.0\n",
    "        return tensor\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Print the grid with agent position\"\"\"\n",
    "        for i in range(self.n_states):\n",
    "            if i == self.state:\n",
    "                print('A', end=' ')  # Agent\n",
    "            elif i == self.goal:\n",
    "                print('G', end=' ')  # Goal\n",
    "            else:\n",
    "                print('.', end=' ')\n",
    "            if (i + 1) % self.size == 0:\n",
    "                print()\n",
    "\n",
    "# Test the environment\n",
    "env = GridWorld()\n",
    "print(\"Starting position:\")\n",
    "env.render()\n",
    "\n",
    "print(\"\\nTaking actions: right, right, down, down, down, right\")\n",
    "actions = [1, 1, 2, 2, 2, 1]  # right, right, down, down, down, right\n",
    "for a in actions:\n",
    "    state, reward, done = env.step(a)\n",
    "    print(f\"Action {['up','right','down','left'][a]}: state={state}, reward={reward}, done={done}\")\n",
    "\n",
    "print(\"\\nFinal position:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_return(rewards, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Calculate discounted return from a sequence of rewards.\n",
    "    G = r0 + Œ≥*r1 + Œ≥¬≤*r2 + ...\n",
    "    \"\"\"\n",
    "    G = 0\n",
    "    for t, r in enumerate(rewards):\n",
    "        G += (gamma ** t) * r\n",
    "    return G\n",
    "\n",
    "# Example: rewards from a 6-step episode\n",
    "rewards = [-1, -1, -1, -1, -1, 10]  # 5 steps of -1, then +10 at goal\n",
    "\n",
    "print(\"Rewards:\", rewards)\n",
    "print(\"\\nDiscounted returns with different Œ≥:\")\n",
    "for gamma in [0.0, 0.5, 0.9, 1.0]:\n",
    "    G = calculate_return(rewards, gamma)\n",
    "    print(f\"  Œ≥={gamma}: G = {G:.2f}\")\n",
    "\n",
    "print(\"\\nNotice: Higher Œ≥ values the +10 goal reward more!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section4'></a>\n",
    "# 4. Temporal Difference Learning & Q-Learning\n",
    "\n",
    "## ‚ö° TD Learning: The Key Insight\n",
    "\n",
    "**The Problem with Monte Carlo:**\n",
    "\n",
    "Monte Carlo waits until the END of an episode to learn. But what if:\n",
    "- Episodes are very long?\n",
    "- Episodes never end (continuing tasks)?\n",
    "- We want to learn faster?\n",
    "\n",
    "**TD's Brilliant Idea: Learn from EVERY step!**\n",
    "\n",
    "Instead of waiting for the true return G, we **estimate** it:\n",
    "\n",
    "```\n",
    "True return:      G = r‚ÇÅ + Œ≥r‚ÇÇ + Œ≥¬≤r‚ÇÉ + ... (need whole episode)\n",
    "TD estimate:      G ‚âà r‚ÇÅ + Œ≥V(next_state)   (just need one step!)\n",
    "```\n",
    "\n",
    "**Why does this work?**\n",
    "\n",
    "If V(s) is a good estimate of future rewards from state s, then:\n",
    "- r + Œ≥V(s') is a good estimate of total return\n",
    "- We \"bootstrap\" - use our own estimates to improve our estimates!\n",
    "\n",
    "**The TD Update (in plain English):**\n",
    "```\n",
    "1. I thought state s was worth V(s)\n",
    "2. I took an action and got reward r\n",
    "3. I ended up in state s', which I think is worth V(s')\n",
    "4. So maybe s is actually worth: r + Œ≥V(s')\n",
    "5. Update V(s) a little bit toward this new estimate\n",
    "```\n",
    "\n",
    "**Formula:**\n",
    "$V(s) \\leftarrow V(s) + \\alpha \\cdot [r + \\gamma V(s') - V(s)]$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = learning rate (how much to update)\n",
    "- $r + \\gamma V(s')$ = TD target (what we think V(s) should be)\n",
    "- $r + \\gamma V(s') - V(s)$ = TD error (how wrong we were)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Q-Learning: Learning the Best Actions\n",
    "\n",
    "**Q-Learning is TD Learning for action-values (Q-values).**\n",
    "\n",
    "**The Key Idea:**\n",
    "\n",
    "We want to learn Q(s, a) - how good is action a in state s?\n",
    "\n",
    "**The Q-Learning Update:**\n",
    "$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$\n",
    "\n",
    "**Let's break this down:**\n",
    "\n",
    "1. **Current estimate:** Q(s, a) - what we currently think\n",
    "\n",
    "2. **TD Target:** r + Œ≥ max Q(s', a')\n",
    "   - r = immediate reward we got\n",
    "   - Œ≥ = discount factor (0.9 means future is 90% as important)\n",
    "   - max Q(s', a') = value of best action in next state\n",
    "   \n",
    "3. **TD Error:** target - current = how wrong we were\n",
    "\n",
    "4. **Update:** Move Q(s,a) a little toward the target\n",
    "\n",
    "**Why \"max\"?**\n",
    "\n",
    "Q-learning is **optimistic** - it assumes we'll act optimally in the future.\n",
    "Even if we explore randomly now, we learn the value of acting optimally.\n",
    "\n",
    "**Simple Example:**\n",
    "```\n",
    "State: s=0, Action: a=right, Reward: r=-1, Next state: s'=1\n",
    "Current: Q(0, right) = 0\n",
    "Next state values: Q(1, up)=2, Q(1, right)=5, Q(1, down)=3\n",
    "Max Q(s', a') = 5\n",
    "\n",
    "Target = -1 + 0.9 √ó 5 = 3.5\n",
    "Error = 3.5 - 0 = 3.5\n",
    "New Q(0, right) = 0 + 0.1 √ó 3.5 = 0.35\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularQLearning:\n",
    "    \"\"\"\n",
    "    Tabular Q-Learning with PyTorch tensors.\n",
    "    \n",
    "    This is the foundation of all value-based RL!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, lr=0.1, gamma=0.99, epsilon=0.1):\n",
    "        # Q-table: stores Q(s,a) for all state-action pairs\n",
    "        self.Q = torch.zeros(n_states, n_actions)\n",
    "        self.lr = lr           # Learning rate (Œ±)\n",
    "        self.gamma = gamma     # Discount factor (Œ≥)\n",
    "        self.epsilon = epsilon # Exploration rate (Œµ)\n",
    "        self.n_actions = n_actions\n",
    "    \n",
    "    def act(self, state, training=True):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.n_actions)  # Explore\n",
    "        return self.Q[state].argmax().item()  # Exploit\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Q-Learning update:\n",
    "        Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]\n",
    "        \"\"\"\n",
    "        # TD Target: r + Œ≥ * max Q(s', a')\n",
    "        if done:\n",
    "            target = reward  # No future rewards if episode ended\n",
    "        else:\n",
    "            target = reward + self.gamma * self.Q[next_state].max().item()\n",
    "        \n",
    "        # TD Error: target - current\n",
    "        td_error = target - self.Q[state, action].item()\n",
    "        \n",
    "        # Update Q-value\n",
    "        self.Q[state, action] += self.lr * td_error\n",
    "        \n",
    "        return td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tabular_q(env, agent, n_episodes=500):\n",
    "    \"\"\"Train Q-Learning agent on GridWorld\"\"\"\n",
    "    rewards_history = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(100):  # Max steps per episode\n",
    "            # 1. Choose action\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # 2. Take action, observe result\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # 3. Learn from experience\n",
    "            agent.learn(state, action, reward, next_state, done)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "    \n",
    "    return rewards_history\n",
    "\n",
    "# Train!\n",
    "env = GridWorld()\n",
    "agent = TabularQLearning(env.n_states, env.n_actions, lr=0.1, gamma=0.9, epsilon=0.1)\n",
    "rewards = train_tabular_q(env, agent, n_episodes=500)\n",
    "\n",
    "print(\"Tabular Q-Learning Results:\")\n",
    "print(f\"Average reward (last 50): {np.mean(rewards[-50:]):.2f}\")\n",
    "\n",
    "# Show learned policy\n",
    "symbols = ['‚Üë', '‚Üí', '‚Üì', '‚Üê']\n",
    "print(\"\\nLearned Policy:\")\n",
    "for i in range(env.n_states):\n",
    "    if i == env.goal:\n",
    "        print('G', end=' ')\n",
    "    else:\n",
    "        print(symbols[agent.Q[i].argmax().item()], end=' ')\n",
    "    if (i + 1) % env.size == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "window = 20\n",
    "smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "plt.plot(smoothed)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Tabular Q-Learning on GridWorld')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"The agent learns to reach the goal efficiently!\")\n",
    "print(f\"Optimal path gives reward: -1 √ó 6 + 10 = 4 (6 steps to goal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA: On-Policy TD Control\n",
    "\n",
    "**SARSA** is similar to Q-Learning but **on-policy**.\n",
    "\n",
    "**Update rule:**\n",
    "\n",
    "$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$\n",
    "\n",
    "**Difference from Q-Learning:**\n",
    "- Q-Learning: Uses max_a Q(s', a) - learns optimal policy\n",
    "- SARSA: Uses Q(s', a') where a' is the actual next action - learns the policy being followed\n",
    "\n",
    "**Name:** SARSA = State, Action, Reward, State, Action\n",
    "\n",
    "| Algorithm | Update Target | Type |\n",
    "|-----------|--------------|------|\n",
    "| Q-Learning | r + Œ≥ max Q(s', a') | Off-policy |\n",
    "| SARSA | r + Œ≥ Q(s', a') | On-policy |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    \"\"\"SARSA: On-policy TD control\"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, lr=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.Q = torch.zeros(n_states, n_actions)\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        return self.Q[state].argmax().item()\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, next_action, done):\n",
    "        \"\"\"SARSA update: uses actual next action, not max\"\"\"\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            # Key difference: use Q(s', a') not max Q(s', a')\n",
    "            target = reward + self.gamma * self.Q[next_state, next_action].item()\n",
    "        \n",
    "        td_error = target - self.Q[state, action].item()\n",
    "        self.Q[state, action] += self.lr * td_error\n",
    "\n",
    "# Train SARSA\n",
    "env = GridWorld()\n",
    "sarsa_agent = SARSA(env.n_states, env.n_actions, lr=0.1, gamma=0.9, epsilon=0.1)\n",
    "sarsa_rewards = []\n",
    "\n",
    "for ep in range(500):\n",
    "    state = env.reset()\n",
    "    action = sarsa_agent.act(state)\n",
    "    total_reward = 0\n",
    "    \n",
    "    for _ in range(100):\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_action = sarsa_agent.act(next_state)\n",
    "        sarsa_agent.learn(state, action, reward, next_state, next_action, done)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state, action = next_state, next_action\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    sarsa_rewards.append(total_reward)\n",
    "\n",
    "print(f\"SARSA Average reward (last 50): {np.mean(sarsa_rewards[-50:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section5'></a>\n",
    "# 5. Deep Q-Networks (DQN)\n",
    "\n",
    "## From Tables to Neural Networks\n",
    "\n",
    "**The Problem with Tabular Q-Learning:**\n",
    "\n",
    "When state spaces are large (or continuous), we can't store Q-values in a table:\n",
    "- Atari games: ~10^70 possible states\n",
    "- Robot control: Continuous positions, velocities\n",
    "- Go: More states than atoms in the universe!\n",
    "\n",
    "**The Solution:** Use a neural network to **approximate** Q(s,a)!\n",
    "\n",
    "Instead of:\n",
    "```\n",
    "Q[state][action] = value  (table lookup)\n",
    "```\n",
    "\n",
    "We use:\n",
    "```\n",
    "Q(state, action) = neural_network(state)[action]  (function approximation)\n",
    "```\n",
    "\n",
    "## Key DQN Innovations (DeepMind, 2015)\n",
    "\n",
    "Training neural networks with RL is tricky! DQN introduced two key ideas:\n",
    "\n",
    "### 1. Experience Replay\n",
    "\n",
    "**Problem:** Sequential experiences are correlated (state t is similar to state t+1).\n",
    "Neural networks don't like correlated data!\n",
    "\n",
    "**Solution:** Store experiences in a buffer, sample randomly to break correlations.\n",
    "\n",
    "```\n",
    "Buffer: [(s‚ÇÅ,a‚ÇÅ,r‚ÇÅ,s‚ÇÅ'), (s‚ÇÇ,a‚ÇÇ,r‚ÇÇ,s‚ÇÇ'), ...]\n",
    "Training: Sample random batch from buffer\n",
    "```\n",
    "\n",
    "### 2. Target Network\n",
    "\n",
    "**Problem:** We're updating Q toward a moving target (Q itself changes during training).\n",
    "This causes instability!\n",
    "\n",
    "**Solution:** Use a separate \"target network\" that updates slowly.\n",
    "\n",
    "```\n",
    "Q_network: Updated every step\n",
    "Q_target: Copied from Q_network every N steps\n",
    "```\n",
    "\n",
    "## The DQN Loss Function\n",
    "\n",
    "$\\mathcal{L} = \\mathbb{E}\\left[\\left(r + \\gamma \\max_{a'} Q_{\\text{target}}(s',a') - Q(s,a)\\right)^2\\right]$\n",
    "\n",
    "This is just MSE between our prediction and the TD target!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience Replay Buffer\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Fixed-size buffer to store experience tuples.\n",
    "    \n",
    "    Why replay?\n",
    "    1. Breaks correlation between consecutive experiences\n",
    "    2. Allows reusing rare experiences multiple times\n",
    "    3. Smooths out learning over many past experiences\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.buffer.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a random batch of transitions\"\"\"\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.stack(batch.state)\n",
    "        actions = torch.tensor(batch.action, dtype=torch.long)\n",
    "        rewards = torch.tensor(batch.reward, dtype=torch.float32)\n",
    "        next_states = torch.stack(batch.next_state)\n",
    "        dones = torch.tensor(batch.done, dtype=torch.float32)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Test replay buffer\n",
    "buffer = ReplayBuffer(1000)\n",
    "env = GridWorld()\n",
    "state = env.reset()\n",
    "state_tensor = env.get_state_tensor(state)\n",
    "\n",
    "# Fill buffer with random experiences\n",
    "for _ in range(100):\n",
    "    action = random.randrange(4)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    next_state_tensor = env.get_state_tensor(next_state)\n",
    "    buffer.push(state_tensor, action, reward, next_state_tensor, done)\n",
    "    state = next_state if not done else env.reset()\n",
    "    state_tensor = env.get_state_tensor(state)\n",
    "\n",
    "print(f\"Buffer size: {len(buffer)}\")\n",
    "states, actions, rewards, next_states, dones = buffer.sample(5)\n",
    "print(f\"Sample batch shapes: states={states.shape}, actions={actions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network: A neural network that outputs Q-values.\n",
    "    \n",
    "    Input: State (one-hot encoded)\n",
    "    Output: Q-value for each action\n",
    "    \n",
    "    Architecture:\n",
    "    state ‚Üí [Linear ‚Üí ReLU] ‚Üí [Linear ‚Üí ReLU] ‚Üí [Linear] ‚Üí Q-values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, hidden_size=64):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_states, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, n_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Output Q-values for all actions\n",
    "\n",
    "# Test the network\n",
    "net = DQN(16, 4)\n",
    "test_state = torch.randn(1, 16)\n",
    "q_values = net(test_state)\n",
    "print(f\"DQN output shape: {q_values.shape}\")\n",
    "print(f\"Q-values: {q_values.detach().numpy().round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    DQN Agent with experience replay and target network.\n",
    "    \n",
    "    This is the complete DQN algorithm from DeepMind!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, hidden_size=64, lr=1e-3, \n",
    "                 gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Q-Network (the one we train)\n",
    "        self.q_network = DQN(n_states, n_actions, hidden_size).to(device)\n",
    "        \n",
    "        # Target Network (for stable targets)\n",
    "        self.target_network = DQN(n_states, n_actions, hidden_size).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.buffer = ReplayBuffer(10000)\n",
    "    \n",
    "    def act(self, state, training=True):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.n_actions)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state = state.unsqueeze(0).to(device)\n",
    "            q_values = self.q_network(state)\n",
    "            return q_values.argmax(dim=1).item()\n",
    "    \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def learn(self, batch_size=64):\n",
    "        \"\"\"Sample from buffer and update Q-network\"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return 0\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(batch_size)\n",
    "        states = states.to(device)\n",
    "        actions = actions.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "        next_states = next_states.to(device)\n",
    "        dones = dones.to(device)\n",
    "        \n",
    "        # Current Q values: Q(s, a)\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Target Q values: r + Œ≥ max Q_target(s', a')\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_states).max(dim=1)[0]\n",
    "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        # Loss: MSE between current and target\n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target(self):\n",
    "        \"\"\"Copy Q-network weights to target network\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Reduce exploration over time\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "# Create DQN agent\n",
    "env = GridWorld()\n",
    "agent = DQNAgent(env.n_states, env.n_actions, hidden_size=64, lr=1e-3)\n",
    "print(f\"DQN Agent created!\")\n",
    "print(f\"Q-Network: {agent.q_network}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env, agent, n_episodes=300, target_update=10, batch_size=64):\n",
    "    \"\"\"Train DQN agent\"\"\"\n",
    "    rewards_history = []\n",
    "    losses = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        state_tensor = env.get_state_tensor(state)\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(100):\n",
    "            # 1. Choose action\n",
    "            action = agent.act(state_tensor)\n",
    "            \n",
    "            # 2. Take action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state_tensor = env.get_state_tensor(next_state)\n",
    "            \n",
    "            # 3. Store experience\n",
    "            agent.store(state_tensor, action, reward, next_state_tensor, done)\n",
    "            \n",
    "            # 4. Learn from replay buffer\n",
    "            loss = agent.learn(batch_size)\n",
    "            if loss > 0:\n",
    "                losses.append(loss)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state_tensor = next_state_tensor\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if episode % target_update == 0:\n",
    "            agent.update_target()\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg = np.mean(rewards_history[-50:])\n",
    "            print(f\"Episode {episode+1}: avg_reward={avg:.2f}, epsilon={agent.epsilon:.3f}\")\n",
    "    \n",
    "    return rewards_history, losses\n",
    "\n",
    "# Train DQN\n",
    "env = GridWorld()\n",
    "agent = DQNAgent(env.n_states, env.n_actions, hidden_size=64, lr=1e-3)\n",
    "rewards, losses = train_dqn(env, agent, n_episodes=300)\n",
    "\n",
    "print(f\"\\nFinal average reward: {np.mean(rewards[-50:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DQN results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Rewards\n",
    "window = 20\n",
    "smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "axes[0].plot(smoothed)\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Total Reward')\n",
    "axes[0].set_title('DQN Learning Curve')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "if losses:\n",
    "    loss_smooth = np.convolve(losses, np.ones(100)/100, mode='valid')\n",
    "    axes[1].plot(loss_smooth)\n",
    "    axes[1].set_xlabel('Training Step')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title('DQN Loss')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show learned policy\n",
    "print(\"\\nDQN Learned Policy:\")\n",
    "symbols = ['‚Üë', '‚Üí', '‚Üì', '‚Üê']\n",
    "for i in range(env.n_states):\n",
    "    if i == env.goal:\n",
    "        print('G', end=' ')\n",
    "    else:\n",
    "        state_tensor = env.get_state_tensor(i)\n",
    "        with torch.no_grad():\n",
    "            q_values = agent.q_network(state_tensor.unsqueeze(0).to(device))\n",
    "            action = q_values.argmax().item()\n",
    "        print(symbols[action], end=' ')\n",
    "    if (i + 1) % env.size == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section6'></a>\n",
    "# 6. Advanced DQN: Double & Dueling\n",
    "\n",
    "## The Problem with Standard DQN\n",
    "\n",
    "Standard DQN tends to **overestimate** Q-values. Why?\n",
    "\n",
    "The max operator in the target:\n",
    "$\\max_{a'} Q(s', a')$\n",
    "\n",
    "If Q-values have noise (they always do during learning), max picks the noisiest one!\n",
    "\n",
    "## Double DQN: Fixing Overestimation\n",
    "\n",
    "**Key Idea:** Separate action selection from action evaluation.\n",
    "\n",
    "**Standard DQN:**\n",
    "- Use target network to both SELECT and EVALUATE the best action\n",
    "\n",
    "**Double DQN:**\n",
    "- Use **online network** to SELECT the best action\n",
    "- Use **target network** to EVALUATE that action\n",
    "\n",
    "$Q_{\\text{target}} = r + \\gamma Q_{\\text{target}}(s', \\arg\\max_{a'} Q_{\\text{online}}(s', a'))$\n",
    "\n",
    "This reduces overestimation because the selection and evaluation use different networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent(DQNAgent):\n",
    "    \"\"\"\n",
    "    Double DQN - uses online network to select actions,\n",
    "    target network to evaluate them.\n",
    "    \n",
    "    Only the learn() method changes!\n",
    "    \"\"\"\n",
    "    \n",
    "    def learn(self, batch_size=64):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return 0\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(batch_size)\n",
    "        states = states.to(device)\n",
    "        actions = actions.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "        next_states = next_states.to(device)\n",
    "        dones = dones.to(device)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Double DQN: use online network to SELECT action, target to EVALUATE\n",
    "        with torch.no_grad():\n",
    "            # Step 1: Online network selects best action\n",
    "            best_actions = self.q_network(next_states).argmax(dim=1, keepdim=True)\n",
    "            # Step 2: Target network evaluates that action\n",
    "            next_q = self.target_network(next_states).gather(1, best_actions).squeeze(1)\n",
    "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "# Train Double DQN\n",
    "env = GridWorld()\n",
    "double_agent = DoubleDQNAgent(env.n_states, env.n_actions, hidden_size=64, lr=1e-3)\n",
    "double_rewards, _ = train_dqn(env, double_agent, n_episodes=300)\n",
    "\n",
    "print(f\"\\nDouble DQN final avg reward: {np.mean(double_rewards[-50:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling DQN: Separating Value and Advantage\n",
    "\n",
    "**Key Insight:** Not all states require knowing the value of each action.\n",
    "\n",
    "Sometimes it's obvious that all actions are bad (or good) regardless of which one you pick.\n",
    "\n",
    "**Dueling Architecture:**\n",
    "\n",
    "Split Q into two streams:\n",
    "- **Value stream V(s):** How good is this state overall?\n",
    "- **Advantage stream A(s,a):** How much better is action a than average?\n",
    "\n",
    "Then combine:\n",
    "$Q(s,a) = V(s) + A(s,a) - \\frac{1}{|A|}\\sum_{a'} A(s,a')$\n",
    "\n",
    "The subtraction ensures identifiability (otherwise V and A could shift arbitrarily).\n",
    "\n",
    "```\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ   Shared    ‚îÇ\n",
    "State ‚îÄ‚îÄ>‚îÇ   Layers    ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ             ‚îÇ\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ Value   ‚îÇ   ‚îÇAdvantage‚îÇ\n",
    "    ‚îÇ Stream  ‚îÇ   ‚îÇ Stream  ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ             ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                ‚îÇ\n",
    "         Q = V + (A - mean(A))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling DQN Architecture.\n",
    "    \n",
    "    Separates Q into Value and Advantage streams.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, hidden_size=64):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(n_states, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Value stream: V(s)\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Advantage stream: A(s,a)\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.shared(x)\n",
    "        \n",
    "        # Value: single number for the state\n",
    "        value = self.value_stream(features)\n",
    "        \n",
    "        # Advantage: one number per action\n",
    "        advantage = self.advantage_stream(features)\n",
    "        \n",
    "        # Combine: Q = V + (A - mean(A))\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values\n",
    "\n",
    "# Test Dueling DQN\n",
    "dueling_net = DuelingDQN(16, 4)\n",
    "test_state = torch.randn(1, 16)\n",
    "q_values = dueling_net(test_state)\n",
    "print(f\"Dueling DQN output shape: {q_values.shape}\")\n",
    "print(f\"Q-values: {q_values.detach().numpy().round(3)}\")\n",
    "print(\"\\nThe architecture separates 'how good is this state' from 'which action is best'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: DQN Variants\n",
    "\n",
    "| Variant | Key Innovation | Benefit |\n",
    "|---------|---------------|--------|\n",
    "| **DQN** | Replay + Target Network | Stable training |\n",
    "| **Double DQN** | Separate selection/evaluation | Reduces overestimation |\n",
    "| **Dueling DQN** | V/A decomposition | Better state evaluation |\n",
    "| **Rainbow** | All of the above + more | State-of-the-art |\n",
    "\n",
    "In practice, these improvements are often combined!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section7'></a>\n",
    "# 7. Policy Gradient Methods (REINFORCE)\n",
    "\n",
    "## A Different Approach: Learning Policies Directly\n",
    "\n",
    "So far, we've learned **value functions** (Q-values) and derived policies from them.\n",
    "\n",
    "**Policy Gradient methods** take a different approach: learn the **policy directly**!\n",
    "\n",
    "**Value-Based (DQN):**\n",
    "```\n",
    "Learn Q(s,a) ‚Üí Policy: pick action with highest Q\n",
    "```\n",
    "\n",
    "**Policy-Based (REINFORCE):**\n",
    "```\n",
    "Learn œÄ(a|s) directly ‚Üí Policy outputs action probabilities\n",
    "```\n",
    "\n",
    "## Why Policy Gradients?\n",
    "\n",
    "**Advantages:**\n",
    "1. **Continuous actions:** Can output any action, not just discrete choices\n",
    "2. **Stochastic policies:** Can learn to randomize (useful in games)\n",
    "3. **Simpler:** No need for max operation, target networks, etc.\n",
    "\n",
    "**Disadvantages:**\n",
    "1. **High variance:** Learning can be noisy\n",
    "2. **Sample inefficient:** Need lots of data\n",
    "3. **Local optima:** Can get stuck\n",
    "\n",
    "## The Policy Gradient Theorem\n",
    "\n",
    "**Goal:** Maximize expected return $J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$\n",
    "\n",
    "**The Magic Formula:**\n",
    "\n",
    "$\\nabla J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t\\right]$\n",
    "\n",
    "**In Plain English:**\n",
    "- If an action led to high return ‚Üí increase its probability\n",
    "- If an action led to low return ‚Üí decrease its probability\n",
    "- The gradient tells us how to adjust the policy\n",
    "\n",
    "## REINFORCE Algorithm\n",
    "\n",
    "1. **Collect** a full episode using current policy\n",
    "2. **Compute returns** $G_t$ for each step\n",
    "3. **Update:** $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$\n",
    "\n",
    "The name comes from: actions that lead to good outcomes get **reinforced**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that outputs action probabilities.\n",
    "    \n",
    "    Unlike DQN which outputs Q-values, this outputs a probability\n",
    "    distribution over actions using softmax.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, hidden_size=64):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_states, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, n_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Softmax converts to probabilities (sum to 1)\n",
    "        return F.softmax(self.fc3(x), dim=-1)\n",
    "\n",
    "# Test policy network\n",
    "policy_net = PolicyNetwork(16, 4)\n",
    "test_state = torch.randn(1, 16)\n",
    "probs = policy_net(test_state)\n",
    "print(f\"Policy output: {probs.detach().numpy().round(3)}\")\n",
    "print(f\"Sum of probabilities: {probs.sum().item():.3f} (should be 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    \"\"\"\n",
    "    REINFORCE (Monte Carlo Policy Gradient).\n",
    "    \n",
    "    The simplest policy gradient algorithm:\n",
    "    1. Collect full episode\n",
    "    2. Compute returns\n",
    "    3. Update policy to increase probability of good actions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, hidden_size=64, lr=1e-3, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.policy = PolicyNetwork(n_states, n_actions, hidden_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        # Episode storage\n",
    "        self.log_probs = []  # log œÄ(a|s) for each step\n",
    "        self.rewards = []    # rewards for each step\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Sample action from policy distribution\"\"\"\n",
    "        state = state.unsqueeze(0).to(device)\n",
    "        probs = self.policy(state)\n",
    "        \n",
    "        # Create categorical distribution and sample\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Store log probability for later\n",
    "        self.log_probs.append(dist.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"Store reward from environment\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Update policy after episode ends.\n",
    "        \n",
    "        Loss = -sum(log_prob * return)\n",
    "        Negative because we want to MAXIMIZE return\n",
    "        \"\"\"\n",
    "        # Calculate returns (backwards from end of episode)\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Normalize returns (reduces variance, helps learning)\n",
    "        if len(returns) > 1:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Policy gradient loss\n",
    "        loss = 0\n",
    "        for log_prob, G in zip(self.log_probs, returns):\n",
    "            # Negative because optimizer minimizes, but we want to maximize\n",
    "            loss -= log_prob * G\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode data\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "# Create REINFORCE agent\n",
    "env = GridWorld()\n",
    "reinforce_agent = REINFORCE(env.n_states, env.n_actions, hidden_size=64, lr=1e-3)\n",
    "print(\"REINFORCE Agent created!\")\n",
    "print(f\"Policy Network: {reinforce_agent.policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce(env, agent, n_episodes=1000):\n",
    "    \"\"\"Train REINFORCE agent\"\"\"\n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        state_tensor = env.get_state_tensor(state)\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Collect full episode\n",
    "        for _ in range(100):\n",
    "            action = agent.act(state_tensor)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.store_reward(reward)\n",
    "            total_reward += reward\n",
    "            state_tensor = env.get_state_tensor(next_state)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Learn from episode (only after it's complete!)\n",
    "        agent.learn()\n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg = np.mean(rewards_history[-100:])\n",
    "            print(f\"Episode {episode+1}: avg_reward={avg:.2f}\")\n",
    "    \n",
    "    return rewards_history\n",
    "\n",
    "# Train REINFORCE\n",
    "env = GridWorld()\n",
    "reinforce_agent = REINFORCE(env.n_states, env.n_actions, hidden_size=64, lr=1e-3)\n",
    "reinforce_rewards = train_reinforce(env, reinforce_agent, n_episodes=1000)\n",
    "\n",
    "print(f\"\\nFinal average reward: {np.mean(reinforce_rewards[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot REINFORCE results\n",
    "plt.figure(figsize=(10, 4))\n",
    "window = 50\n",
    "smoothed = np.convolve(reinforce_rewards, np.ones(window)/window, mode='valid')\n",
    "plt.plot(smoothed)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('REINFORCE on GridWorld')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Show learned policy\n",
    "print(\"\\nREINFORCE Learned Policy:\")\n",
    "symbols = ['‚Üë', '‚Üí', '‚Üì', '‚Üê']\n",
    "for i in range(env.n_states):\n",
    "    if i == env.goal:\n",
    "        print('G', end=' ')\n",
    "    else:\n",
    "        state_tensor = env.get_state_tensor(i)\n",
    "        with torch.no_grad():\n",
    "            probs = reinforce_agent.policy(state_tensor.unsqueeze(0).to(device))\n",
    "            action = probs.argmax().item()\n",
    "        print(symbols[action], end=' ')\n",
    "    if (i + 1) % env.size == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section8'></a>\n",
    "# 8. Actor-Critic Methods\n",
    "\n",
    "## The Problem with REINFORCE\n",
    "\n",
    "REINFORCE has **high variance** because:\n",
    "- We use the full episode return $G_t$\n",
    "- Returns can vary wildly between episodes\n",
    "- This makes learning slow and unstable\n",
    "\n",
    "**Example:** Imagine two episodes with the same good action:\n",
    "- Episode 1: Good action ‚Üí lucky outcomes ‚Üí G = 100\n",
    "- Episode 2: Good action ‚Üí unlucky outcomes ‚Üí G = 10\n",
    "\n",
    "The same action gets very different credit!\n",
    "\n",
    "## The Solution: Use a Critic\n",
    "\n",
    "**Key Idea:** Instead of using raw returns, use a **baseline** to reduce variance.\n",
    "\n",
    "**Actor-Critic Architecture:**\n",
    "- **Actor**: Policy network œÄ(a|s) - decides what to do\n",
    "- **Critic**: Value network V(s) - evaluates how good states are\n",
    "\n",
    "**The Advantage Function:**\n",
    "\n",
    "$A(s,a) = Q(s,a) - V(s)$\n",
    "\n",
    "\"How much better is this action than average?\"\n",
    "\n",
    "We can estimate this with the **TD error**:\n",
    "\n",
    "$A(s,a) \\approx r + \\gamma V(s') - V(s)$\n",
    "\n",
    "**Why does this help?**\n",
    "- If action is better than expected: A > 0 ‚Üí increase probability\n",
    "- If action is worse than expected: A < 0 ‚Üí decrease probability\n",
    "- The baseline V(s) removes the \"luck\" factor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic network with shared feature layers.\n",
    "    \n",
    "    Outputs both:\n",
    "    - Policy (action probabilities)\n",
    "    - Value (state value estimate)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, hidden_size=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(n_states, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor head: outputs action probabilities\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Critic head: outputs state value\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.shared(x)\n",
    "        policy = self.actor(features)\n",
    "        value = self.critic(features)\n",
    "        return policy, value\n",
    "    \n",
    "    def act(self, state):\n",
    "        policy, value = self.forward(state)\n",
    "        dist = Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    \"\"\"Advantage Actor-Critic Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, hidden_size=64, lr=1e-3, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.network = ActorCritic(n_states, n_actions, hidden_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = state.unsqueeze(0).to(device)\n",
    "        action, log_prob, value = self.network.act(state)\n",
    "        return action, log_prob, value.squeeze()\n",
    "    \n",
    "    def learn(self, log_prob, value, reward, next_value, done):\n",
    "        # TD error (advantage estimate)\n",
    "        target = reward + (0 if done else self.gamma * next_value.item())\n",
    "        advantage = target - value.item()\n",
    "        \n",
    "        # Actor loss (policy gradient with advantage)\n",
    "        actor_loss = -log_prob * advantage\n",
    "        \n",
    "        # Critic loss (value function error)\n",
    "        critic_loss = F.mse_loss(value, torch.tensor([target]).to(device))\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = actor_loss + 0.5 * critic_loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_a2c(env, agent, n_episodes=1000):\n",
    "    \"\"\"Train A2C agent (online, step-by-step)\"\"\"\n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        state_tensor = env.get_state_tensor(state)\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(100):\n",
    "            action, log_prob, value = agent.act(state_tensor)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state_tensor = env.get_state_tensor(next_state)\n",
    "            \n",
    "            # Get next value for TD target\n",
    "            with torch.no_grad():\n",
    "                _, next_value = agent.network(next_state_tensor.unsqueeze(0).to(device))\n",
    "                next_value = next_value.squeeze()\n",
    "            \n",
    "            # Learn from this step\n",
    "            agent.learn(log_prob, value, reward, next_value, done)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state_tensor = next_state_tensor\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg = np.mean(rewards_history[-100:])\n",
    "            print(f\"Episode {episode+1}: avg_reward={avg:.2f}\")\n",
    "    \n",
    "    return rewards_history\n",
    "\n",
    "# Train A2C\n",
    "env = GridWorld()\n",
    "a2c_agent = A2CAgent(env.n_states, env.n_actions, hidden_size=64, lr=1e-3)\n",
    "a2c_rewards = train_a2c(env, a2c_agent, n_episodes=1000)\n",
    "\n",
    "print(f\"\\nFinal average reward: {np.mean(a2c_rewards[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare REINFORCE vs A2C\n",
    "plt.figure(figsize=(10, 4))\n",
    "window = 50\n",
    "\n",
    "reinforce_smooth = np.convolve(reinforce_rewards, np.ones(window)/window, mode='valid')\n",
    "a2c_smooth = np.convolve(a2c_rewards, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.plot(reinforce_smooth, label='REINFORCE', alpha=0.8)\n",
    "plt.plot(a2c_smooth, label='A2C', alpha=0.8)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('REINFORCE vs Actor-Critic')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"A2C typically learns faster due to lower variance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section9'></a>\n",
    "# 9. Proximal Policy Optimization (PPO)\n",
    "\n",
    "## The Problem with Vanilla Policy Gradients\n",
    "\n",
    "Large policy updates can **destroy** learning:\n",
    "- If we update too much, the policy can become terrible\n",
    "- Then we collect bad data, which makes learning worse\n",
    "- This creates a death spiral!\n",
    "\n",
    "## PPO's Solution: Constrained Updates\n",
    "\n",
    "**Key Idea:** Keep the new policy \"close\" to the old policy.\n",
    "\n",
    "**How?** Use a clipped objective that prevents too-large updates.\n",
    "\n",
    "**The PPO Clipped Objective:**\n",
    "\n",
    "$L^{CLIP}(\\theta) = \\mathbb{E}\\left[\\min\\left(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t\\right)\\right]$\n",
    "\n",
    "Where:\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the probability ratio\n",
    "- $\\epsilon$ is typically 0.2 (allows 20% change)\n",
    "- $A_t$ is the advantage\n",
    "\n",
    "**In Plain English:**\n",
    "- If the new policy is too different from the old one, clip the objective\n",
    "- This prevents the policy from changing too drastically\n",
    "- Result: stable, reliable learning!\n",
    "\n",
    "## Why PPO is Popular\n",
    "\n",
    "1. **Simple:** Easy to implement (compared to TRPO)\n",
    "2. **Stable:** Doesn't blow up during training\n",
    "3. **Effective:** Works well on many tasks\n",
    "4. **Scalable:** Used by OpenAI for ChatGPT RLHF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    \"\"\"Storage for PPO trajectories\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "    \n",
    "    def store(self, state, action, reward, done, log_prob, value):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "    \n",
    "    def get_batch(self):\n",
    "        return (\n",
    "            torch.stack(self.states),\n",
    "            torch.tensor(self.actions),\n",
    "            torch.tensor(self.rewards, dtype=torch.float32),\n",
    "            torch.tensor(self.dones, dtype=torch.float32),\n",
    "            torch.stack(self.log_probs),\n",
    "            torch.stack(self.values).squeeze()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization Agent.\n",
    "    \n",
    "    Key features:\n",
    "    1. Clipped objective for stable updates\n",
    "    2. Multiple epochs of updates per batch\n",
    "    3. GAE for advantage estimation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, hidden_size=64, lr=3e-4, \n",
    "                 gamma=0.99, gae_lambda=0.95, clip_epsilon=0.2, \n",
    "                 value_coef=0.5, entropy_coef=0.01):\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        self.network = ActorCritic(n_states, n_actions, hidden_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        self.memory = PPOMemory()\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = state.unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            policy, value = self.network(state)\n",
    "        \n",
    "        dist = Categorical(policy)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob, value.squeeze()\n",
    "    \n",
    "    def store(self, state, action, reward, done, log_prob, value):\n",
    "        self.memory.store(state, action, reward, done, log_prob, value)\n",
    "    \n",
    "    def compute_gae(self, rewards, values, dones, next_value):\n",
    "        \"\"\"Compute Generalized Advantage Estimation\"\"\"\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        values = torch.cat([values, next_value.unsqueeze(0)])\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * values[t+1] * (1 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        return torch.tensor(advantages, dtype=torch.float32)\n",
    "    \n",
    "    def learn(self, next_value, n_epochs=4):\n",
    "        states, actions, rewards, dones, old_log_probs, values = self.memory.get_batch()\n",
    "        \n",
    "        # Compute advantages\n",
    "        advantages = self.compute_gae(rewards, values, dones, next_value)\n",
    "        returns = advantages + values\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Move to device\n",
    "        states = states.to(device)\n",
    "        actions = actions.to(device)\n",
    "        old_log_probs = old_log_probs.to(device)\n",
    "        advantages = advantages.to(device)\n",
    "        returns = returns.to(device)\n",
    "        \n",
    "        # PPO update for multiple epochs\n",
    "        for _ in range(n_epochs):\n",
    "            # Get current policy\n",
    "            policy, values = self.network(states)\n",
    "            dist = Categorical(policy)\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            # Probability ratio\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs.squeeze())\n",
    "            \n",
    "            # Clipped surrogate objective\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Value loss\n",
    "            critic_loss = F.mse_loss(values.squeeze(), returns)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        self.memory.clear()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env, agent, n_episodes=1000, update_freq=20):\n",
    "    \"\"\"Train PPO agent\"\"\"\n",
    "    rewards_history = []\n",
    "    steps = 0\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        state_tensor = env.get_state_tensor(state)\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(100):\n",
    "            action, log_prob, value = agent.act(state_tensor)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state_tensor = env.get_state_tensor(next_state)\n",
    "            \n",
    "            agent.store(state_tensor, action, reward, done, log_prob, value)\n",
    "            steps += 1\n",
    "            \n",
    "            total_reward += reward\n",
    "            state_tensor = next_state_tensor\n",
    "            \n",
    "            # Update every update_freq steps\n",
    "            if steps % update_freq == 0:\n",
    "                with torch.no_grad():\n",
    "                    _, next_value = agent.network(next_state_tensor.unsqueeze(0).to(device))\n",
    "                agent.learn(next_value.squeeze())\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg = np.mean(rewards_history[-100:])\n",
    "            print(f\"Episode {episode+1}: avg_reward={avg:.2f}\")\n",
    "    \n",
    "    return rewards_history\n",
    "\n",
    "# Train PPO\n",
    "env = GridWorld()\n",
    "ppo_agent = PPOAgent(env.n_states, env.n_actions, hidden_size=64, lr=3e-4)\n",
    "ppo_rewards = train_ppo(env, ppo_agent, n_episodes=1000)\n",
    "\n",
    "print(f\"\\nFinal average reward: {np.mean(ppo_rewards[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all policy gradient methods\n",
    "plt.figure(figsize=(12, 5))\n",
    "window = 50\n",
    "\n",
    "methods = [\n",
    "    ('REINFORCE', reinforce_rewards),\n",
    "    ('A2C', a2c_rewards),\n",
    "    ('PPO', ppo_rewards)\n",
    "]\n",
    "\n",
    "for name, rewards in methods:\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(smoothed, label=name, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Policy Gradient Methods Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section10'></a>\n",
    "# 10. Summary & Next Steps\n",
    "\n",
    "## What We Covered\n",
    "\n",
    "| Method | Type | Key Idea | PyTorch Component |\n",
    "|--------|------|----------|------------------|\n",
    "| **Q-Learning** | Value-based | Learn Q(s,a), act greedily | `torch.zeros` for Q-table |\n",
    "| **DQN** | Deep Value-based | Neural network Q-function | `nn.Module`, replay buffer |\n",
    "| **Double DQN** | Deep Value-based | Reduce overestimation | Two networks |\n",
    "| **Dueling DQN** | Deep Value-based | Separate V(s) and A(s,a) | Split architecture |\n",
    "| **REINFORCE** | Policy Gradient | Direct policy optimization | `Categorical` distribution |\n",
    "| **Actor-Critic** | Hybrid | Policy + value function | Shared network |\n",
    "| **PPO** | Policy Gradient | Clipped objective | Ratio clipping |\n",
    "\n",
    "## Key PyTorch Patterns for RL\n",
    "\n",
    "```python\n",
    "# 1. Neural Network Definition\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(...)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# 2. Training Loop\n",
    "optimizer = optim.Adam(network.parameters(), lr=1e-3)\n",
    "loss = F.mse_loss(predicted, target)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# 3. Action Sampling (Policy Gradients)\n",
    "dist = Categorical(policy_probs)\n",
    "action = dist.sample()\n",
    "log_prob = dist.log_prob(action)\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Try Gymnasium environments**: `pip install gymnasium`\n",
    "2. **Continuous actions**: Use Gaussian policies (SAC, TD3)\n",
    "3. **Advanced algorithms**: Rainbow DQN, SAC, TD3\n",
    "4. **Multi-agent RL**: MARL, self-play\n",
    "5. **Model-based RL**: World models, MuZero\n",
    "6. **RLHF**: Reinforcement Learning from Human Feedback (used in ChatGPT!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üéì Reinforcement Learning with PyTorch: Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAlgorithms implemented:\")\n",
    "print(\"  ‚úì Multi-Armed Bandits (Œµ-greedy, UCB)\")\n",
    "print(\"  ‚úì Tabular Q-Learning\")\n",
    "print(\"  ‚úì SARSA\")\n",
    "print(\"  ‚úì Deep Q-Network (DQN)\")\n",
    "print(\"  ‚úì Double DQN\")\n",
    "print(\"  ‚úì Dueling DQN\")\n",
    "print(\"  ‚úì REINFORCE\")\n",
    "print(\"  ‚úì Actor-Critic (A2C)\")\n",
    "print(\"  ‚úì Proximal Policy Optimization (PPO)\")\n",
    "print(\"\\nüî• All using PyTorch!\")\n",
    "print(\"\\nüìö Key concepts covered:\")\n",
    "print(\"  ‚Ä¢ Exploration vs Exploitation\")\n",
    "print(\"  ‚Ä¢ Markov Decision Processes\")\n",
    "print(\"  ‚Ä¢ Value Functions & Bellman Equations\")\n",
    "print(\"  ‚Ä¢ Temporal Difference Learning\")\n",
    "print(\"  ‚Ä¢ Experience Replay & Target Networks\")\n",
    "print(\"  ‚Ä¢ Policy Gradients & Advantage Functions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
