{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe6e674a",
   "metadata": {},
   "source": [
    "# ðŸ§  LLM Learning Notebook: From Scratch to Pro\n",
    "\n",
    "A comprehensive, hands-on guide to understanding Large Language Models and Transformer architectures using PyTorch.\n",
    "\n",
    "**What you'll learn:**\n",
    "- PyTorch fundamentals and neural network basics\n",
    "- Attention mechanisms and transformer architecture from scratch\n",
    "- BERT, GPT, and modern LLM architectures\n",
    "- Advanced topics: MoE, efficient attention, scaling laws\n",
    "- Fine-tuning techniques: LoRA, QLoRA, adapters\n",
    "- Inference optimization and deployment\n",
    "- Vision Transformers and multi-modal models\n",
    "- VLA and World Foundation Models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5d9f7c",
   "metadata": {},
   "source": [
    "## ðŸ“š Table of Contents\n",
    "\n",
    "### Foundations\n",
    "1. [PyTorch and Neural Network Fundamentals](#module-1-pytorch-fundamentals) â±ï¸ ~2 hours\n",
    "2. [Text Representation and Embeddings](#module-2-text-representation) â±ï¸ ~1.5 hours\n",
    "3. [Core Attention Mechanisms](#module-3-attention-mechanisms) â±ï¸ ~2 hours\n",
    "4. [Transformer Architecture from Scratch](#module-4-transformer-architecture) â±ï¸ ~3 hours\n",
    "\n",
    "### Architecture Variants\n",
    "5. [BERT: Encoder-Only Architecture](#module-5-bert) â±ï¸ ~2 hours\n",
    "6. [GPT: Decoder-Only Architecture](#module-6-gpt) â±ï¸ ~2 hours\n",
    "\n",
    "### Advanced Architectures\n",
    "7. [Efficient Attention Variants](#module-7-efficient-attention) â±ï¸ ~1.5 hours\n",
    "8. [Mixture of Experts (MoE)](#module-8-moe) â±ï¸ ~2 hours\n",
    "\n",
    "### Modern Techniques\n",
    "9. [Modern LLM Techniques](#module-9-modern-techniques) â±ï¸ ~2 hours\n",
    "10. [Training Fundamentals](#module-10-training) â±ï¸ ~1.5 hours\n",
    "11. [Fine-Tuning Techniques](#module-11-fine-tuning) â±ï¸ ~2.5 hours\n",
    "\n",
    "### Deep Dives\n",
    "12. [Tokenizer Deep Dive (BPE)](#module-12-tokenizers) â±ï¸ ~1.5 hours\n",
    "13. [Scaling Laws](#module-13-scaling-laws) â±ï¸ ~1 hour\n",
    "14. [Inference Optimization](#module-14-inference) â±ï¸ ~2 hours\n",
    "15. [Context Length Extension](#module-15-context-length) â±ï¸ ~1.5 hours\n",
    "\n",
    "### Multi-Modal & Embodied AI\n",
    "16. [Vision Transformers](#module-16-vit) â±ï¸ ~2 hours\n",
    "17. [VLA and World Models](#module-17-vla) â±ï¸ ~1.5 hours\n",
    "\n",
    "### Practice\n",
    "18. [Projects and Exercises](#module-18-projects) â±ï¸ ~4 hours\n",
    "\n",
    "**Total estimated time: ~35 hours**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011c264",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Setup\n",
    "\n",
    "Run the cell below to install dependencies and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd50e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install torch numpy matplotlib seaborn tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple, List\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"\\nâœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad8a9a",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "Utility functions used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape(name: str, tensor: torch.Tensor):\n",
    "    \"\"\"Print tensor name and shape.\"\"\"\n",
    "    print(f\"{name}: {tuple(tensor.shape)}\")\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    \"\"\"Count trainable parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def plot_attention(attention_weights: torch.Tensor, \n",
    "                   x_labels: List[str] = None,\n",
    "                   y_labels: List[str] = None,\n",
    "                   title: str = \"Attention Weights\"):\n",
    "    \"\"\"Visualize attention weights as a heatmap.\"\"\"\n",
    "    weights = attention_weights.detach().cpu().numpy()\n",
    "    if weights.ndim > 2:\n",
    "        weights = weights[0]  # Take first head/batch\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(weights, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=x_labels, yticklabels=y_labels)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Key')\n",
    "    plt.ylabel('Query')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e59548",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-1-pytorch-fundamentals'></a>\n",
    "# Module 1: PyTorch and Neural Network Fundamentals\n",
    "\n",
    "**Prerequisites:** Basic Python programming, familiarity with linear algebra concepts (matrices, vectors)\n",
    "\n",
    "**Learning Objectives:**\n",
    "By the end of this module, you will be able to:\n",
    "- Create and manipulate PyTorch tensors with confidence\n",
    "- Understand how automatic differentiation (autograd) works under the hood\n",
    "- Build custom neural network layers from scratch using nn.Module\n",
    "- Implement activation functions, loss functions, and optimizers manually\n",
    "- Train a simple neural network and understand each step of the process\n",
    "\n",
    "**Why This Matters for LLMs:**\n",
    "Every component of a Large Language Modelâ€”from attention mechanisms to feed-forward networksâ€”is built on these fundamentals. Understanding tensors, gradients, and the training loop is essential before diving into transformer architectures.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c843b",
   "metadata": {},
   "source": [
    "## 1.1 Tensor Basics\n",
    "\n",
    "### What is a Tensor?\n",
    "\n",
    "A **tensor** is a multi-dimensional arrayâ€”the fundamental data structure in PyTorch and the building block of all deep learning. If you're familiar with NumPy arrays, tensors are conceptually identical, but with two game-changing advantages:\n",
    "\n",
    "1. **GPU Acceleration**: Tensors can live on GPUs, enabling massively parallel computation. A single GPU can perform thousands of operations simultaneously, making training feasible for models with billions of parameters.\n",
    "\n",
    "2. **Automatic Differentiation**: PyTorch tracks every operation performed on tensors and can automatically compute gradients. This is the magic that makes backpropagation work without manual calculus.\n",
    "\n",
    "### Understanding Tensor Dimensions\n",
    "\n",
    "Think of tensors as nested containers:\n",
    "\n",
    "| Rank | Name | Real-World Analogy | Example Shape |\n",
    "|------|------|-------------------|---------------|\n",
    "| 0 | Scalar | A single temperature reading | `()` or `torch.tensor(42.0)` |\n",
    "| 1 | Vector | A list of daily temperatures | `(7,)` for a week |\n",
    "| 2 | Matrix | A spreadsheet of data | `(rows, cols)` |\n",
    "| 3 | 3D Tensor | A stack of spreadsheets | `(sheets, rows, cols)` |\n",
    "| 4 | 4D Tensor | Multiple stacks (batches) | `(batch, sheets, rows, cols)` |\n",
    "\n",
    "### Shape Conventions in Transformers\n",
    "\n",
    "Throughout this notebook, we'll consistently use these dimension names:\n",
    "\n",
    "| Symbol | Meaning | Typical Values |\n",
    "|--------|---------|----------------|\n",
    "| `B` | Batch size | 1-64 (limited by GPU memory) |\n",
    "| `S` or `T` | Sequence length | 512, 2048, 4096, 128K |\n",
    "| `D` or `d_model` | Model/embedding dimension | 768, 1024, 4096 |\n",
    "| `H` | Number of attention heads | 8, 12, 32, 64 |\n",
    "| `V` | Vocabulary size | 32K, 50K, 100K |\n",
    "| `d_k` | Key/Query dimension per head | `d_model // H` |\n",
    "\n",
    "**The Most Important Shape:** `(B, S, D)` = `(batch_size, seq_len, d_model)`\n",
    "\n",
    "This is the shape of data flowing through most of a transformer. Each token in each sequence in the batch is represented by a D-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b9329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TENSOR CREATION - Multiple Methods\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"TENSOR CREATION METHODS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: From Python lists (explicit values)\n",
    "# Use when you have specific data\n",
    "print(\"\\nðŸ“Œ Method 1: From Python lists\")\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "print(f\"   torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\")\n",
    "print(f\"   Result: {x}\")\n",
    "print(f\"   Shape: {x.shape} (1D tensor with 5 elements)\")\n",
    "print(f\"   Dtype: {x.dtype} (32-bit float by default for decimals)\")\n",
    "\n",
    "# 2D tensor (matrix)\n",
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "print(f\"\\n   2D tensor (matrix):\")\n",
    "print(f\"   {matrix}\")\n",
    "print(f\"   Shape: {matrix.shape} (2 rows, 3 columns)\")\n",
    "\n",
    "# Method 2: Initialized tensors (common patterns)\n",
    "print(\"\\nðŸ“Œ Method 2: Initialized tensors\")\n",
    "print(f\"   torch.zeros(2, 3):    All zeros\")\n",
    "print(f\"   {torch.zeros(2, 3)}\")\n",
    "print(f\"\\n   torch.ones(2, 3):     All ones\")\n",
    "print(f\"   {torch.ones(2, 3)}\")\n",
    "print(f\"\\n   torch.eye(3):         Identity matrix\")\n",
    "print(f\"   {torch.eye(3)}\")\n",
    "\n",
    "# Method 3: Random tensors (crucial for initialization)\n",
    "print(\"\\nðŸ“Œ Method 3: Random tensors\")\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "print(f\"   torch.rand(2, 3):     Uniform [0, 1)\")\n",
    "print(f\"   {torch.rand(2, 3)}\")\n",
    "print(f\"\\n   torch.randn(2, 3):    Normal (mean=0, std=1)\")\n",
    "print(f\"   {torch.randn(2, 3)}\")\n",
    "print(f\"\\n   torch.randint(0, 10, (2, 3)):  Random integers [0, 10)\")\n",
    "print(f\"   {torch.randint(0, 10, (2, 3))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c6ac25",
   "metadata": {},
   "source": [
    "### Why Random Initialization Matters\n",
    "\n",
    "When we create neural network weights, we initialize them randomly. This is crucial because:\n",
    "\n",
    "1. **Symmetry Breaking**: If all weights start the same, all neurons learn the same thing\n",
    "2. **Gradient Flow**: Proper initialization prevents gradients from vanishing or exploding\n",
    "3. **Convergence Speed**: Good initialization helps the network learn faster\n",
    "\n",
    "We'll explore specific initialization strategies (Xavier, He) later in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da18f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TENSOR SHAPES IN TRANSFORMERS - Detailed\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRANSFORMER TENSOR SHAPES - A Complete Picture\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Let's trace data through a transformer\n",
    "batch_size = 2      # Processing 2 sentences simultaneously\n",
    "seq_len = 4         # Each sentence has 4 tokens\n",
    "d_model = 8         # Each token represented by 8 numbers\n",
    "vocab_size = 100    # Our vocabulary has 100 unique tokens\n",
    "n_heads = 2         # Using 2 attention heads\n",
    "d_k = d_model // n_heads  # Dimension per head\n",
    "\n",
    "print(f\"\"\"\n",
    "Configuration:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  batch_size (B)  = {batch_size:4d}                  â”‚\n",
    "â”‚  seq_len (S)     = {seq_len:4d}                  â”‚\n",
    "â”‚  d_model (D)     = {d_model:4d}                  â”‚\n",
    "â”‚  vocab_size (V)  = {vocab_size:4d}                 â”‚\n",
    "â”‚  n_heads (H)     = {n_heads:4d}                  â”‚\n",
    "â”‚  d_k (D/H)       = {d_k:4d}                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "# Stage 1: Input - Token IDs\n",
    "print(\"Stage 1: INPUT TOKEN IDs\")\n",
    "print(\"-\" * 40)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "print(f\"Shape: (B, S) = ({batch_size}, {seq_len})\")\n",
    "print(f\"These are integer indices into the vocabulary:\")\n",
    "print(f\"{input_ids}\")\n",
    "print(f\"Each number represents a word/subword in our vocabulary\\n\")\n",
    "\n",
    "# Stage 2: After Embedding\n",
    "print(\"Stage 2: AFTER EMBEDDING LAYER\")\n",
    "print(\"-\" * 40)\n",
    "embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"Shape: (B, S, D) = ({batch_size}, {seq_len}, {d_model})\")\n",
    "print(f\"Each token ID is now a {d_model}-dimensional vector:\")\n",
    "print(f\"Token 0 of Sentence 0: {embeddings[0, 0, :4]}... (showing first 4 dims)\\n\")\n",
    "\n",
    "# Stage 3: Inside Attention\n",
    "print(\"Stage 3: INSIDE MULTI-HEAD ATTENTION\")\n",
    "print(\"-\" * 40)\n",
    "# Q, K, V after projection and head splitting\n",
    "Q = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "K = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "V = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "print(f\"Q, K, V shapes: (B, H, S, d_k) = ({batch_size}, {n_heads}, {seq_len}, {d_k})\")\n",
    "print(f\"Each head processes {d_k}-dimensional queries, keys, values\\n\")\n",
    "\n",
    "# Attention weights\n",
    "attn_weights = torch.softmax(torch.randn(batch_size, n_heads, seq_len, seq_len), dim=-1)\n",
    "print(f\"Attention weights: (B, H, S, S) = {attn_weights.shape}\")\n",
    "print(f\"This is a {seq_len}x{seq_len} matrix showing how each token attends to others\")\n",
    "print(f\"Head 0, Sentence 0 attention pattern:\")\n",
    "print(f\"{attn_weights[0, 0].round(decimals=2)}\\n\")\n",
    "\n",
    "# Stage 4: Output Logits\n",
    "print(\"Stage 4: OUTPUT LOGITS\")\n",
    "print(\"-\" * 40)\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "print(f\"Shape: (B, S, V) = ({batch_size}, {seq_len}, {vocab_size})\")\n",
    "print(f\"For each position, we have {vocab_size} scores (one per vocab token)\")\n",
    "print(f\"Apply softmax to get probability distribution over vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db53af",
   "metadata": {},
   "source": [
    "## 1.2 Tensor Operations\n",
    "\n",
    "### The Operations That Power Transformers\n",
    "\n",
    "Understanding tensor operations is crucial because transformers are essentially a carefully orchestrated sequence of:\n",
    "1. **Matrix multiplications** (attention scores, projections)\n",
    "2. **Element-wise operations** (activations, residual connections)\n",
    "3. **Reductions** (softmax normalization, loss computation)\n",
    "\n",
    "Let's master each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaff5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MATRIX MULTIPLICATION - The Heart of Deep Learning\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"MATRIX MULTIPLICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "Matrix multiplication is THE most important operation in deep learning.\n",
    "Every linear layer, every attention computation uses it.\n",
    "\n",
    "For matrices A (mÃ—n) and B (nÃ—p), the result C = A @ B has shape (mÃ—p)\n",
    "Each element C[i,j] = sum of A[i,:] * B[:,j] (dot product of row and column)\n",
    "\"\"\")\n",
    "\n",
    "# Simple example\n",
    "A = torch.tensor([[1., 2.],\n",
    "                  [3., 4.]])\n",
    "B = torch.tensor([[5., 6.],\n",
    "                  [7., 8.]])\n",
    "\n",
    "print(\"A =\")\n",
    "print(A)\n",
    "print(\"\\nB =\")\n",
    "print(B)\n",
    "\n",
    "# Three equivalent ways to multiply\n",
    "C1 = A @ B                    # Preferred syntax\n",
    "C2 = torch.matmul(A, B)       # Explicit function\n",
    "C3 = torch.mm(A, B)           # Only for 2D matrices\n",
    "\n",
    "print(f\"\\nA @ B =\")\n",
    "print(C1)\n",
    "print(f\"\\nManual calculation of C[0,0]:\")\n",
    "print(f\"  A[0,:] Â· B[:,0] = [1,2] Â· [5,7] = 1*5 + 2*7 = {1*5 + 2*7}\")\n",
    "\n",
    "# Batched matrix multiplication (crucial for transformers!)\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"BATCHED MATRIX MULTIPLICATION\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\"\"\n",
    "In transformers, we process multiple samples (batch) and multiple heads simultaneously.\n",
    "torch.matmul handles this automatically!\n",
    "\"\"\")\n",
    "\n",
    "batch, heads, seq, dim = 2, 4, 8, 16\n",
    "Q = torch.randn(batch, heads, seq, dim)\n",
    "K = torch.randn(batch, heads, seq, dim)\n",
    "\n",
    "# Attention scores: Q @ K^T\n",
    "# We want (B, H, S, D) @ (B, H, D, S) -> (B, H, S, S)\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1))  # Transpose last two dims\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"K transposed shape: {K.transpose(-2, -1).shape}\")\n",
    "print(f\"Attention scores (Q @ K^T) shape: {scores.shape}\")\n",
    "print(f\"\\nThis gives us a {seq}x{seq} attention matrix for each head in each batch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56072075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ELEMENT-WISE OPERATIONS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ELEMENT-WISE OPERATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "Element-wise operations apply the same operation to each element independently.\n",
    "Used for: activations, residual connections, scaling, masking\n",
    "\"\"\")\n",
    "\n",
    "a = torch.tensor([1., 2., 3., 4.])\n",
    "b = torch.tensor([10., 20., 30., 40.])\n",
    "\n",
    "print(f\"a = {a.tolist()}\")\n",
    "print(f\"b = {b.tolist()}\")\n",
    "print(f\"\\na + b = {(a + b).tolist()}  (element-wise addition)\")\n",
    "print(f\"a * b = {(a * b).tolist()}  (element-wise multiplication, NOT dot product)\")\n",
    "print(f\"a / b = {(a / b).tolist()}  (element-wise division)\")\n",
    "print(f\"a ** 2 = {(a ** 2).tolist()}  (element-wise power)\")\n",
    "\n",
    "# Residual connection example\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"RESIDUAL CONNECTION EXAMPLE\")\n",
    "print(\"-\" * 40)\n",
    "x = torch.randn(2, 4)  # Input\n",
    "sublayer_output = torch.randn(2, 4)  # Output of attention or FFN\n",
    "\n",
    "# Residual: add input to output\n",
    "residual_output = x + sublayer_output\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Sublayer output shape: {sublayer_output.shape}\")\n",
    "print(f\"Residual (x + sublayer) shape: {residual_output.shape}\")\n",
    "print(\"\\nResidual connections help gradients flow and enable training deep networks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BROADCASTING - Automatic Shape Expansion\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BROADCASTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "Broadcasting automatically expands smaller tensors to match larger ones.\n",
    "Rules:\n",
    "1. Dimensions are compared from right to left\n",
    "2. Dimensions match if they're equal OR one of them is 1\n",
    "3. Missing dimensions are treated as 1\n",
    "\n",
    "This is ESSENTIAL for efficient transformer operations!\n",
    "\"\"\")\n",
    "\n",
    "# Example 1: Scalar + Matrix\n",
    "matrix = torch.ones(2, 3)\n",
    "scalar = 5\n",
    "result = matrix + scalar\n",
    "print(\"Example 1: Scalar + Matrix\")\n",
    "print(f\"Matrix (2,3) + Scalar () = Result (2,3)\")\n",
    "print(f\"{matrix} + {scalar} =\\n{result}\\n\")\n",
    "\n",
    "# Example 2: Vector + Matrix (add to each row)\n",
    "matrix = torch.ones(2, 3)\n",
    "row_vector = torch.tensor([1., 2., 3.])\n",
    "result = matrix + row_vector\n",
    "print(\"Example 2: Matrix + Row Vector\")\n",
    "print(f\"Matrix (2,3) + Vector (3,) = Result (2,3)\")\n",
    "print(f\"Vector is broadcast across rows:\\n{result}\\n\")\n",
    "\n",
    "# Example 3: Attention mask broadcasting (CRITICAL for transformers)\n",
    "print(\"Example 3: Attention Mask Broadcasting\")\n",
    "print(\"-\" * 40)\n",
    "batch, heads, seq = 2, 4, 8\n",
    "\n",
    "# Attention scores: (B, H, S, S)\n",
    "scores = torch.randn(batch, heads, seq, seq)\n",
    "\n",
    "# Causal mask: (1, 1, S, S) - same mask for all batches and heads\n",
    "# Upper triangular = future positions = -inf\n",
    "mask = torch.triu(torch.ones(1, 1, seq, seq) * float('-inf'), diagonal=1)\n",
    "\n",
    "print(f\"Scores shape: {scores.shape}\")\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "print(f\"Mask (showing 4x4 corner):\")\n",
    "print(mask[0, 0, :4, :4])\n",
    "\n",
    "# Broadcasting: (B, H, S, S) + (1, 1, S, S) -> (B, H, S, S)\n",
    "masked_scores = scores + mask\n",
    "print(f\"\\nMasked scores shape: {masked_scores.shape}\")\n",
    "print(\"The mask broadcasts across all batches and heads automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9854f5dc",
   "metadata": {},
   "source": [
    "## 1.3 Automatic Differentiation (Autograd)\n",
    "\n",
    "### The Magic Behind Training Neural Networks\n",
    "\n",
    "Training a neural network requires computing **gradients**â€”derivatives that tell us how to adjust each parameter to reduce the loss. For a network with millions of parameters, computing these manually would be impossible.\n",
    "\n",
    "PyTorch's **autograd** system solves this by:\n",
    "1. Building a **computational graph** as you perform operations\n",
    "2. Automatically computing gradients via the **chain rule** when you call `.backward()`\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Forward Pass:  x â†’ [operation] â†’ y â†’ [operation] â†’ loss\n",
    "                    â†“                    â†“\n",
    "Backward Pass: âˆ‚loss/âˆ‚x â† [gradient] â† âˆ‚loss/âˆ‚y â† [gradient] â† 1\n",
    "```\n",
    "\n",
    "The chain rule states: if `y = f(x)` and `loss = g(y)`, then:\n",
    "$$\\frac{\\partial loss}{\\partial x} = \\frac{\\partial loss}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$$\n",
    "\n",
    "PyTorch applies this automatically through the entire computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0580c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# AUTOGRAD BASICS\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"AUTOGRAD - Automatic Differentiation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a tensor that tracks gradients\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "print(f\"x = {x}\")\n",
    "print(f\"requires_grad = {x.requires_grad}\")\n",
    "\n",
    "# Perform operations (PyTorch builds computational graph)\n",
    "y = x ** 2          # y = [4, 9]\n",
    "z = y * 3           # z = [12, 27]\n",
    "loss = z.sum()      # loss = 39 (need scalar for backward)\n",
    "\n",
    "print(f\"\\nForward pass:\")\n",
    "print(f\"  y = xÂ² = {y.tolist()}\")\n",
    "print(f\"  z = 3y = {z.tolist()}\")\n",
    "print(f\"  loss = sum(z) = {loss.item()}\")\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nBackward pass (chain rule):\")\n",
    "print(f\"  âˆ‚loss/âˆ‚z = [1, 1] (derivative of sum)\")\n",
    "print(f\"  âˆ‚z/âˆ‚y = [3, 3] (derivative of 3y)\")\n",
    "print(f\"  âˆ‚y/âˆ‚x = [2xâ‚, 2xâ‚‚] = [4, 6] (derivative of xÂ²)\")\n",
    "print(f\"  âˆ‚loss/âˆ‚x = âˆ‚loss/âˆ‚z Â· âˆ‚z/âˆ‚y Â· âˆ‚y/âˆ‚x = 1 Â· 3 Â· 2x = 6x\")\n",
    "print(f\"\\nComputed gradient: x.grad = {x.grad.tolist()}\")\n",
    "print(f\"Expected: 6 * [2, 3] = [12, 18] âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPUTATIONAL GRAPH VISUALIZATION\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPUTATIONAL GRAPH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "For the computation: loss = sum(3 * xÂ²)\n",
    "\n",
    "Forward Pass (building the graph):\n",
    "                                    \n",
    "    x â”€â”€â”€â”€â”€â”€â”¬â”€â”€â–º xÂ² â”€â”€â–º *3 â”€â”€â–º sum â”€â”€â–º loss\n",
    "            â”‚    â†‘       â†‘       â†‘\n",
    "            â”‚   y=xÂ²    z=3y   scalar\n",
    "            â”‚\n",
    "    Each node remembers how to compute its gradient!\n",
    "\n",
    "Backward Pass (traversing the graph):\n",
    "\n",
    "    âˆ‚loss/âˆ‚x â—„â”€â”€ âˆ‚y/âˆ‚x â—„â”€â”€ âˆ‚z/âˆ‚y â—„â”€â”€ âˆ‚loss/âˆ‚z â—„â”€â”€ 1\n",
    "       6x    Â·    2x    Â·    3    Â·     1\n",
    "\"\"\")\n",
    "\n",
    "# More complex example: neural network layer\n",
    "print(\"\\nNeural Network Layer Example:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Simulating: output = relu(x @ W + b)\n",
    "x = torch.randn(2, 3, requires_grad=True)   # Input\n",
    "W = torch.randn(3, 4, requires_grad=True)   # Weights\n",
    "b = torch.randn(4, requires_grad=True)      # Bias\n",
    "\n",
    "# Forward pass\n",
    "linear = x @ W + b                           # Linear transformation\n",
    "output = torch.relu(linear)                  # Activation\n",
    "loss = output.sum()                          # Dummy loss\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"W shape: {W.shape}\")\n",
    "print(f\"b shape: {b.shape}\")\n",
    "print(f\"output shape: {output.shape}\")\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nGradients computed:\")\n",
    "print(f\"  x.grad shape: {x.grad.shape} (same as x)\")\n",
    "print(f\"  W.grad shape: {W.grad.shape} (same as W)\")\n",
    "print(f\"  b.grad shape: {b.grad.shape} (same as b)\")\n",
    "print(\"\\nThese gradients tell us how to update W and b to reduce loss!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d4feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GRADIENT ACCUMULATION (Important!)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GRADIENT ACCUMULATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "âš ï¸ IMPORTANT: Gradients ACCUMULATE by default!\n",
    "Each call to .backward() ADDS to existing gradients.\n",
    "You must call optimizer.zero_grad() or tensor.grad.zero_() before each backward pass.\n",
    "\"\"\")\n",
    "\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "# First backward\n",
    "y1 = (x ** 2).sum()\n",
    "y1.backward()\n",
    "print(f\"After first backward: x.grad = {x.grad.tolist()}\")\n",
    "\n",
    "# Second backward WITHOUT zeroing\n",
    "y2 = (x ** 2).sum()\n",
    "y2.backward()\n",
    "print(f\"After second backward (accumulated!): x.grad = {x.grad.tolist()}\")\n",
    "\n",
    "# Correct approach: zero gradients first\n",
    "x.grad.zero_()\n",
    "y3 = (x ** 2).sum()\n",
    "y3.backward()\n",
    "print(f\"After zeroing and third backward: x.grad = {x.grad.tolist()}\")\n",
    "\n",
    "print(\"\"\"\n",
    "This is why every training loop has:\n",
    "    optimizer.zero_grad()  # Clear old gradients\n",
    "    loss.backward()        # Compute new gradients\n",
    "    optimizer.step()       # Update parameters\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db434ab0",
   "metadata": {},
   "source": [
    "## 1.4 Building Custom Layers with nn.Module\n",
    "\n",
    "### The nn.Module Pattern\n",
    "\n",
    "Every neural network component in PyTorch inherits from `nn.Module`. This base class provides:\n",
    "\n",
    "1. **Parameter Management**: Automatically tracks learnable parameters\n",
    "2. **Device Movement**: `.to(device)` moves all parameters to GPU/CPU\n",
    "3. **Training Mode**: `.train()` and `.eval()` for dropout/batchnorm behavior\n",
    "4. **Serialization**: `.state_dict()` for saving/loading models\n",
    "\n",
    "### The Standard Pattern\n",
    "\n",
    "```python\n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()  # Always call parent __init__\n",
    "        # Define parameters and sub-modules here\n",
    "        self.weight = nn.Parameter(torch.randn(...))\n",
    "        self.sublayer = nn.Linear(...)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define computation here\n",
    "        return self.weight * x + self.sublayer(x)\n",
    "```\n",
    "\n",
    "### nn.Parameter vs Regular Tensor\n",
    "\n",
    "- `nn.Parameter`: Automatically registered, included in `.parameters()`, saved in state_dict\n",
    "- Regular tensor: Not tracked, not saved, not updated by optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad660e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPLEMENTING LINEAR LAYER FROM SCRATCH\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"LINEAR LAYER FROM SCRATCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class LinearFromScratch(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear transformation: y = xW^T + b\n",
    "    \n",
    "    This is the fundamental building block of neural networks.\n",
    "    Every projection in a transformer (Q, K, V, output) uses this.\n",
    "    \n",
    "    Args:\n",
    "        in_features: Size of each input sample\n",
    "        out_features: Size of each output sample\n",
    "        bias: If True, adds a learnable bias\n",
    "    \n",
    "    Shape:\n",
    "        - Input: (*, in_features) where * means any number of dimensions\n",
    "        - Output: (*, out_features)\n",
    "    \n",
    "    Example:\n",
    "        For d_model=512, projecting to d_k=64:\n",
    "        >>> proj = LinearFromScratch(512, 64)\n",
    "        >>> x = torch.randn(batch, seq_len, 512)\n",
    "        >>> out = proj(x)  # Shape: (batch, seq_len, 64)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initialize weights using Xavier/Glorot initialization\n",
    "        # This helps maintain variance of activations across layers\n",
    "        std = math.sqrt(2.0 / (in_features + out_features))\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * std)\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            # Register as None so it's tracked but not a parameter\n",
    "            self.register_parameter('bias', None)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: y = xW^T + b\n",
    "        \n",
    "        Why W^T? Convention is weight shape (out, in), but we want\n",
    "        x @ W^T where x is (..., in) to get (..., out)\n",
    "        \"\"\"\n",
    "        # Matrix multiplication: (..., in) @ (in, out) -> (..., out)\n",
    "        output = torch.matmul(x, self.weight.T)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias  # Broadcasting handles this\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        \"\"\"String representation for print(model)\"\"\"\n",
    "        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}'\n",
    "\n",
    "# Test our implementation\n",
    "print(\"\\nTesting LinearFromScratch:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "in_features, out_features = 4, 3\n",
    "batch_size, seq_len = 2, 5\n",
    "\n",
    "layer = LinearFromScratch(in_features, out_features)\n",
    "x = torch.randn(batch_size, seq_len, in_features)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Weight shape: {layer.weight.shape}\")\n",
    "print(f\"Bias shape: {layer.bias.shape}\")\n",
    "output = layer(x)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "print(f\"\\nModel structure:\")\n",
    "print(layer)\n",
    "\n",
    "print(f\"\\nParameters:\")\n",
    "for name, param in layer.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}, requires_grad={param.requires_grad}\")\n",
    "\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in layer.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85f0896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VERIFY AGAINST PYTORCH'S nn.Linear\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICATION: Our Implementation vs PyTorch\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create both layers\n",
    "our_layer = LinearFromScratch(4, 3)\n",
    "pytorch_layer = nn.Linear(4, 3)\n",
    "\n",
    "# Copy weights from PyTorch to our layer\n",
    "with torch.no_grad():\n",
    "    our_layer.weight.copy_(pytorch_layer.weight)\n",
    "    our_layer.bias.copy_(pytorch_layer.bias)\n",
    "\n",
    "# Test with same input\n",
    "x = torch.randn(2, 5, 4)\n",
    "our_output = our_layer(x)\n",
    "pytorch_output = pytorch_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Our output shape: {our_output.shape}\")\n",
    "print(f\"PyTorch output shape: {pytorch_output.shape}\")\n",
    "\n",
    "# Check if outputs match\n",
    "match = torch.allclose(our_output, pytorch_output, atol=1e-6)\n",
    "print(f\"\\nOutputs match: {match} âœ“\" if match else f\"\\nOutputs DON'T match âœ—\")\n",
    "\n",
    "if match:\n",
    "    print(\"\\nðŸŽ‰ Our from-scratch implementation is correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b8fda",
   "metadata": {},
   "source": [
    "## 1.5 Activation Functions\n",
    "\n",
    "### Why Do We Need Activation Functions?\n",
    "\n",
    "Without activation functions, a neural network is just a series of linear transformations:\n",
    "```\n",
    "y = Wâ‚ƒ(Wâ‚‚(Wâ‚x + bâ‚) + bâ‚‚) + bâ‚ƒ = W_combined Â· x + b_combined\n",
    "```\n",
    "\n",
    "No matter how many layers, it collapses to a single linear transformation! **Activation functions introduce non-linearity**, allowing networks to learn complex patterns.\n",
    "\n",
    "### The Activation Functions You'll See in Transformers\n",
    "\n",
    "| Function | Formula | Used In | Why |\n",
    "|----------|---------|---------|-----|\n",
    "| ReLU | max(0, x) | Classic FFN | Simple, fast, sparse |\n",
    "| GELU | x Â· Î¦(x) | BERT, GPT-2+ | Smooth, probabilistic |\n",
    "| SiLU/Swish | x Â· Ïƒ(x) | LLaMA, modern LLMs | Smooth, self-gated |\n",
    "| SwiGLU | Swish(xW) âŠ™ (xV) | LLaMA 2, Mistral | Best performance |\n",
    "| Softmax | exp(xáµ¢)/Î£exp(xâ±¼) | Attention | Probability distribution |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32538bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ACTIVATION FUNCTIONS FROM SCRATCH\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"ACTIVATION FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class ActivationFunctions:\n",
    "    \"\"\"Collection of activation functions implemented from scratch.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        ReLU: Rectified Linear Unit\n",
    "        f(x) = max(0, x)\n",
    "        \n",
    "        Pros: Simple, fast, creates sparsity\n",
    "        Cons: \"Dying ReLU\" - neurons can get stuck at 0\n",
    "        \"\"\"\n",
    "        return torch.maximum(x, torch.tensor(0.0))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        GELU: Gaussian Error Linear Unit\n",
    "        f(x) = x * Î¦(x) where Î¦ is the CDF of standard normal\n",
    "        \n",
    "        Approximation: 0.5 * x * (1 + tanh(sqrt(2/Ï€) * (x + 0.044715 * xÂ³)))\n",
    "        \n",
    "        Used in: BERT, GPT-2, GPT-3\n",
    "        Why: Smooth, allows small negative values, probabilistic interpretation\n",
    "        \"\"\"\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x ** 3)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def silu(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        SiLU/Swish: Sigmoid Linear Unit\n",
    "        f(x) = x * sigmoid(x)\n",
    "        \n",
    "        Used in: LLaMA, modern LLMs\n",
    "        Why: Self-gated, smooth, works well in practice\n",
    "        \"\"\"\n",
    "        return x * torch.sigmoid(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Softmax: Converts logits to probability distribution\n",
    "        f(xáµ¢) = exp(xáµ¢) / Î£â±¼ exp(xâ±¼)\n",
    "        \n",
    "        Used in: Attention weights, output layer\n",
    "        \n",
    "        Numerical stability: Subtract max before exp to prevent overflow\n",
    "        \"\"\"\n",
    "        x_max = x.max(dim=dim, keepdim=True).values\n",
    "        exp_x = torch.exp(x - x_max)  # Subtract max for stability\n",
    "        return exp_x / exp_x.sum(dim=dim, keepdim=True)\n",
    "\n",
    "# Visualize activation functions\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# ReLU\n",
    "ax = axes[0, 0]\n",
    "y_relu = ActivationFunctions.relu(x)\n",
    "ax.plot(x.numpy(), y_relu.numpy(), 'b-', linewidth=2, label='ReLU')\n",
    "ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax.set_title('ReLU: max(0, x)', fontsize=12)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# GELU\n",
    "ax = axes[0, 1]\n",
    "y_gelu = ActivationFunctions.gelu(x)\n",
    "ax.plot(x.numpy(), y_gelu.numpy(), 'g-', linewidth=2, label='GELU')\n",
    "ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax.set_title('GELU: x Â· Î¦(x)', fontsize=12)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# SiLU/Swish\n",
    "ax = axes[1, 0]\n",
    "y_silu = ActivationFunctions.silu(x)\n",
    "ax.plot(x.numpy(), y_silu.numpy(), 'r-', linewidth=2, label='SiLU/Swish')\n",
    "ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax.set_title('SiLU/Swish: x Â· Ïƒ(x)', fontsize=12)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Softmax\n",
    "ax = axes[1, 1]\n",
    "logits = torch.tensor([-1.0, 0.5, 2.0, 0.0])\n",
    "probs = ActivationFunctions.softmax(logits)\n",
    "ax.bar(range(len(logits)), probs.numpy(), color='purple', alpha=0.7)\n",
    "ax.set_title(f'Softmax: logits {logits.tolist()} â†’ probs', fontsize=12)\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(range(len(logits)))\n",
    "for i, (l, p) in enumerate(zip(logits, probs)):\n",
    "    ax.annotate(f'{p:.2f}', (i, p + 0.02), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Activation functions visualized!\")\n",
    "print(\"\\nKey insight: GELU and SiLU are smooth approximations of ReLU\")\n",
    "print(\"that allow small negative gradients, helping with training stability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d37f46",
   "metadata": {},
   "source": [
    "## 1.6 Loss Functions\n",
    "\n",
    "### What is a Loss Function?\n",
    "\n",
    "A **loss function** (or cost function) measures how wrong our model's predictions are. Training minimizes this loss by adjusting parameters via gradient descent.\n",
    "\n",
    "### Loss Functions in Language Models\n",
    "\n",
    "| Loss | Formula | Used For |\n",
    "|------|---------|----------|\n",
    "| Cross-Entropy | -Î£ yáµ¢ log(Å·áµ¢) | Classification, next-token prediction |\n",
    "| MSE | (1/n) Î£(y - Å·)Â² | Regression, embeddings |\n",
    "| Binary CE | -[y log(Å·) + (1-y) log(1-Å·)] | Binary classification |\n",
    "\n",
    "### Cross-Entropy: The Language Model Loss\n",
    "\n",
    "For language models, we predict the next token from a vocabulary of V tokens. Cross-entropy loss measures how well our predicted probability distribution matches the true distribution (which is 1 for the correct token, 0 for others).\n",
    "\n",
    "**Intuition**: If the model assigns probability 0.9 to the correct token, loss is low (-log(0.9) â‰ˆ 0.1). If it assigns 0.01, loss is high (-log(0.01) â‰ˆ 4.6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e10617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOSS FUNCTIONS FROM SCRATCH\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"LOSS FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class LossFunctions:\n",
    "    \"\"\"Loss functions implemented from scratch.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse_loss(predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Mean Squared Error Loss\n",
    "        L = (1/n) Î£(y - Å·)Â²\n",
    "        \n",
    "        Used for: Regression tasks, embedding similarity\n",
    "        \"\"\"\n",
    "        return ((predictions - targets) ** 2).mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entropy_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Cross-Entropy Loss for classification\n",
    "        L = -Î£ yáµ¢ log(softmax(logits)áµ¢)\n",
    "        \n",
    "        For one-hot targets, this simplifies to: L = -log(softmax(logits)[target])\n",
    "        \n",
    "        Args:\n",
    "            logits: Raw scores, shape (batch, num_classes) or (batch, seq, vocab)\n",
    "            targets: Class indices, shape (batch,) or (batch, seq)\n",
    "        \n",
    "        Used for: Classification, language modeling (next token prediction)\n",
    "        \"\"\"\n",
    "        # Numerical stability: log_softmax is more stable than log(softmax)\n",
    "        # log_softmax(x) = x - log(Î£exp(x))\n",
    "        log_probs = logits - torch.logsumexp(logits, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Gather the log probability of the correct class\n",
    "        # targets shape: (...) -> (..., 1) for gather\n",
    "        targets_expanded = targets.unsqueeze(-1)\n",
    "        correct_log_probs = log_probs.gather(dim=-1, index=targets_expanded)\n",
    "        \n",
    "        # Return negative mean log probability\n",
    "        return -correct_log_probs.mean()\n",
    "\n",
    "# Example: Cross-entropy for language modeling\n",
    "print(\"\\nðŸ“Œ Cross-Entropy Loss Example (Language Modeling)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "vocab_size = 5\n",
    "\n",
    "# Model outputs logits for each position\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "print(f\"Logits shape: {logits.shape} (batch, seq, vocab)\")\n",
    "\n",
    "# True next tokens\n",
    "targets = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "print(f\"Targets shape: {targets.shape}\")\n",
    "print(f\"Targets: {targets}\")\n",
    "\n",
    "# Compute loss\n",
    "our_loss = LossFunctions.cross_entropy_loss(logits.view(-1, vocab_size), targets.view(-1))\n",
    "pytorch_loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "print(f\"\\nOur cross-entropy loss: {our_loss.item():.4f}\")\n",
    "print(f\"PyTorch cross-entropy:  {pytorch_loss.item():.4f}\")\n",
    "print(f\"Match: {torch.allclose(our_loss, pytorch_loss, atol=1e-5)} âœ“\")\n",
    "\n",
    "# Visualize: How loss changes with confidence\n",
    "print(\"\\nðŸ“Œ Loss vs Confidence Visualization\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "confidences = torch.linspace(0.01, 0.99, 100)\n",
    "losses = -torch.log(confidences)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(confidences.numpy(), losses.numpy(), 'b-', linewidth=2)\n",
    "plt.xlabel('Predicted Probability for Correct Class', fontsize=12)\n",
    "plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "plt.title('Cross-Entropy Loss: Higher confidence â†’ Lower loss', fontsize=14)\n",
    "plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate key points\n",
    "plt.annotate('Low confidence\\n(p=0.1, loss=2.3)', xy=(0.1, -np.log(0.1)), \n",
    "             xytext=(0.25, 3), fontsize=10, arrowprops=dict(arrowstyle='->', color='red'))\n",
    "plt.annotate('High confidence\\n(p=0.9, loss=0.1)', xy=(0.9, -np.log(0.9)), \n",
    "             xytext=(0.65, 1), fontsize=10, arrowprops=dict(arrowstyle='->', color='green'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3da43",
   "metadata": {},
   "source": [
    "## 1.7 Optimizers: How Models Learn\n",
    "\n",
    "### The Optimization Loop\n",
    "\n",
    "Training a neural network is an optimization problem: find parameters Î¸ that minimize the loss L(Î¸).\n",
    "\n",
    "```\n",
    "repeat:\n",
    "    1. Forward pass: compute predictions and loss\n",
    "    2. Backward pass: compute gradients âˆ‚L/âˆ‚Î¸\n",
    "    3. Update: Î¸ = Î¸ - learning_rate Ã— âˆ‚L/âˆ‚Î¸\n",
    "```\n",
    "\n",
    "### Optimizer Evolution\n",
    "\n",
    "| Optimizer | Update Rule | Key Idea |\n",
    "|-----------|-------------|----------|\n",
    "| SGD | Î¸ -= lr Ã— g | Simple gradient descent |\n",
    "| Momentum | v = Î²v + g; Î¸ -= lr Ã— v | Accumulate velocity |\n",
    "| Adam | Combines momentum + adaptive lr | Best of both worlds |\n",
    "| AdamW | Adam + decoupled weight decay | Standard for transformers |\n",
    "\n",
    "### Adam: The Transformer's Optimizer\n",
    "\n",
    "Adam (Adaptive Moment Estimation) maintains:\n",
    "- **m**: First moment (mean of gradients) - like momentum\n",
    "- **v**: Second moment (variance of gradients) - for adaptive learning rate\n",
    "\n",
    "This allows different learning rates for different parameters, which is crucial for training transformers where some parameters need larger updates than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af513d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPTIMIZERS FROM SCRATCH\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"OPTIMIZERS FROM SCRATCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent\n",
    "    \n",
    "    Update rule: Î¸ = Î¸ - lr Ã— âˆ‚L/âˆ‚Î¸\n",
    "    \n",
    "    Simple but effective. The foundation of all optimizers.\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters, lr: float = 0.01):\n",
    "        self.parameters = list(parameters)\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters:\n",
    "                if param.grad is not None:\n",
    "                    param -= self.lr * param.grad\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    \"\"\"\n",
    "    Adam Optimizer (Adaptive Moment Estimation)\n",
    "    \n",
    "    Maintains running averages of:\n",
    "    - m: First moment (mean of gradients)\n",
    "    - v: Second moment (uncentered variance of gradients)\n",
    "    \n",
    "    Update rule:\n",
    "        m = Î²â‚m + (1-Î²â‚)g\n",
    "        v = Î²â‚‚v + (1-Î²â‚‚)gÂ²\n",
    "        mÌ‚ = m / (1-Î²â‚áµ—)  # Bias correction\n",
    "        vÌ‚ = v / (1-Î²â‚‚áµ—)\n",
    "        Î¸ = Î¸ - lr Ã— mÌ‚ / (âˆšvÌ‚ + Îµ)\n",
    "    \n",
    "    Default hyperparameters (from the paper):\n",
    "        lr = 0.001, Î²â‚ = 0.9, Î²â‚‚ = 0.999, Îµ = 1e-8\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters, lr: float = 0.001, \n",
    "                 betas: Tuple[float, float] = (0.9, 0.999), eps: float = 1e-8):\n",
    "        self.parameters = list(parameters)\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "        self.t = 0  # Time step\n",
    "        \n",
    "        # Initialize moment estimates\n",
    "        self.m = [torch.zeros_like(p) for p in self.parameters]\n",
    "        self.v = [torch.zeros_like(p) for p in self.parameters]\n",
    "    \n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        with torch.no_grad():\n",
    "            for i, param in enumerate(self.parameters):\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                g = param.grad\n",
    "                \n",
    "                # Update biased first moment estimate\n",
    "                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * g\n",
    "                \n",
    "                # Update biased second moment estimate\n",
    "                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (g ** 2)\n",
    "                \n",
    "                # Bias correction\n",
    "                m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "                v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "                \n",
    "                # Update parameters\n",
    "                param -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "\n",
    "# Compare optimizers on a simple problem\n",
    "print(\"\\nðŸ“Œ Optimizer Comparison: Minimize f(x) = xÂ²\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def run_optimization(optimizer_class, optimizer_kwargs, n_steps=50):\n",
    "    x = torch.tensor([5.0], requires_grad=True)\n",
    "    optimizer = optimizer_class([x], **optimizer_kwargs)\n",
    "    history = [x.item()]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        loss = x ** 2  # f(x) = xÂ²\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        history.append(x.item())\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Run both optimizers\n",
    "sgd_history = run_optimization(SGD, {'lr': 0.1})\n",
    "adam_history = run_optimization(Adam, {'lr': 0.5})\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(sgd_history, 'b-', label='SGD (lr=0.1)', linewidth=2)\n",
    "plt.plot(adam_history, 'r-', label='Adam (lr=0.5)', linewidth=2)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('x value')\n",
    "plt.title('Optimizer Convergence: x â†’ 0')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sgd_loss = [x**2 for x in sgd_history]\n",
    "adam_loss = [x**2 for x in adam_history]\n",
    "plt.semilogy(sgd_loss, 'b-', label='SGD', linewidth=2)\n",
    "plt.semilogy(adam_loss, 'r-', label='Adam', linewidth=2)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Loss Convergence')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Both optimizers find the minimum (x=0), but Adam adapts faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c2358",
   "metadata": {},
   "source": [
    "## 1.8 Putting It All Together: Training an MLP\n",
    "\n",
    "Now let's combine everything we've learned to train a simple Multi-Layer Perceptron (MLP) from scratch. This is the same pattern used to train transformersâ€”just with different architectures.\n",
    "\n",
    "### The Training Loop\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        # 1. Forward pass\n",
    "        predictions = model(batch.inputs)\n",
    "        loss = loss_fn(predictions, batch.targets)\n",
    "        \n",
    "        # 2. Backward pass\n",
    "        optimizer.zero_grad()  # Clear old gradients\n",
    "        loss.backward()        # Compute new gradients\n",
    "        \n",
    "        # 3. Update parameters\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "This exact pattern scales from our simple MLP to GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb24e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPLETE MLP TRAINING EXAMPLE\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING AN MLP FROM SCRATCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple classification dataset\n",
    "print(\"\\nðŸ“Œ Creating Dataset: XOR Problem\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# XOR is not linearly separable - needs hidden layer!\n",
    "X = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "y = torch.tensor([0, 1, 1, 0])  # XOR outputs\n",
    "\n",
    "print(\"XOR Truth Table:\")\n",
    "print(\"  Input  | Output\")\n",
    "print(\"  -------|-------\")\n",
    "for xi, yi in zip(X, y):\n",
    "    print(f\"  {xi.tolist()} |   {yi.item()}\")\n",
    "\n",
    "# Define MLP\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron for binary classification.\n",
    "    \n",
    "    Architecture: Input(2) -> Hidden(8) -> ReLU -> Output(2)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int = 2, hidden_dim: int = 8, output_dim: int = 2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x  # Return logits (raw scores)\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {count_parameters(model)}\")\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nðŸ“Œ Training\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    logits = model(X)\n",
    "    loss = criterion(logits, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track metrics\n",
    "    losses.append(loss.item())\n",
    "    predictions = logits.argmax(dim=1)\n",
    "    accuracy = (predictions == y).float().mean().item()\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 200 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:4d} | Loss: {loss.item():.4f} | Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "\n",
    "# Visualize training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(losses, 'b-', alpha=0.7)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(accuracies, 'g-', alpha=0.7)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training Accuracy')\n",
    "axes[1].set_ylim([0, 1.1])\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test the trained model\n",
    "print(\"\\nðŸ“Œ Testing Trained Model\")\n",
    "print(\"-\" * 50)\n",
    "with torch.no_grad():\n",
    "    logits = model(X)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    predictions = logits.argmax(dim=1)\n",
    "    \n",
    "print(\"  Input  | True | Pred | Confidence\")\n",
    "print(\"  -------|------|------|------------\")\n",
    "for xi, yi, pred, prob in zip(X, y, predictions, probs):\n",
    "    conf = prob[pred].item()\n",
    "    status = \"âœ“\" if yi == pred else \"âœ—\"\n",
    "    print(f\"  {xi.tolist()} |  {yi.item()}   |  {pred.item()}   | {conf:.2%} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944d3a78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 1: Key Takeaways\n",
    "\n",
    "### Tensors\n",
    "- Tensors are multi-dimensional arrays that support GPU acceleration and automatic differentiation\n",
    "- The key shape in transformers is `(batch, sequence, d_model)` or `(B, S, D)`\n",
    "- Master tensor operations: matrix multiplication (`@`), element-wise ops, broadcasting\n",
    "\n",
    "### Autograd\n",
    "- PyTorch builds a computational graph during forward pass\n",
    "- `.backward()` computes gradients via chain rule automatically\n",
    "- Always call `optimizer.zero_grad()` before `.backward()` to clear accumulated gradients\n",
    "\n",
    "### nn.Module\n",
    "- All neural network components inherit from `nn.Module`\n",
    "- Use `nn.Parameter` for learnable weights (automatically tracked)\n",
    "- Implement `__init__` for setup and `forward` for computation\n",
    "\n",
    "### Activation Functions\n",
    "- ReLU: Simple, fast, but can \"die\" (stuck at 0)\n",
    "- GELU: Smooth, used in BERT/GPT, allows small negative values\n",
    "- SiLU/Swish: Self-gated, used in modern LLMs like LLaMA\n",
    "- Softmax: Converts logits to probability distribution\n",
    "\n",
    "### Loss Functions\n",
    "- Cross-entropy: Standard for classification and language modeling\n",
    "- Measures how well predicted distribution matches true distribution\n",
    "- Lower loss = higher confidence in correct answer\n",
    "\n",
    "### Optimizers\n",
    "- SGD: Simple gradient descent, foundation of all optimizers\n",
    "- Adam: Adaptive learning rates + momentum, standard for transformers\n",
    "- AdamW: Adam with proper weight decay, used in most LLM training\n",
    "\n",
    "### The Training Loop\n",
    "```python\n",
    "for batch in data:\n",
    "    predictions = model(batch.inputs)      # Forward\n",
    "    loss = loss_fn(predictions, targets)   # Compute loss\n",
    "    optimizer.zero_grad()                  # Clear gradients\n",
    "    loss.backward()                        # Backward\n",
    "    optimizer.step()                       # Update weights\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [Text Representation and Embeddings](#module-2-text-representation) - How do we convert text into tensors that transformers can process?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22606469",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-2-text-representation'></a>\n",
    "# Module 2: Text Representation and Embeddings\n",
    "\n",
    "**Prerequisites:** Module 1 (PyTorch fundamentals, tensors, nn.Module)\n",
    "\n",
    "**Learning Objectives:**\n",
    "By the end of this module, you will be able to:\n",
    "- Understand different tokenization strategies and their trade-offs\n",
    "- Implement an embedding layer from scratch\n",
    "- Understand why positional encodings are necessary\n",
    "- Implement both sinusoidal and learnable positional encodings\n",
    "- Visualize and interpret embedding spaces\n",
    "\n",
    "**Why This Matters for LLMs:**\n",
    "Before a transformer can process text, it must convert discrete tokens into continuous vectors. This module covers the critical first step of any language model: turning text into numbers that neural networks can manipulate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8337f3",
   "metadata": {},
   "source": [
    "## 2.1 Tokenization: From Text to Numbers\n",
    "\n",
    "### The Tokenization Problem\n",
    "\n",
    "Neural networks operate on numbers, not text. **Tokenization** is the process of converting text into a sequence of integers (token IDs) that can be processed by a model.\n",
    "\n",
    "### Tokenization Strategies\n",
    "\n",
    "| Strategy | Unit | Vocabulary Size | Example |\n",
    "|----------|------|-----------------|---------|\n",
    "| Character-level | Single characters | ~100-300 | \"hello\" â†’ ['h','e','l','l','o'] |\n",
    "| Word-level | Whole words | 50K-200K | \"hello world\" â†’ ['hello', 'world'] |\n",
    "| Subword (BPE) | Variable chunks | 32K-100K | \"unhappiness\" â†’ ['un', 'happiness'] |\n",
    "\n",
    "### Why Subword Tokenization Won\n",
    "\n",
    "**Character-level** problems:\n",
    "- Very long sequences (expensive attention: O(nÂ²))\n",
    "- Hard to learn word meanings from individual characters\n",
    "\n",
    "**Word-level** problems:\n",
    "- Huge vocabulary (memory intensive)\n",
    "- Can't handle new/rare words (OOV problem)\n",
    "- No morphological awareness (\"run\", \"running\", \"runs\" are unrelated)\n",
    "\n",
    "**Subword** advantages:\n",
    "- Balanced sequence length\n",
    "- Manageable vocabulary size\n",
    "- Handles rare words by breaking them down\n",
    "- Captures morphology (\"un\" + \"happy\" + \"ness\")\n",
    "\n",
    "### The BPE Algorithm (Preview)\n",
    "\n",
    "Byte Pair Encoding (BPE) builds vocabulary by iteratively merging frequent character pairs:\n",
    "1. Start with character vocabulary\n",
    "2. Count all adjacent pairs\n",
    "3. Merge most frequent pair into new token\n",
    "4. Repeat until desired vocabulary size\n",
    "\n",
    "We'll implement BPE from scratch in Module 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TOKENIZATION IMPLEMENTATIONS\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"TOKENIZATION STRATEGIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class CharacterTokenizer:\n",
    "    \"\"\"\n",
    "    Character-level tokenizer.\n",
    "    \n",
    "    Pros: Small vocabulary, handles any text\n",
    "    Cons: Long sequences, hard to learn semantics\n",
    "    \n",
    "    Used by: Early models, some specialized applications\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.char_to_id = {}\n",
    "        self.id_to_char = {}\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def fit(self, text: str):\n",
    "        \"\"\"Build vocabulary from text.\"\"\"\n",
    "        chars = sorted(set(text))\n",
    "        self.char_to_id = {c: i for i, c in enumerate(chars)}\n",
    "        self.id_to_char = {i: c for c, i in self.char_to_id.items()}\n",
    "        self.vocab_size = len(chars)\n",
    "        return self\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert text to token IDs.\"\"\"\n",
    "        return [self.char_to_id[c] for c in text]\n",
    "    \n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Convert token IDs back to text.\"\"\"\n",
    "        return ''.join(self.id_to_char[i] for i in ids)\n",
    "\n",
    "class WordTokenizer:\n",
    "    \"\"\"\n",
    "    Word-level tokenizer with special tokens.\n",
    "    \n",
    "    Pros: Semantically meaningful units\n",
    "    Cons: Large vocabulary, OOV problem\n",
    "    \n",
    "    Special tokens:\n",
    "    - [PAD]: Padding for batch alignment\n",
    "    - [UNK]: Unknown/out-of-vocabulary words\n",
    "    - [CLS]: Classification token (BERT)\n",
    "    - [SEP]: Separator between sentences\n",
    "    - [MASK]: Masked token for MLM\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        self.special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def fit(self, text: str, min_freq: int = 1):\n",
    "        \"\"\"Build vocabulary from text.\"\"\"\n",
    "        # Count word frequencies\n",
    "        words = text.lower().split()\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        \n",
    "        # Build vocabulary with special tokens first\n",
    "        vocab = self.special_tokens.copy()\n",
    "        vocab.extend([w for w, c in sorted(word_counts.items()) if c >= min_freq])\n",
    "        \n",
    "        self.word_to_id = {w: i for i, w in enumerate(vocab)}\n",
    "        self.id_to_word = {i: w for w, i in self.word_to_id.items()}\n",
    "        self.vocab_size = len(vocab)\n",
    "        return self\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert text to token IDs.\"\"\"\n",
    "        words = text.lower().split()\n",
    "        unk_id = self.word_to_id['[UNK]']\n",
    "        return [self.word_to_id.get(w, unk_id) for w in words]\n",
    "    \n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Convert token IDs back to text.\"\"\"\n",
    "        return ' '.join(self.id_to_word[i] for i in ids)\n",
    "\n",
    "# Demo both tokenizers\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "print(\"\\nðŸ“Œ Character-level Tokenization\")\n",
    "print(\"-\" * 50)\n",
    "char_tok = CharacterTokenizer().fit(sample_text)\n",
    "char_ids = char_tok.encode(sample_text)\n",
    "print(f\"Text: '{sample_text}'\")\n",
    "print(f\"Vocabulary size: {char_tok.vocab_size}\")\n",
    "print(f\"Token IDs: {char_ids}\")\n",
    "print(f\"Sequence length: {len(char_ids)}\")\n",
    "print(f\"Decoded: '{char_tok.decode(char_ids)}'\")\n",
    "\n",
    "print(\"\\nðŸ“Œ Word-level Tokenization\")\n",
    "print(\"-\" * 50)\n",
    "word_tok = WordTokenizer().fit(sample_text)\n",
    "word_ids = word_tok.encode(sample_text)\n",
    "print(f\"Text: '{sample_text}'\")\n",
    "print(f\"Vocabulary size: {word_tok.vocab_size}\")\n",
    "print(f\"Token IDs: {word_ids}\")\n",
    "print(f\"Sequence length: {len(word_ids)}\")\n",
    "print(f\"Decoded: '{word_tok.decode(word_ids)}'\")\n",
    "\n",
    "# Show OOV handling\n",
    "print(\"\\nðŸ“Œ Out-of-Vocabulary (OOV) Handling\")\n",
    "print(\"-\" * 50)\n",
    "new_text = \"The quick red cat\"\n",
    "new_ids = word_tok.encode(new_text)\n",
    "print(f\"New text: '{new_text}'\")\n",
    "print(f\"Token IDs: {new_ids}\")\n",
    "print(f\"Decoded: '{word_tok.decode(new_ids)}'\")\n",
    "print(\"Note: 'red' and 'cat' become [UNK] because they weren't in training data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa037cdf",
   "metadata": {},
   "source": [
    "## 2.2 Embeddings: From Token IDs to Vectors\n",
    "\n",
    "### What is an Embedding?\n",
    "\n",
    "An **embedding** is a learned mapping from discrete tokens to continuous vectors. Each token ID becomes a dense vector of real numbers that captures semantic meaning.\n",
    "\n",
    "### The Embedding Matrix\n",
    "\n",
    "$$\\text{Embedding}(x) = W_e[x]$$\n",
    "\n",
    "Where:\n",
    "- $W_e \\in \\mathbb{R}^{V \\times D}$ is the embedding matrix\n",
    "- $V$ = vocabulary size\n",
    "- $D$ = embedding dimension (d_model)\n",
    "- $x$ = token ID (integer index)\n",
    "\n",
    "The embedding operation is simply a **table lookup**: row $x$ of the embedding matrix.\n",
    "\n",
    "### Why Embeddings Work\n",
    "\n",
    "Through training, embeddings learn to place semantically similar words close together in the vector space:\n",
    "- \"king\" - \"man\" + \"woman\" â‰ˆ \"queen\"\n",
    "- \"Paris\" - \"France\" + \"Germany\" â‰ˆ \"Berlin\"\n",
    "\n",
    "This emergent structure enables transformers to reason about relationships between concepts.\n",
    "\n",
    "### Embedding Dimension Trade-offs\n",
    "\n",
    "| d_model | Model Size | Capacity | Examples |\n",
    "|---------|------------|----------|----------|\n",
    "| 256 | Small | Limited | DistilBERT |\n",
    "| 768 | Medium | Good | BERT-base, GPT-2 |\n",
    "| 1024 | Large | Better | BERT-large |\n",
    "| 4096 | Very Large | Excellent | GPT-3, LLaMA |\n",
    "| 8192+ | Huge | Maximum | GPT-4, Claude |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EMBEDDING LAYER FROM SCRATCH\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"EMBEDDING LAYER FROM SCRATCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class EmbeddingFromScratch(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding layer: maps token IDs to dense vectors.\n",
    "    \n",
    "    Mathematically: E(x) = W_e[x] (table lookup)\n",
    "    \n",
    "    This is equivalent to one-hot encoding followed by matrix multiplication,\n",
    "    but implemented as a direct lookup for efficiency.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (V): Number of unique tokens\n",
    "        embedding_dim (D): Dimension of embedding vectors\n",
    "    \n",
    "    Shape:\n",
    "        - Input: (*, ) where * is any shape of integer indices\n",
    "        - Output: (*, D) where D is embedding_dim\n",
    "    \n",
    "    Example:\n",
    "        >>> embed = EmbeddingFromScratch(vocab_size=1000, embedding_dim=256)\n",
    "        >>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # (batch=2, seq=3)\n",
    "        >>> out = embed(x)  # (2, 3, 256)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # The embedding matrix: each row is a token's vector\n",
    "        # Initialize from normal distribution (will be learned during training)\n",
    "        self.weight = nn.Parameter(torch.randn(vocab_size, embedding_dim) * 0.02)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Look up embeddings for input token IDs.\n",
    "        \n",
    "        This is equivalent to:\n",
    "            one_hot = F.one_hot(x, self.vocab_size).float()  # (*, V)\n",
    "            return one_hot @ self.weight  # (*, V) @ (V, D) -> (*, D)\n",
    "        \n",
    "        But direct indexing is much more efficient!\n",
    "        \"\"\"\n",
    "        return self.weight[x]  # Advanced indexing: select rows\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f'vocab_size={self.vocab_size}, embedding_dim={self.embedding_dim}'\n",
    "\n",
    "# Test our implementation\n",
    "print(\"\\nðŸ“Œ Testing EmbeddingFromScratch\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "vocab_size = 100\n",
    "embedding_dim = 8\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "\n",
    "embed = EmbeddingFromScratch(vocab_size, embedding_dim)\n",
    "token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "print(f\"Vocabulary size (V): {vocab_size}\")\n",
    "print(f\"Embedding dimension (D): {embedding_dim}\")\n",
    "print(f\"Embedding matrix shape: {embed.weight.shape}\")\n",
    "print(f\"\\nInput token IDs shape: {token_ids.shape}\")\n",
    "print(f\"Input token IDs:\\n{token_ids}\")\n",
    "\n",
    "embeddings = embed(token_ids)\n",
    "print(f\"\\nOutput embeddings shape: {embeddings.shape}\")\n",
    "print(f\"First token embedding (truncated): {embeddings[0, 0, :4]}...\")\n",
    "\n",
    "# Verify against PyTorch\n",
    "print(\"\\nðŸ“Œ Verification against PyTorch nn.Embedding\")\n",
    "print(\"-\" * 50)\n",
    "pytorch_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "with torch.no_grad():\n",
    "    embed.weight.copy_(pytorch_embed.weight)\n",
    "\n",
    "our_output = embed(token_ids)\n",
    "pytorch_output = pytorch_embed(token_ids)\n",
    "\n",
    "match = torch.allclose(our_output, pytorch_output)\n",
    "print(f\"Outputs match: {match} âœ“\" if match else \"Outputs DON'T match âœ—\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c404b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EMBEDDING VISUALIZATION\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EMBEDDING SPACE VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a small vocabulary with semantic relationships\n",
    "words = ['king', 'queen', 'man', 'woman', 'prince', 'princess', \n",
    "         'dog', 'cat', 'puppy', 'kitten']\n",
    "word_to_id = {w: i for i, w in enumerate(words)}\n",
    "\n",
    "# Create embeddings with some structure (normally learned)\n",
    "# We'll manually create embeddings that show semantic relationships\n",
    "torch.manual_seed(42)\n",
    "embed_dim = 2  # 2D for visualization\n",
    "\n",
    "# Create embeddings with semantic structure\n",
    "embeddings_2d = torch.tensor([\n",
    "    [1.0, 1.0],    # king (royal, male)\n",
    "    [1.0, -1.0],   # queen (royal, female)\n",
    "    [-1.0, 1.0],   # man (common, male)\n",
    "    [-1.0, -1.0],  # woman (common, female)\n",
    "    [0.8, 0.8],    # prince (royal, male, young)\n",
    "    [0.8, -0.8],   # princess (royal, female, young)\n",
    "    [2.0, 0.5],    # dog (animal, larger)\n",
    "    [2.0, -0.5],   # cat (animal, smaller)\n",
    "    [2.2, 0.3],    # puppy (animal, larger, young)\n",
    "    [2.2, -0.3],   # kitten (animal, smaller, young)\n",
    "])\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, word in enumerate(words):\n",
    "    x, y = embeddings_2d[i]\n",
    "    plt.scatter(x, y, s=100)\n",
    "    plt.annotate(word, (x, y), fontsize=12, ha='center', va='bottom')\n",
    "\n",
    "# Draw relationship arrows\n",
    "plt.annotate('', xy=embeddings_2d[1], xytext=embeddings_2d[0],\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "plt.annotate('', xy=embeddings_2d[3], xytext=embeddings_2d[2],\n",
    "            arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
    "\n",
    "plt.xlabel('Dimension 1 (royalty â†” common / human â†” animal)', fontsize=12)\n",
    "plt.ylabel('Dimension 2 (male â†” female)', fontsize=12)\n",
    "plt.title('Word Embeddings: Semantic Relationships in Vector Space', fontsize=14)\n",
    "plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Demonstrate vector arithmetic\n",
    "print(\"\\nðŸ“Œ Word Vector Arithmetic\")\n",
    "print(\"-\" * 50)\n",
    "king = embeddings_2d[word_to_id['king']]\n",
    "man = embeddings_2d[word_to_id['man']]\n",
    "woman = embeddings_2d[word_to_id['woman']]\n",
    "\n",
    "result = king - man + woman\n",
    "print(f\"king - man + woman = {result.tolist()}\")\n",
    "print(f\"queen embedding =    {embeddings_2d[word_to_id['queen']].tolist()}\")\n",
    "print(\"\\nThe result is close to 'queen'! This is the famous word2vec analogy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25006e",
   "metadata": {},
   "source": [
    "## 2.3 Positional Encodings: Adding Position Information\n",
    "\n",
    "### The Position Problem\n",
    "\n",
    "Unlike RNNs that process tokens sequentially, transformers process all tokens in parallel. This is great for speed, but it means **the model has no inherent notion of position**.\n",
    "\n",
    "Without positional information:\n",
    "- \"The cat sat on the mat\" \n",
    "- \"mat the on sat cat The\"\n",
    "\n",
    "Would produce identical representations! We need to inject position information.\n",
    "\n",
    "### Positional Encoding Approaches\n",
    "\n",
    "| Method | Formula | Learned? | Used In |\n",
    "|--------|---------|----------|---------|\n",
    "| Sinusoidal | sin/cos functions | No | Original Transformer |\n",
    "| Learned | Embedding lookup | Yes | BERT, GPT-2 |\n",
    "| Rotary (RoPE) | Rotation matrices | No | LLaMA, GPT-NeoX |\n",
    "| ALiBi | Attention bias | No | BLOOM, MPT |\n",
    "\n",
    "### Sinusoidal Positional Encoding (Vaswani et al., 2017)\n",
    "\n",
    "From \"Attention Is All You Need\":\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $pos$ = position in sequence (0, 1, 2, ...)\n",
    "- $i$ = dimension index (0, 1, 2, ..., d_model/2)\n",
    "- $d_{model}$ = embedding dimension\n",
    "\n",
    "**Why sin/cos?**\n",
    "1. Bounded values [-1, 1] (stable training)\n",
    "2. Unique encoding for each position\n",
    "3. Can extrapolate to longer sequences than seen in training\n",
    "4. Relative positions can be computed via linear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SINUSOIDAL POSITIONAL ENCODING\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"SINUSOIDAL POSITIONAL ENCODING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding from \"Attention Is All You Need\".\n",
    "    \n",
    "    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \n",
    "    The encoding is precomputed and stored as a buffer (not a parameter).\n",
    "    \n",
    "    Args:\n",
    "        d_model: Embedding dimension\n",
    "        max_len: Maximum sequence length to precompute\n",
    "        dropout: Dropout probability\n",
    "    \n",
    "    Shape:\n",
    "        - Input: (batch, seq_len, d_model)\n",
    "        - Output: (batch, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        \n",
    "        # Compute the divisor term: 10000^(2i/d_model)\n",
    "        # Using exp and log for numerical stability:\n",
    "        # 10000^(2i/d_model) = exp(2i * log(10000) / d_model)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )  # (d_model/2,)\n",
    "        \n",
    "        # Apply sin to even indices, cos to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even dimensions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd dimensions\n",
    "        \n",
    "        # Add batch dimension and register as buffer (not a parameter)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Add positional encoding to input embeddings.\n",
    "        \n",
    "        Args:\n",
    "            x: Token embeddings (batch, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Embeddings with positional information added\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Test and visualize\n",
    "print(\"\\nðŸ“Œ Creating Sinusoidal Positional Encoding\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_model = 64\n",
    "max_len = 100\n",
    "pe_layer = SinusoidalPositionalEncoding(d_model, max_len, dropout=0.0)\n",
    "\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"max_len: {max_len}\")\n",
    "print(f\"PE matrix shape: {pe_layer.pe.shape}\")\n",
    "\n",
    "# Visualize the encoding\n",
    "pe_matrix = pe_layer.pe.squeeze(0).numpy()  # (max_len, d_model)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Full encoding heatmap\n",
    "ax = axes[0]\n",
    "im = ax.imshow(pe_matrix[:50, :32], aspect='auto', cmap='RdBu')\n",
    "ax.set_xlabel('Embedding Dimension')\n",
    "ax.set_ylabel('Position')\n",
    "ax.set_title('Sinusoidal Positional Encoding (first 50 positions, 32 dims)')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Individual dimension patterns\n",
    "ax = axes[1]\n",
    "positions = np.arange(50)\n",
    "for dim in [0, 1, 4, 5, 16, 17]:\n",
    "    ax.plot(positions, pe_matrix[:50, dim], label=f'dim {dim}', alpha=0.7)\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('Encoding Value')\n",
    "ax.set_title('Encoding Values by Dimension')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Notice: Lower dimensions have higher frequency, higher dimensions have lower frequency\")\n",
    "print(\"   This allows the model to attend to both fine-grained and coarse positions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32886290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LEARNABLE POSITIONAL ENCODING\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LEARNABLE POSITIONAL ENCODING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned positional encoding (used in BERT, GPT-2).\n",
    "    \n",
    "    Instead of fixed sin/cos patterns, we learn a separate embedding\n",
    "    for each position. This is simply another embedding layer!\n",
    "    \n",
    "    Pros:\n",
    "    - Can learn task-specific position patterns\n",
    "    - Simple implementation\n",
    "    \n",
    "    Cons:\n",
    "    - Cannot extrapolate beyond max_len seen in training\n",
    "    - Adds parameters to the model\n",
    "    \n",
    "    Args:\n",
    "        d_model: Embedding dimension\n",
    "        max_len: Maximum sequence length\n",
    "        dropout: Dropout probability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Learnable position embeddings\n",
    "        self.position_embedding = nn.Embedding(max_len, d_model)\n",
    "        \n",
    "        # Register position indices as buffer\n",
    "        positions = torch.arange(max_len)\n",
    "        self.register_buffer('positions', positions)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Add learned positional encoding to input embeddings.\n",
    "        \n",
    "        Args:\n",
    "            x: Token embeddings (batch, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Embeddings with positional information added\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        positions = self.positions[:seq_len]  # (seq_len,)\n",
    "        pos_embeddings = self.position_embedding(positions)  # (seq_len, d_model)\n",
    "        x = x + pos_embeddings  # Broadcasting: (batch, seq_len, d_model) + (seq_len, d_model)\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Compare both approaches\n",
    "print(\"\\nðŸ“Œ Comparing Positional Encoding Approaches\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_model = 64\n",
    "max_len = 100\n",
    "batch_size = 2\n",
    "seq_len = 20\n",
    "\n",
    "# Create both encodings\n",
    "sinusoidal_pe = SinusoidalPositionalEncoding(d_model, max_len, dropout=0.0)\n",
    "learned_pe = LearnedPositionalEncoding(d_model, max_len, dropout=0.0)\n",
    "\n",
    "# Create dummy embeddings\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Apply both\n",
    "sin_output = sinusoidal_pe(x)\n",
    "learned_output = learned_pe(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Sinusoidal output shape: {sin_output.shape}\")\n",
    "print(f\"Learned output shape: {learned_output.shape}\")\n",
    "\n",
    "print(f\"\\nSinusoidal PE parameters: 0 (fixed)\")\n",
    "print(f\"Learned PE parameters: {sum(p.numel() for p in learned_pe.parameters())}\")\n",
    "\n",
    "# Visualize learned embeddings (random initialization)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "sin_pe = sinusoidal_pe.pe.squeeze(0)[:50, :32].numpy()\n",
    "im = ax.imshow(sin_pe, aspect='auto', cmap='RdBu')\n",
    "ax.set_xlabel('Dimension')\n",
    "ax.set_ylabel('Position')\n",
    "ax.set_title('Sinusoidal PE (Fixed)')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "ax = axes[1]\n",
    "learned_pe_matrix = learned_pe.position_embedding.weight[:50, :32].detach().numpy()\n",
    "im = ax.imshow(learned_pe_matrix, aspect='auto', cmap='RdBu')\n",
    "ax.set_xlabel('Dimension')\n",
    "ax.set_ylabel('Position')\n",
    "ax.set_title('Learned PE (Random Init - will be trained)')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Sinusoidal: Structured patterns, extrapolates to longer sequences\")\n",
    "print(\"   Learned: Random at init, adapts during training, fixed max length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a789168",
   "metadata": {},
   "source": [
    "## 2.4 Complete Embedding Pipeline\n",
    "\n",
    "### The Full Picture\n",
    "\n",
    "In a transformer, the input goes through this pipeline:\n",
    "\n",
    "```\n",
    "Text â†’ Tokenizer â†’ Token IDs â†’ Token Embedding â†’ + Position Encoding â†’ Transformer Input\n",
    "```\n",
    "\n",
    "Let's implement the complete pipeline that converts raw text into the tensor format transformers expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d15cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPLETE EMBEDDING PIPELINE\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE EMBEDDING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete embedding layer for transformers.\n",
    "    \n",
    "    Combines:\n",
    "    1. Token embedding: token_id â†’ vector\n",
    "    2. Positional encoding: adds position information\n",
    "    3. Dropout: regularization\n",
    "    \n",
    "    This is the first layer of any transformer model.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary\n",
    "        d_model: Embedding dimension\n",
    "        max_len: Maximum sequence length\n",
    "        dropout: Dropout probability\n",
    "        use_learned_pe: If True, use learned positional encoding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int, max_len: int = 512,\n",
    "                 dropout: float = 0.1, use_learned_pe: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        if use_learned_pe:\n",
    "            self.pos_encoding = LearnedPositionalEncoding(d_model, max_len, dropout)\n",
    "        else:\n",
    "            self.pos_encoding = SinusoidalPositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Scale factor (from original paper)\n",
    "        # Embeddings are scaled by sqrt(d_model) to maintain variance\n",
    "        self.scale = math.sqrt(d_model)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert token IDs to transformer input.\n",
    "        \n",
    "        Args:\n",
    "            x: Token IDs (batch, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            Embeddings with position info (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Token embedding with scaling\n",
    "        # Scaling helps maintain gradient magnitude during training\n",
    "        embeddings = self.token_embedding(x) * self.scale\n",
    "        \n",
    "        # Add positional encoding\n",
    "        return self.pos_encoding(embeddings)\n",
    "\n",
    "# Demo the complete pipeline\n",
    "print(\"\\nðŸ“Œ Complete Embedding Pipeline Demo\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Configuration (typical small model)\n",
    "vocab_size = 1000\n",
    "d_model = 64\n",
    "max_len = 128\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# Create embedding layer\n",
    "embedding_layer = TransformerEmbedding(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    max_len=max_len,\n",
    "    dropout=0.1,\n",
    "    use_learned_pe=False  # Using sinusoidal\n",
    ")\n",
    "\n",
    "# Simulate tokenized input\n",
    "token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  vocab_size: {vocab_size}\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  max_len: {max_len}\")\n",
    "print(f\"\\nInput token IDs shape: {token_ids.shape}\")\n",
    "print(f\"Input token IDs (first sequence): {token_ids[0].tolist()}\")\n",
    "\n",
    "# Forward pass\n",
    "transformer_input = embedding_layer(token_ids)\n",
    "\n",
    "print(f\"\\nOutput shape: {transformer_input.shape}\")\n",
    "print(f\"Output dtype: {transformer_input.dtype}\")\n",
    "print(f\"\\nThis tensor is now ready to be fed into transformer layers!\")\n",
    "\n",
    "# Parameter count\n",
    "total_params = sum(p.numel() for p in embedding_layer.parameters())\n",
    "print(f\"\\nTotal embedding parameters: {total_params:,}\")\n",
    "print(f\"  Token embedding: {vocab_size * d_model:,} ({vocab_size} Ã— {d_model})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20824c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 2: Key Takeaways\n",
    "\n",
    "### Tokenization\n",
    "- **Character-level**: Small vocab, long sequences, hard to learn semantics\n",
    "- **Word-level**: Large vocab, OOV problems, no morphology\n",
    "- **Subword (BPE)**: Best of both worlds, used by all modern LLMs\n",
    "- Special tokens: [PAD], [UNK], [CLS], [SEP], [MASK] serve specific purposes\n",
    "\n",
    "### Embeddings\n",
    "- Embedding = learned lookup table mapping token IDs to vectors\n",
    "- Embedding matrix shape: (vocab_size, d_model)\n",
    "- Semantically similar words cluster together in embedding space\n",
    "- Famous property: king - man + woman â‰ˆ queen\n",
    "\n",
    "### Positional Encoding\n",
    "- Transformers have no inherent position awareness (parallel processing)\n",
    "- Must explicitly add position information\n",
    "\n",
    "**Sinusoidal (Original Transformer)**:\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "- Fixed, no parameters, can extrapolate to longer sequences\n",
    "\n",
    "**Learned (BERT, GPT-2)**:\n",
    "- Separate embedding for each position\n",
    "- Adapts to task, but fixed max length\n",
    "\n",
    "### The Complete Pipeline\n",
    "```\n",
    "Text â†’ Tokenize â†’ Token IDs â†’ Embed â†’ Scale by âˆšd_model â†’ Add Position â†’ Dropout â†’ Transformer\n",
    "```\n",
    "\n",
    "### Embedding Scaling\n",
    "- Embeddings are scaled by $\\sqrt{d_{model}}$ to maintain variance\n",
    "- This helps with gradient flow during training\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [Core Attention Mechanisms](#module-3-attention-mechanisms) - The heart of the transformer architecture\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f00d813",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-3-attention-mechanisms'></a>\n",
    "# Module 3: Core Attention Mechanisms\n",
    "\n",
    "**Prerequisites:** Module 1-2 (PyTorch, embeddings, positional encoding)\n",
    "\n",
    "**Learning Objectives:**\n",
    "By the end of this module, you will be able to:\n",
    "- Understand the intuition behind attention mechanisms\n",
    "- Implement scaled dot-product attention from scratch\n",
    "- Implement multi-head attention with proper head splitting\n",
    "- Visualize and interpret attention patterns\n",
    "- Implement causal masking for autoregressive models\n",
    "\n",
    "**Why This Matters:**\n",
    "Attention is THE breakthrough that enabled transformers. It allows the model to dynamically focus on relevant parts of the input, regardless of distance. This module is the heart of everything that follows.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe72db7",
   "metadata": {},
   "source": [
    "## 3.1 The Attention Mechanism: Intuition\n",
    "\n",
    "### What Problem Does Attention Solve?\n",
    "\n",
    "Before attention, sequence models (RNNs, LSTMs) had a fundamental limitation: information had to flow sequentially through the network. To connect the first and last words of a long sentence, information had to pass through every intermediate step, leading to:\n",
    "\n",
    "1. **Vanishing gradients**: Hard to learn long-range dependencies\n",
    "2. **Sequential bottleneck**: Can't parallelize computation\n",
    "3. **Fixed context**: Each position has the same \"view\" of context\n",
    "\n",
    "### The Attention Solution\n",
    "\n",
    "Attention allows **direct connections** between any two positions:\n",
    "\n",
    "```\n",
    "\"The cat sat on the mat because it was tired\"\n",
    "                                    â†‘\n",
    "                                   \"it\" attends directly to \"cat\"\n",
    "```\n",
    "\n",
    "Instead of passing information step-by-step, attention computes a weighted sum of all positions, where weights indicate relevance.\n",
    "\n",
    "### Query, Key, Value: The Database Analogy\n",
    "\n",
    "Think of attention like a database query:\n",
    "\n",
    "| Concept | Database | Attention |\n",
    "|---------|----------|-----------|\n",
    "| **Query (Q)** | What you're searching for | Current token asking \"what should I attend to?\" |\n",
    "| **Key (K)** | Index/tags of stored items | Each token's \"identifier\" for matching |\n",
    "| **Value (V)** | Actual stored content | Information to retrieve if matched |\n",
    "\n",
    "The attention process:\n",
    "1. Compare Query with all Keys (compute similarity scores)\n",
    "2. Convert scores to weights (softmax)\n",
    "3. Weighted sum of Values\n",
    "\n",
    "### The Attention Formula (Vaswani et al., 2017)\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q \\in \\mathbb{R}^{n \\times d_k}$ = Query matrix\n",
    "- $K \\in \\mathbb{R}^{m \\times d_k}$ = Key matrix  \n",
    "- $V \\in \\mathbb{R}^{m \\times d_v}$ = Value matrix\n",
    "- $d_k$ = Key/Query dimension\n",
    "- $\\sqrt{d_k}$ = Scaling factor (prevents softmax saturation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc9fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ATTENTION STEP BY STEP\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"ATTENTION MECHANISM - STEP BY STEP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple example: 4 tokens, dimension 3\n",
    "seq_len = 4\n",
    "d_k = 3\n",
    "\n",
    "# Create Q, K, V (normally these come from linear projections)\n",
    "torch.manual_seed(42)\n",
    "Q = torch.randn(seq_len, d_k)\n",
    "K = torch.randn(seq_len, d_k)\n",
    "V = torch.randn(seq_len, d_k)\n",
    "\n",
    "print(\"\\nðŸ“Œ Step 1: Query, Key, Value matrices\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Q (Query) shape: {Q.shape}\")\n",
    "print(f\"K (Key) shape: {K.shape}\")\n",
    "print(f\"V (Value) shape: {V.shape}\")\n",
    "\n",
    "print(\"\\nðŸ“Œ Step 2: Compute attention scores (Q @ K^T)\")\n",
    "print(\"-\" * 50)\n",
    "# Each query attends to all keys\n",
    "scores = Q @ K.T  # (seq_len, seq_len)\n",
    "print(f\"Scores shape: {scores.shape}\")\n",
    "print(f\"Scores matrix (how much each query matches each key):\")\n",
    "print(scores.round(decimals=2))\n",
    "\n",
    "print(\"\\nðŸ“Œ Step 3: Scale by sqrt(d_k)\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"d_k = {d_k}, sqrt(d_k) = {math.sqrt(d_k):.3f}\")\n",
    "scaled_scores = scores / math.sqrt(d_k)\n",
    "print(f\"Scaled scores:\")\n",
    "print(scaled_scores.round(decimals=2))\n",
    "print(\"\\nWhy scale? Large dot products â†’ extreme softmax â†’ vanishing gradients\")\n",
    "\n",
    "print(\"\\nðŸ“Œ Step 4: Apply softmax (convert to probabilities)\")\n",
    "print(\"-\" * 50)\n",
    "attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "print(f\"Attention weights (each row sums to 1):\")\n",
    "print(attention_weights.round(decimals=2))\n",
    "print(f\"Row sums: {attention_weights.sum(dim=-1).tolist()}\")\n",
    "\n",
    "print(\"\\nðŸ“Œ Step 5: Weighted sum of values\")\n",
    "print(\"-\" * 50)\n",
    "output = attention_weights @ V  # (seq_len, d_k)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output (each row is a weighted combination of V rows):\")\n",
    "print(output.round(decimals=2))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "im = ax.imshow(scores.numpy(), cmap='RdBu', aspect='auto')\n",
    "ax.set_title('Raw Scores (Q @ K^T)')\n",
    "ax.set_xlabel('Key position')\n",
    "ax.set_ylabel('Query position')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "ax = axes[1]\n",
    "im = ax.imshow(scaled_scores.numpy(), cmap='RdBu', aspect='auto')\n",
    "ax.set_title(f'Scaled Scores (Ã· âˆš{d_k})')\n",
    "ax.set_xlabel('Key position')\n",
    "ax.set_ylabel('Query position')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "ax = axes[2]\n",
    "im = ax.imshow(attention_weights.numpy(), cmap='Blues', aspect='auto')\n",
    "ax.set_title('Attention Weights (softmax)')\n",
    "ax.set_xlabel('Key position')\n",
    "ax.set_ylabel('Query position')\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        ax.text(j, i, f'{attention_weights[i,j]:.2f}', ha='center', va='center', fontsize=9)\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b81b13f",
   "metadata": {},
   "source": [
    "## 3.2 Scaled Dot-Product Attention Implementation\n",
    "\n",
    "### The Complete Formula\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### Why Scale by $\\sqrt{d_k}$?\n",
    "\n",
    "From the original paper: \"We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.\"\n",
    "\n",
    "**Mathematical intuition**: If Q and K have components with mean 0 and variance 1, then $Q \\cdot K$ has variance $d_k$. Dividing by $\\sqrt{d_k}$ normalizes this back to variance 1.\n",
    "\n",
    "### Handling Masks\n",
    "\n",
    "We often need to prevent attention to certain positions:\n",
    "- **Padding mask**: Don't attend to padding tokens\n",
    "- **Causal mask**: Don't attend to future tokens (for autoregressive models)\n",
    "\n",
    "Masking is done by setting scores to $-\\infty$ before softmax, so those positions get weight 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1894154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SCALED DOT-PRODUCT ATTENTION FROM SCRATCH\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"SCALED DOT-PRODUCT ATTENTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    "    dropout: float = 0.0,\n",
    "    training: bool = True\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention from \"Attention Is All You Need\".\n",
    "    \n",
    "    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
    "    \n",
    "    Args:\n",
    "        query: Query tensor (..., seq_len_q, d_k)\n",
    "        key: Key tensor (..., seq_len_k, d_k)\n",
    "        value: Value tensor (..., seq_len_k, d_v)\n",
    "        mask: Optional mask tensor, 1 = keep, 0 = mask out\n",
    "        dropout: Dropout probability on attention weights\n",
    "        training: Whether in training mode (for dropout)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (..., seq_len_q, d_v)\n",
    "        attention_weights: Attention weights (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    Shape notation:\n",
    "        ... = batch dimensions (can be multiple, e.g., batch, heads)\n",
    "        seq_len_q = query sequence length\n",
    "        seq_len_k = key/value sequence length (can differ from q in cross-attention)\n",
    "        d_k = key/query dimension\n",
    "        d_v = value dimension\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    # Step 1: Compute attention scores\n",
    "    # (..., seq_len_q, d_k) @ (..., d_k, seq_len_k) -> (..., seq_len_q, seq_len_k)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        # Mask positions with -inf so softmax gives 0\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Step 4: Softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Handle case where entire row is masked (all -inf -> nan after softmax)\n",
    "    attention_weights = attention_weights.nan_to_num(0.0)\n",
    "    \n",
    "    # Step 5: Apply dropout (during training)\n",
    "    if dropout > 0 and training:\n",
    "        attention_weights = F.dropout(attention_weights, p=dropout, training=training)\n",
    "    \n",
    "    # Step 6: Weighted sum of values\n",
    "    # (..., seq_len_q, seq_len_k) @ (..., seq_len_k, d_v) -> (..., seq_len_q, d_v)\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test the implementation\n",
    "print(\"\\nðŸ“Œ Testing Scaled Dot-Product Attention\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Query shape: {Q.shape}\")\n",
    "print(f\"Key shape: {K.shape}\")\n",
    "print(f\"Value shape: {V.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "\n",
    "# Verify against PyTorch's implementation\n",
    "pytorch_output = F.scaled_dot_product_attention(Q, K, V)\n",
    "match = torch.allclose(output, pytorch_output, atol=1e-5)\n",
    "print(f\"\\nMatches PyTorch F.scaled_dot_product_attention: {match} âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66ee22",
   "metadata": {},
   "source": [
    "## 3.3 Multi-Head Attention\n",
    "\n",
    "### Why Multiple Heads?\n",
    "\n",
    "Single attention can only focus on one type of relationship at a time. **Multi-head attention** runs multiple attention operations in parallel, each learning different patterns:\n",
    "\n",
    "- Head 1 might learn syntactic relationships (subject-verb)\n",
    "- Head 2 might learn semantic relationships (synonyms)\n",
    "- Head 3 might learn positional patterns (adjacent words)\n",
    "\n",
    "### The Multi-Head Formula (Vaswani et al., 2017)\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "Where each head is:\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "### Dimension Management\n",
    "\n",
    "For efficiency, we don't run $h$ separate attention operations. Instead:\n",
    "\n",
    "1. Project Q, K, V to full dimension: $(B, S, d_{model})$\n",
    "2. Reshape to separate heads: $(B, S, d_{model}) \\rightarrow (B, h, S, d_k)$ where $d_k = d_{model}/h$\n",
    "3. Run attention in parallel across heads\n",
    "4. Reshape back and project: $(B, h, S, d_k) \\rightarrow (B, S, d_{model})$\n",
    "\n",
    "### Parameter Count\n",
    "\n",
    "- $W^Q, W^K, W^V$: Each is $(d_{model}, d_{model})$ = $3 \\times d_{model}^2$\n",
    "- $W^O$: $(d_{model}, d_{model})$ = $d_{model}^2$\n",
    "- **Total**: $4 \\times d_{model}^2$ parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96987554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MULTI-HEAD ATTENTION FROM SCRATCH\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"MULTI-HEAD ATTENTION FROM SCRATCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention from \"Attention Is All You Need\".\n",
    "    \n",
    "    MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O\n",
    "    where head_i = Attention(Q W_i^Q, K W_i^K, V W_i^V)\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension (embedding size)\n",
    "        n_heads: Number of attention heads\n",
    "        dropout: Dropout probability\n",
    "    \n",
    "    Shape:\n",
    "        - Input: query, key, value each (batch, seq_len, d_model)\n",
    "        - Output: (batch, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads  # Dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        # Each projects from d_model to d_model (all heads combined)\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Store attention weights for visualization\n",
    "        self.attention_weights = None\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "            query: (batch, seq_len_q, d_model)\n",
    "            key: (batch, seq_len_k, d_model)\n",
    "            value: (batch, seq_len_k, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len_q, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Step 1: Linear projections\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # Step 2: Reshape to (batch, n_heads, seq_len, d_k)\n",
    "        # This splits d_model into n_heads Ã— d_k\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Step 3: Scaled dot-product attention for all heads in parallel\n",
    "        # Q, K, V: (batch, n_heads, seq_len, d_k)\n",
    "        attn_output, self.attention_weights = scaled_dot_product_attention(\n",
    "            Q, K, V, mask=mask, dropout=self.dropout.p, training=self.training\n",
    "        )\n",
    "        \n",
    "        # Step 4: Concatenate heads\n",
    "        # (batch, n_heads, seq_len, d_k) -> (batch, seq_len, n_heads, d_k) -> (batch, seq_len, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Step 5: Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test the implementation\n",
    "print(\"\\nðŸ“Œ Testing Multi-Head Attention\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_model = 64\n",
    "n_heads = 8\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "mha = MultiHeadAttention(d_model, n_heads, dropout=0.0)\n",
    "\n",
    "# Self-attention: Q = K = V\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = mha(x, x, x)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  n_heads: {n_heads}\")\n",
    "print(f\"  d_k (per head): {d_model // n_heads}\")\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {mha.attention_weights.shape}\")\n",
    "\n",
    "# Parameter count\n",
    "total_params = sum(p.numel() for p in mha.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"  W_q, W_k, W_v: 3 Ã— {d_model}Â² = {3 * d_model**2:,}\")\n",
    "print(f\"  W_o: {d_model}Â² = {d_model**2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f2dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZE MULTI-HEAD ATTENTION PATTERNS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ATTENTION PATTERN VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple example with interpretable patterns\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]\n",
    "seq_len = len(tokens)\n",
    "d_model = 32\n",
    "n_heads = 4\n",
    "\n",
    "# Create MHA and run on random embeddings\n",
    "mha = MultiHeadAttention(d_model, n_heads, dropout=0.0)\n",
    "x = torch.randn(1, seq_len, d_model)\n",
    "_ = mha(x, x, x)\n",
    "\n",
    "# Get attention weights: (1, n_heads, seq_len, seq_len)\n",
    "attn = mha.attention_weights[0].detach()  # (n_heads, seq_len, seq_len)\n",
    "\n",
    "# Visualize each head\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head_idx in range(n_heads):\n",
    "    ax = axes[head_idx]\n",
    "    weights = attn[head_idx].numpy()\n",
    "    \n",
    "    im = ax.imshow(weights, cmap='Blues', aspect='auto')\n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(tokens)\n",
    "    ax.set_xlabel('Key (attending to)')\n",
    "    ax.set_ylabel('Query (from)')\n",
    "    ax.set_title(f'Head {head_idx + 1}')\n",
    "    \n",
    "    # Add values\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            ax.text(j, i, f'{weights[i,j]:.2f}', ha='center', va='center', \n",
    "                   fontsize=7, color='white' if weights[i,j] > 0.5 else 'black')\n",
    "\n",
    "plt.suptitle('Multi-Head Attention Patterns (Random Init)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Each head learns different attention patterns!\")\n",
    "print(\"   After training, heads specialize in different linguistic relationships.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a600571e",
   "metadata": {},
   "source": [
    "## 3.4 Self-Attention vs Cross-Attention\n",
    "\n",
    "### Self-Attention\n",
    "\n",
    "In **self-attention**, Q, K, and V all come from the same sequence:\n",
    "\n",
    "```python\n",
    "output = attention(x, x, x)  # Q=K=V=x\n",
    "```\n",
    "\n",
    "Used in:\n",
    "- Encoder layers (BERT)\n",
    "- Decoder self-attention (GPT)\n",
    "\n",
    "Each token attends to all other tokens in the same sequence.\n",
    "\n",
    "### Cross-Attention\n",
    "\n",
    "In **cross-attention**, Q comes from one sequence, K and V from another:\n",
    "\n",
    "```python\n",
    "output = attention(decoder_state, encoder_output, encoder_output)\n",
    "```\n",
    "\n",
    "Used in:\n",
    "- Encoder-decoder models (original Transformer, T5)\n",
    "- Decoder attending to encoder output\n",
    "\n",
    "The decoder \"queries\" the encoder to find relevant information.\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| Aspect | Self-Attention | Cross-Attention |\n",
    "|--------|---------------|-----------------|\n",
    "| Q source | Same sequence | Target sequence |\n",
    "| K, V source | Same sequence | Source sequence |\n",
    "| Purpose | Model relationships within sequence | Connect two sequences |\n",
    "| Example | \"The cat sat\" understanding itself | Translation: English â†’ French |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca72c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SELF-ATTENTION VS CROSS-ATTENTION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"SELF-ATTENTION VS CROSS-ATTENTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "d_model = 32\n",
    "n_heads = 4\n",
    "batch_size = 1\n",
    "\n",
    "# Create attention layer\n",
    "attn = MultiHeadAttention(d_model, n_heads, dropout=0.0)\n",
    "\n",
    "# Self-attention example\n",
    "print(\"\\nðŸ“Œ Self-Attention\")\n",
    "print(\"-\" * 50)\n",
    "encoder_seq = torch.randn(batch_size, 5, d_model)  # 5 tokens\n",
    "self_attn_output = attn(encoder_seq, encoder_seq, encoder_seq)\n",
    "print(f\"Input sequence shape: {encoder_seq.shape}\")\n",
    "print(f\"Q = K = V (same sequence)\")\n",
    "print(f\"Output shape: {self_attn_output.shape}\")\n",
    "print(f\"Attention matrix shape: {attn.attention_weights.shape[2:]} (5Ã—5)\")\n",
    "\n",
    "# Cross-attention example\n",
    "print(\"\\nðŸ“Œ Cross-Attention\")\n",
    "print(\"-\" * 50)\n",
    "decoder_seq = torch.randn(batch_size, 3, d_model)  # 3 tokens (target)\n",
    "encoder_output = torch.randn(batch_size, 5, d_model)  # 5 tokens (source)\n",
    "\n",
    "cross_attn_output = attn(decoder_seq, encoder_output, encoder_output)\n",
    "print(f\"Query (decoder) shape: {decoder_seq.shape}\")\n",
    "print(f\"Key/Value (encoder) shape: {encoder_output.shape}\")\n",
    "print(f\"Output shape: {cross_attn_output.shape}\")\n",
    "print(f\"Attention matrix shape: {attn.attention_weights.shape[2:]} (3Ã—5)\")\n",
    "print(\"\\nNote: Each decoder token (3) attends to all encoder tokens (5)\")\n",
    "\n",
    "# Visualize the difference\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Self-attention\n",
    "_ = attn(encoder_seq, encoder_seq, encoder_seq)\n",
    "self_weights = attn.attention_weights[0, 0].detach().numpy()\n",
    "ax = axes[0]\n",
    "im = ax.imshow(self_weights, cmap='Blues', aspect='auto')\n",
    "ax.set_title('Self-Attention (5Ã—5)')\n",
    "ax.set_xlabel('Key position')\n",
    "ax.set_ylabel('Query position')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Cross-attention\n",
    "_ = attn(decoder_seq, encoder_output, encoder_output)\n",
    "cross_weights = attn.attention_weights[0, 0].detach().numpy()\n",
    "ax = axes[1]\n",
    "im = ax.imshow(cross_weights, cmap='Blues', aspect='auto')\n",
    "ax.set_title('Cross-Attention (3Ã—5)')\n",
    "ax.set_xlabel('Encoder position (Key)')\n",
    "ax.set_ylabel('Decoder position (Query)')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236c47d5",
   "metadata": {},
   "source": [
    "## 3.5 Causal Masking for Autoregressive Models\n",
    "\n",
    "### The Autoregressive Constraint\n",
    "\n",
    "In language models like GPT, we predict the next token based only on previous tokens:\n",
    "\n",
    "$$P(x_t | x_1, x_2, ..., x_{t-1})$$\n",
    "\n",
    "During training, we process entire sequences at once for efficiency, but each position should only \"see\" previous positions. This is enforced with a **causal mask**.\n",
    "\n",
    "### The Causal Mask\n",
    "\n",
    "A causal (or \"look-ahead\") mask is a lower triangular matrix:\n",
    "\n",
    "```\n",
    "Position:  0  1  2  3\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    0   â”‚ 1  0  0  0  â”‚  Token 0 sees only itself\n",
    "    1   â”‚ 1  1  0  0  â”‚  Token 1 sees 0, 1\n",
    "    2   â”‚ 1  1  1  0  â”‚  Token 2 sees 0, 1, 2\n",
    "    3   â”‚ 1  1  1  1  â”‚  Token 3 sees all\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Positions with 0 are set to $-\\infty$ before softmax, giving attention weight 0.\n",
    "\n",
    "### Why Causal Masking Matters\n",
    "\n",
    "Without causal masking:\n",
    "- Model could \"cheat\" by looking at future tokens\n",
    "- Would learn to copy rather than predict\n",
    "- Wouldn't work for generation (no future tokens exist)\n",
    "\n",
    "With causal masking:\n",
    "- Each position learns to predict from context only\n",
    "- Same model works for training and generation\n",
    "- Enables parallel training of autoregressive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19de3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CAUSAL MASKING IMPLEMENTATION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"CAUSAL MASKING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_causal_mask(seq_len: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create a causal (look-ahead) mask for autoregressive attention.\n",
    "    \n",
    "    Returns a lower triangular matrix where:\n",
    "    - 1 = can attend (current and past positions)\n",
    "    - 0 = cannot attend (future positions)\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "    \n",
    "    Returns:\n",
    "        mask: (seq_len, seq_len) boolean tensor\n",
    "    \"\"\"\n",
    "    # torch.tril creates lower triangular matrix\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask\n",
    "\n",
    "# Visualize causal mask\n",
    "print(\"\\nðŸ“Œ Causal Mask Structure\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "seq_len = 6\n",
    "mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(f\"Causal mask for sequence length {seq_len}:\")\n",
    "print(mask.int())\n",
    "print(\"\\n1 = can attend, 0 = masked (cannot attend)\")\n",
    "\n",
    "# Show effect on attention\n",
    "print(\"\\nðŸ“Œ Effect of Causal Mask on Attention\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_k = 8\n",
    "Q = torch.randn(1, seq_len, d_k)\n",
    "K = torch.randn(1, seq_len, d_k)\n",
    "V = torch.randn(1, seq_len, d_k)\n",
    "\n",
    "# Without mask\n",
    "output_no_mask, weights_no_mask = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# With causal mask\n",
    "output_masked, weights_masked = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "im = ax.imshow(mask.numpy(), cmap='Greens', aspect='auto')\n",
    "ax.set_title('Causal Mask')\n",
    "ax.set_xlabel('Key position')\n",
    "ax.set_ylabel('Query position')\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        ax.text(j, i, int(mask[i,j].item()), ha='center', va='center')\n",
    "\n",
    "ax = axes[1]\n",
    "im = ax.imshow(weights_no_mask[0].numpy(), cmap='Blues', aspect='auto')\n",
    "ax.set_title('Attention WITHOUT Mask')\n",
    "ax.set_xlabel('Key position')\n",
    "ax.set_ylabel('Query position')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "ax = axes[2]\n",
    "im = ax.imshow(weights_masked[0].numpy(), cmap='Blues', aspect='auto')\n",
    "ax.set_title('Attention WITH Causal Mask')\n",
    "ax.set_xlabel('Key position')\n",
    "ax.set_ylabel('Query position')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… With causal mask, each position can only attend to itself and earlier positions!\")\n",
    "print(\"   This is essential for autoregressive language models like GPT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d46f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MASKED MULTI-HEAD ATTENTION\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MASKED MULTI-HEAD ATTENTION (GPT-style)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class CausalMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention with built-in causal masking.\n",
    "    \n",
    "    This is the attention used in decoder-only models like GPT.\n",
    "    Each position can only attend to previous positions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, max_len: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "        # Pre-compute causal mask for efficiency\n",
    "        # Register as buffer (not a parameter)\n",
    "        mask = create_causal_mask(max_len)\n",
    "        self.register_buffer('causal_mask', mask)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with automatic causal masking.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        # Get the appropriate slice of the pre-computed mask\n",
    "        mask = self.causal_mask[:seq_len, :seq_len]\n",
    "        return self.mha(x, x, x, mask=mask)\n",
    "\n",
    "# Test causal MHA\n",
    "d_model = 64\n",
    "n_heads = 8\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "causal_mha = CausalMultiHeadAttention(d_model, n_heads, max_len=100)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = causal_mha(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nThis is the attention layer used in GPT-style models!\")\n",
    "\n",
    "# Verify masking is working\n",
    "attn_weights = causal_mha.mha.attention_weights[0, 0].detach()\n",
    "print(f\"\\nAttention weights (first head, first batch):\")\n",
    "print(f\"Upper triangle should be ~0 (masked):\")\n",
    "upper_triangle = torch.triu(attn_weights, diagonal=1)\n",
    "print(f\"Max value in upper triangle: {upper_triangle.max().item():.6f}\")\n",
    "print(f\"(Should be ~0, confirming future positions are masked)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3242ec9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 3: Key Takeaways\n",
    "\n",
    "### The Attention Formula\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "- **Q (Query)**: What am I looking for?\n",
    "- **K (Key)**: What do I contain?\n",
    "- **V (Value)**: What information do I provide?\n",
    "- **Scaling by $\\sqrt{d_k}$**: Prevents softmax saturation for large dimensions\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "- Multiple heads learn different attention patterns in parallel\n",
    "- Each head has dimension $d_k = d_{model} / h$\n",
    "- Total parameters: $4 \\times d_{model}^2$\n",
    "\n",
    "### Self-Attention vs Cross-Attention\n",
    "\n",
    "| Type | Q, K, V Source | Use Case |\n",
    "|------|---------------|----------|\n",
    "| Self-Attention | Same sequence | Encoder, decoder self-attention |\n",
    "| Cross-Attention | Q: target, K/V: source | Encoder-decoder connection |\n",
    "\n",
    "### Causal Masking\n",
    "\n",
    "- Essential for autoregressive models (GPT)\n",
    "- Lower triangular mask: each position sees only past\n",
    "- Implemented by setting future positions to $-\\infty$ before softmax\n",
    "\n",
    "### Attention Complexity\n",
    "\n",
    "- Time: $O(n^2 \\cdot d)$ where $n$ = sequence length\n",
    "- Memory: $O(n^2)$ for attention weights\n",
    "- This quadratic scaling is why long sequences are challenging!\n",
    "\n",
    "### Key Implementation Details\n",
    "\n",
    "1. **Reshape for heads**: $(B, S, d_{model}) \\rightarrow (B, h, S, d_k)$\n",
    "2. **Parallel attention**: All heads computed simultaneously\n",
    "3. **Concatenate and project**: $(B, h, S, d_k) \\rightarrow (B, S, d_{model})$\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [Transformer Architecture from Scratch](#module-4-transformer-architecture) - Putting it all together\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f2010c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-4-transformer-architecture'></a>\n",
    "# Module 4: Transformer Architecture from Scratch\n",
    "\n",
    "**Prerequisites:** Modules 1-3 (PyTorch, embeddings, attention mechanisms)\n",
    "\n",
    "**Learning Objectives:**\n",
    "By the end of this module, you will be able to:\n",
    "- Implement all transformer sub-components from scratch\n",
    "- Understand the role of each component in the architecture\n",
    "- Build complete encoder and decoder blocks\n",
    "- Assemble a full encoder-decoder transformer\n",
    "- Train a transformer on a simple sequence-to-sequence task\n",
    "\n",
    "**The Original Transformer (Vaswani et al., 2017)**\n",
    "\n",
    "This module implements the architecture from \"Attention Is All You Need\" - the paper that started the LLM revolution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e998ce3d",
   "metadata": {},
   "source": [
    "## 4.1 Position-wise Feed-Forward Network\n",
    "\n",
    "### The FFN Formula (Vaswani et al., 2017)\n",
    "\n",
    "$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$\n",
    "\n",
    "Or more generally with any activation:\n",
    "\n",
    "$\\text{FFN}(x) = \\text{Activation}(xW_1 + b_1)W_2 + b_2$\n",
    "\n",
    "### Architecture Details\n",
    "\n",
    "The FFN is applied **identically** to each position (hence \"position-wise\"):\n",
    "- Input: $(\\text{batch}, \\text{seq\\_len}, d_{model})$\n",
    "- Hidden: $(\\text{batch}, \\text{seq\\_len}, d_{ff})$ where typically $d_{ff} = 4 \\times d_{model}$\n",
    "- Output: $(\\text{batch}, \\text{seq\\_len}, d_{model})$\n",
    "\n",
    "### Why This Expansion?\n",
    "\n",
    "The FFN provides:\n",
    "1. **Non-linearity**: Attention is linear in V; FFN adds non-linear transformations\n",
    "2. **Capacity**: The 4x expansion increases model capacity\n",
    "3. **Per-position processing**: Each token gets independent transformation\n",
    "\n",
    "### Parameter Count\n",
    "\n",
    "- $W_1$: $(d_{model}, d_{ff})$ = $d_{model} \\times 4d_{model}$\n",
    "- $W_2$: $(d_{ff}, d_{model})$ = $4d_{model} \\times d_{model}$\n",
    "- **Total**: $8 \\times d_{model}^2$ (2x the attention parameters!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e92db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# POSITION-WISE FEED-FORWARD NETWORK\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"POSITION-WISE FEED-FORWARD NETWORK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class PositionwiseFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network from \"Attention Is All You Need\".\n",
    "    \n",
    "    FFN(x) = Activation(x @ W1 + b1) @ W2 + b2\n",
    "    \n",
    "    Applied identically to each position in the sequence.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        d_ff: Feed-forward hidden dimension (typically 4 * d_model)\n",
    "        dropout: Dropout probability\n",
    "        activation: Activation function ('relu' or 'gelu')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model: int, \n",
    "        d_ff: int = None, \n",
    "        dropout: float = 0.1,\n",
    "        activation: str = 'relu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        # -> (batch, seq_len, d_ff)\n",
    "        # -> (batch, seq_len, d_model)\n",
    "        \n",
    "        x = self.linear1(x)      # Expand: d_model -> d_ff\n",
    "        x = self.activation(x)    # Non-linearity\n",
    "        x = self.dropout(x)       # Regularization\n",
    "        x = self.linear2(x)       # Contract: d_ff -> d_model\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test FFN\n",
    "print(\"\\nðŸ“Œ Testing Position-wise FFN\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_model = 64\n",
    "d_ff = 256  # 4x expansion\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "ffn = PositionwiseFFN(d_model, d_ff, dropout=0.0)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = ffn(x)\n",
    "\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"d_ff: {d_ff} (4x expansion)\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Parameter count\n",
    "total_params = sum(p.numel() for p in ffn.parameters())\n",
    "print(f\"\\nParameter count:\")\n",
    "print(f\"  W1: {d_model} Ã— {d_ff} = {d_model * d_ff:,}\")\n",
    "print(f\"  b1: {d_ff:,}\")\n",
    "print(f\"  W2: {d_ff} Ã— {d_model} = {d_ff * d_model:,}\")\n",
    "print(f\"  b2: {d_model:,}\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "\n",
    "# Verify position-wise independence\n",
    "print(\"\\nðŸ“Œ Verifying Position-wise Independence\")\n",
    "print(\"-\" * 50)\n",
    "# Each position should be processed independently\n",
    "x_single = x[:, 0:1, :]  # Just first position\n",
    "output_single = ffn(x_single)\n",
    "output_first = output[:, 0:1, :]\n",
    "\n",
    "# They should be identical\n",
    "match = torch.allclose(output_single, output_first, atol=1e-6)\n",
    "print(f\"Single position output matches full sequence: {match} âœ“\")\n",
    "print(\"Each position is processed independently (no cross-position interaction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9959595a",
   "metadata": {},
   "source": [
    "## 4.2 Layer Normalization from Scratch\n",
    "\n",
    "### The LayerNorm Formula (Ba et al., 2016)\n",
    "\n",
    "$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$\n",
    "\n",
    "Where:\n",
    "- $\\mu = \\frac{1}{d}\\sum_{i=1}^{d} x_i$ (mean over features)\n",
    "- $\\sigma^2 = \\frac{1}{d}\\sum_{i=1}^{d} (x_i - \\mu)^2$ (variance over features)\n",
    "- $\\gamma, \\beta$ are learnable scale and shift parameters\n",
    "- $\\epsilon$ is a small constant for numerical stability\n",
    "\n",
    "### Why Layer Normalization?\n",
    "\n",
    "1. **Stabilizes training**: Keeps activations in a reasonable range\n",
    "2. **Enables deeper networks**: Prevents vanishing/exploding gradients\n",
    "3. **Position-independent**: Unlike BatchNorm, works the same for any sequence length\n",
    "\n",
    "### LayerNorm vs BatchNorm\n",
    "\n",
    "| Aspect | LayerNorm | BatchNorm |\n",
    "|--------|-----------|-----------|\n",
    "| Normalizes over | Features (d_model) | Batch dimension |\n",
    "| Depends on batch size | No | Yes |\n",
    "| Works for variable seq_len | Yes | Problematic |\n",
    "| Used in Transformers | Yes | Rarely |\n",
    "\n",
    "### Pre-Norm vs Post-Norm\n",
    "\n",
    "**Original Transformer (Post-Norm):**\n",
    "$x = \\text{LayerNorm}(x + \\text{Sublayer}(x))$\n",
    "\n",
    "**Modern Transformers (Pre-Norm):**\n",
    "$x = x + \\text{Sublayer}(\\text{LayerNorm}(x))$\n",
    "\n",
    "Pre-Norm is more stable for training deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LAYER NORMALIZATION FROM SCRATCH\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"LAYER NORMALIZATION FROM SCRATCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization from \"Layer Normalization\" (Ba et al., 2016).\n",
    "    \n",
    "    LayerNorm(x) = gamma * (x - mean) / sqrt(var + eps) + beta\n",
    "    \n",
    "    Normalizes over the last dimension (features).\n",
    "    \n",
    "    Args:\n",
    "        d_model: Feature dimension to normalize over\n",
    "        eps: Small constant for numerical stability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.eps = eps\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))   # Scale\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))   # Shift\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (..., d_model) - any shape with d_model as last dim\n",
    "        Returns:\n",
    "            normalized: Same shape as input\n",
    "        \"\"\"\n",
    "        # Compute mean and variance over last dimension\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # Scale and shift\n",
    "        output = self.gamma * x_norm + self.beta\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test our implementation\n",
    "print(\"\\nðŸ“Œ Testing LayerNorm Implementation\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_model = 64\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "our_ln = LayerNorm(d_model)\n",
    "pytorch_ln = nn.LayerNorm(d_model)\n",
    "\n",
    "# Copy parameters for fair comparison\n",
    "pytorch_ln.weight.data = our_ln.gamma.data.clone()\n",
    "pytorch_ln.bias.data = our_ln.beta.data.clone()\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "our_output = our_ln(x)\n",
    "pytorch_output = pytorch_ln(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {our_output.shape}\")\n",
    "\n",
    "match = torch.allclose(our_output, pytorch_output, atol=1e-5)\n",
    "print(f\"\\nMatches PyTorch nn.LayerNorm: {match} âœ“\")\n",
    "\n",
    "# Show normalization effect\n",
    "print(\"\\nðŸ“Œ Normalization Effect\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Before LayerNorm:\")\n",
    "print(f\"  Mean: {x[0, 0].mean().item():.4f}\")\n",
    "print(f\"  Std:  {x[0, 0].std().item():.4f}\")\n",
    "print(f\"After LayerNorm:\")\n",
    "print(f\"  Mean: {our_output[0, 0].mean().item():.4f} (â‰ˆ 0)\")\n",
    "print(f\"  Std:  {our_output[0, 0].std().item():.4f} (â‰ˆ 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZE LAYERNORM EFFECT\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LAYERNORM VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create data with varying scales\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(1, 5, 64)\n",
    "x[:, 0, :] *= 0.1   # Small scale\n",
    "x[:, 1, :] *= 1.0   # Normal scale\n",
    "x[:, 2, :] *= 10.0  # Large scale\n",
    "x[:, 3, :] += 5.0   # Shifted mean\n",
    "x[:, 4, :] = x[:, 4, :] * 5.0 - 3.0  # Both\n",
    "\n",
    "ln = LayerNorm(64)\n",
    "x_norm = ln(x)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Before normalization - distributions\n",
    "ax = axes[0, 0]\n",
    "for i in range(5):\n",
    "    ax.hist(x[0, i].numpy(), bins=20, alpha=0.5, label=f'Position {i}')\n",
    "ax.set_title('Before LayerNorm: Feature Distributions')\n",
    "ax.set_xlabel('Value')\n",
    "ax.legend()\n",
    "\n",
    "# After normalization - distributions\n",
    "ax = axes[0, 1]\n",
    "for i in range(5):\n",
    "    ax.hist(x_norm[0, i].detach().numpy(), bins=20, alpha=0.5, label=f'Position {i}')\n",
    "ax.set_title('After LayerNorm: Feature Distributions')\n",
    "ax.set_xlabel('Value')\n",
    "ax.legend()\n",
    "\n",
    "# Mean and std before\n",
    "ax = axes[1, 0]\n",
    "means = [x[0, i].mean().item() for i in range(5)]\n",
    "stds = [x[0, i].std().item() for i in range(5)]\n",
    "x_pos = range(5)\n",
    "ax.bar([p - 0.2 for p in x_pos], means, 0.4, label='Mean', color='blue', alpha=0.7)\n",
    "ax.bar([p + 0.2 for p in x_pos], stds, 0.4, label='Std', color='orange', alpha=0.7)\n",
    "ax.set_title('Before LayerNorm: Mean & Std per Position')\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.legend()\n",
    "\n",
    "# Mean and std after\n",
    "ax = axes[1, 1]\n",
    "means = [x_norm[0, i].mean().item() for i in range(5)]\n",
    "stds = [x_norm[0, i].std().item() for i in range(5)]\n",
    "ax.bar([p - 0.2 for p in x_pos], means, 0.4, label='Mean', color='blue', alpha=0.7)\n",
    "ax.bar([p + 0.2 for p in x_pos], stds, 0.4, label='Std', color='orange', alpha=0.7)\n",
    "ax.set_title('After LayerNorm: Mean & Std per Position')\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.axhline(y=0, color='blue', linestyle='--', alpha=0.5)\n",
    "ax.axhline(y=1, color='orange', linestyle='--', alpha=0.5)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… LayerNorm normalizes each position independently to meanâ‰ˆ0, stdâ‰ˆ1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f95a888",
   "metadata": {},
   "source": [
    "## 4.3 Residual Connections\n",
    "\n",
    "### The Skip Connection Formula (He et al., 2016)\n",
    "\n",
    "$\\text{output} = x + \\text{Sublayer}(x)$\n",
    "\n",
    "Or with Pre-Norm:\n",
    "$\\text{output} = x + \\text{Sublayer}(\\text{LayerNorm}(x))$\n",
    "\n",
    "### Why Residual Connections?\n",
    "\n",
    "**The Gradient Flow Problem:**\n",
    "In deep networks, gradients must flow through many layers during backpropagation. Each layer can shrink gradients (vanishing) or explode them.\n",
    "\n",
    "**The Solution:**\n",
    "Residual connections provide a \"gradient highway\" - gradients can flow directly through the skip connection, bypassing problematic layers.\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial \\text{output}} \\cdot \\left(1 + \\frac{\\partial \\text{Sublayer}(x)}{\\partial x}\\right)$\n",
    "\n",
    "The \"1\" term ensures gradients always flow, even if the sublayer gradient is small.\n",
    "\n",
    "### Residual Connections in Transformers\n",
    "\n",
    "Each transformer block has TWO residual connections:\n",
    "1. Around the attention sublayer\n",
    "2. Around the FFN sublayer\n",
    "\n",
    "```\n",
    "x â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                  â”‚\n",
    "â””â”€â”€â–º LayerNorm â”€â”€â–º Attention â”€â”€â”€â”€â”€â”€â”´â”€â”€â–º + â”€â”€â–º output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RESIDUAL CONNECTIONS\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"RESIDUAL CONNECTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual connection with layer normalization.\n",
    "    \n",
    "    Implements: output = x + Dropout(Sublayer(LayerNorm(x)))\n",
    "    \n",
    "    This is the Pre-Norm variant used in modern transformers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, sublayer: callable) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            sublayer: Function to apply (attention or FFN)\n",
    "        Returns:\n",
    "            x + sublayer(norm(x))\n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "# Demonstrate gradient flow\n",
    "print(\"\\nðŸ“Œ Gradient Flow Through Residual Connection\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_model = 64\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# Simple sublayer that might have vanishing gradients\n",
    "class PotentiallyProblematicLayer(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        # Initialize with small weights (could cause vanishing gradients)\n",
    "        nn.init.normal_(self.linear.weight, std=0.01)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.tanh(self.linear(x))  # tanh can saturate\n",
    "\n",
    "# Without residual connection\n",
    "layer_no_res = PotentiallyProblematicLayer(d_model)\n",
    "x = torch.randn(batch_size, seq_len, d_model, requires_grad=True)\n",
    "output_no_res = layer_no_res(x)\n",
    "loss_no_res = output_no_res.sum()\n",
    "loss_no_res.backward()\n",
    "grad_no_res = x.grad.abs().mean().item()\n",
    "\n",
    "# With residual connection\n",
    "x = torch.randn(batch_size, seq_len, d_model, requires_grad=True)\n",
    "residual = ResidualConnection(d_model, dropout=0.0)\n",
    "layer_with_res = PotentiallyProblematicLayer(d_model)\n",
    "output_with_res = residual(x, layer_with_res)\n",
    "loss_with_res = output_with_res.sum()\n",
    "loss_with_res.backward()\n",
    "grad_with_res = x.grad.abs().mean().item()\n",
    "\n",
    "print(f\"Mean gradient magnitude WITHOUT residual: {grad_no_res:.6f}\")\n",
    "print(f\"Mean gradient magnitude WITH residual:    {grad_with_res:.6f}\")\n",
    "print(f\"Ratio (with/without): {grad_with_res/grad_no_res:.2f}x\")\n",
    "print(\"\\nâœ… Residual connections help maintain gradient flow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c69581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZE GRADIENT FLOW IN DEEP NETWORKS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GRADIENT FLOW IN DEEP NETWORKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def measure_gradient_flow(n_layers, use_residual):\n",
    "    \"\"\"Measure gradient magnitude at each layer in a deep network.\"\"\"\n",
    "    d_model = 32\n",
    "    \n",
    "    class DeepNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers = nn.ModuleList([\n",
    "                nn.Linear(d_model, d_model) for _ in range(n_layers)\n",
    "            ])\n",
    "            # Small initialization\n",
    "            for layer in self.layers:\n",
    "                nn.init.normal_(layer.weight, std=0.1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            activations = [x]\n",
    "            for layer in self.layers:\n",
    "                if use_residual:\n",
    "                    x = x + torch.tanh(layer(x))\n",
    "                else:\n",
    "                    x = torch.tanh(layer(x))\n",
    "                activations.append(x)\n",
    "            return x, activations\n",
    "    \n",
    "    model = DeepNetwork()\n",
    "    x = torch.randn(1, d_model, requires_grad=True)\n",
    "    output, activations = model(x)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Measure gradient at each layer\n",
    "    gradients = []\n",
    "    for layer in model.layers:\n",
    "        grad_norm = layer.weight.grad.abs().mean().item()\n",
    "        gradients.append(grad_norm)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "n_layers = 20\n",
    "\n",
    "grads_no_res = measure_gradient_flow(n_layers, use_residual=False)\n",
    "grads_with_res = measure_gradient_flow(n_layers, use_residual=True)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, n_layers + 1), grads_no_res, 'r-o', label='Without Residual')\n",
    "plt.plot(range(1, n_layers + 1), grads_with_res, 'b-o', label='With Residual')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Mean Gradient Magnitude')\n",
    "plt.title('Gradient Flow Through Layers')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(range(1, n_layers + 1), grads_no_res, 'r-o', label='Without Residual')\n",
    "plt.semilogy(range(1, n_layers + 1), grads_with_res, 'b-o', label='With Residual')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Mean Gradient Magnitude (log scale)')\n",
    "plt.title('Gradient Flow (Log Scale)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Without residual connections, gradients vanish in deep networks!\")\n",
    "print(\"   Residual connections maintain gradient flow through all layers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c9b0a",
   "metadata": {},
   "source": [
    "## 4.4 Dropout from Scratch\n",
    "\n",
    "### The Dropout Formula (Srivastava et al., 2014)\n",
    "\n",
    "**During Training:**\n",
    "$y_i = \\frac{x_i \\cdot m_i}{1 - p}$ where $m_i \\sim \\text{Bernoulli}(1-p)$\n",
    "\n",
    "**During Inference:**\n",
    "$y = x$ (no dropout)\n",
    "\n",
    "### Why Scale by $\\frac{1}{1-p}$?\n",
    "\n",
    "This is called \"inverted dropout\". Without scaling:\n",
    "- Training: Expected value = $(1-p) \\cdot x$\n",
    "- Inference: Expected value = $x$\n",
    "\n",
    "With scaling during training:\n",
    "- Training: Expected value = $\\frac{(1-p) \\cdot x}{1-p} = x$\n",
    "- Inference: Expected value = $x$\n",
    "\n",
    "This keeps expected values consistent between training and inference.\n",
    "\n",
    "### Dropout in Transformers\n",
    "\n",
    "Dropout is applied in three places:\n",
    "1. **After attention weights** (before multiplying with V)\n",
    "2. **After each sublayer** (attention and FFN)\n",
    "3. **After embeddings** (input dropout)\n",
    "\n",
    "Typical dropout rate: 0.1 for large models, 0.3 for smaller models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e952364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DROPOUT FROM SCRATCH\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"DROPOUT FROM SCRATCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class Dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Dropout regularization from \"Dropout: A Simple Way to Prevent \n",
    "    Neural Networks from Overfitting\" (Srivastava et al., 2014).\n",
    "    \n",
    "    During training: randomly zero out elements with probability p,\n",
    "    then scale remaining elements by 1/(1-p).\n",
    "    \n",
    "    During inference: identity function (no dropout).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, p: float = 0.5):\n",
    "        super().__init__()\n",
    "        if not 0 <= p < 1:\n",
    "            raise ValueError(f\"Dropout probability must be in [0, 1), got {p}\")\n",
    "        self.p = p\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.training or self.p == 0:\n",
    "            return x\n",
    "        \n",
    "        # Create binary mask: 1 with probability (1-p), 0 with probability p\n",
    "        mask = torch.bernoulli(torch.full_like(x, 1 - self.p))\n",
    "        \n",
    "        # Apply mask and scale\n",
    "        return x * mask / (1 - self.p)\n",
    "\n",
    "# Test dropout\n",
    "print(\"\\nðŸ“Œ Testing Dropout Implementation\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "p = 0.3\n",
    "our_dropout = Dropout(p)\n",
    "pytorch_dropout = nn.Dropout(p)\n",
    "\n",
    "x = torch.ones(1000)\n",
    "\n",
    "# Training mode\n",
    "our_dropout.train()\n",
    "pytorch_dropout.train()\n",
    "\n",
    "our_output = our_dropout(x)\n",
    "pytorch_output = pytorch_dropout(x)\n",
    "\n",
    "print(f\"Dropout probability: {p}\")\n",
    "print(f\"Input: all ones, shape {x.shape}\")\n",
    "print(f\"\\nTraining mode:\")\n",
    "print(f\"  Our dropout - zeros: {(our_output == 0).sum().item()}, \"\n",
    "      f\"non-zeros scaled to: {our_output[our_output > 0].mean():.3f}\")\n",
    "print(f\"  Expected zeros: ~{int(p * len(x))}, expected scale: {1/(1-p):.3f}\")\n",
    "\n",
    "# Verify expected value is preserved\n",
    "print(f\"\\n  Input mean: {x.mean():.3f}\")\n",
    "print(f\"  Output mean (our): {our_output.mean():.3f}\")\n",
    "print(f\"  Output mean (pytorch): {pytorch_output.mean():.3f}\")\n",
    "print(\"  (Should be approximately equal due to scaling)\")\n",
    "\n",
    "# Eval mode\n",
    "our_dropout.eval()\n",
    "our_output_eval = our_dropout(x)\n",
    "print(f\"\\nEval mode:\")\n",
    "print(f\"  Output equals input: {torch.equal(our_output_eval, x)} âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5369d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DROPOUT EFFECT ON TRAINING\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DROPOUT REGULARIZATION EFFECT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple overfitting scenario\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Small dataset that's easy to overfit\n",
    "n_samples = 50\n",
    "n_features = 20\n",
    "X = torch.randn(n_samples, n_features)\n",
    "# Non-linear target with noise\n",
    "y = (X[:, 0] * X[:, 1] + 0.5 * X[:, 2]**2 + torch.randn(n_samples) * 0.1).unsqueeze(1)\n",
    "\n",
    "# Split\n",
    "X_train, X_test = X[:40], X[40:]\n",
    "y_train, y_test = y[:40], y[40:]\n",
    "\n",
    "class MLPWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_p):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_model(dropout_p, epochs=500):\n",
    "    model = MLPWithDropout(dropout_p)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_train)\n",
    "        loss = criterion(pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Test\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_pred = model(X_test)\n",
    "            test_loss = criterion(test_pred, y_test)\n",
    "            test_losses.append(test_loss.item())\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "# Train with different dropout rates\n",
    "results = {}\n",
    "for p in [0.0, 0.3, 0.5]:\n",
    "    print(f\"Training with dropout={p}...\")\n",
    "    results[p] = train_model(p)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "for p, (train_loss, _) in results.items():\n",
    "    ax.plot(train_loss, label=f'Dropout={p}')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Training Loss')\n",
    "ax.set_title('Training Loss')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax = axes[1]\n",
    "for p, (_, test_loss) in results.items():\n",
    "    ax.plot(test_loss, label=f'Dropout={p}')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Test Loss')\n",
    "ax.set_title('Test Loss (Generalization)')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Dropout helps prevent overfitting by regularizing the network!\")\n",
    "print(\"   Without dropout: low train loss, high test loss (overfitting)\")\n",
    "print(\"   With dropout: slightly higher train loss, but better generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35561c5f",
   "metadata": {},
   "source": [
    "## 4.5 Transformer Encoder Block\n",
    "\n",
    "### Encoder Block Architecture\n",
    "\n",
    "The encoder block consists of two sublayers with residual connections:\n",
    "\n",
    "```\n",
    "Input x\n",
    "    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â–¼                               â”‚\n",
    "LayerNorm                           â”‚\n",
    "    â”‚                               â”‚\n",
    "    â–¼                               â”‚\n",
    "Multi-Head Self-Attention           â”‚\n",
    "    â”‚                               â”‚\n",
    "    â–¼                               â”‚\n",
    "Dropout                             â”‚\n",
    "    â”‚                               â”‚\n",
    "    â–¼                               â”‚\n",
    "    + â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â–¼                               â”‚\n",
    "LayerNorm                           â”‚\n",
    "    â”‚                               â”‚\n",
    "    â–¼                               â”‚\n",
    "Position-wise FFN                   â”‚\n",
    "    â”‚                               â”‚\n",
    "    â–¼                               â”‚\n",
    "Dropout                             â”‚\n",
    "    â”‚                               â”‚\n",
    "    â–¼                               â”‚\n",
    "    + â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚\n",
    "    â–¼\n",
    "Output\n",
    "```\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. **Bidirectional attention**: Each position attends to all positions\n",
    "2. **Pre-Norm**: LayerNorm before sublayers (modern variant)\n",
    "3. **Two residual connections**: One per sublayer\n",
    "4. **Same input/output dimension**: $(B, S, d_{model})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf57c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRANSFORMER ENCODER BLOCK\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSFORMER ENCODER BLOCK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Encoder Block.\n",
    "    \n",
    "    Architecture (Pre-Norm variant):\n",
    "        x -> LayerNorm -> MultiHeadAttention -> Dropout -> + -> \n",
    "             LayerNorm -> FFN -> Dropout -> + -> output\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        n_heads: Number of attention heads\n",
    "        d_ff: FFN hidden dimension\n",
    "        dropout: Dropout probability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        d_ff: int = None,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        \n",
    "        # Self-attention sublayer\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # FFN sublayer\n",
    "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Self-attention with residual\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.self_attn(x, x, x, mask)  # Q=K=V for self-attention\n",
    "        x = self.dropout1(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        # FFN with residual\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test encoder block\n",
    "print(\"\\nðŸ“Œ Testing Encoder Block\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_model = 64\n",
    "n_heads = 8\n",
    "d_ff = 256\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "encoder_block = TransformerEncoderBlock(d_model, n_heads, d_ff, dropout=0.1)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = encoder_block(x)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  n_heads: {n_heads}\")\n",
    "print(f\"  d_ff: {d_ff}\")\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Parameter count\n",
    "total_params = sum(p.numel() for p in encoder_block.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Breakdown\n",
    "attn_params = sum(p.numel() for p in encoder_block.self_attn.parameters())\n",
    "ffn_params = sum(p.numel() for p in encoder_block.ffn.parameters())\n",
    "norm_params = sum(p.numel() for p in encoder_block.norm1.parameters()) + \\\n",
    "              sum(p.numel() for p in encoder_block.norm2.parameters())\n",
    "print(f\"  Attention: {attn_params:,}\")\n",
    "print(f\"  FFN: {ffn_params:,}\")\n",
    "print(f\"  LayerNorms: {norm_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d13f5f6",
   "metadata": {},
   "source": [
    "## 4.6 Transformer Decoder Block\n",
    "\n",
    "### Decoder Block Architecture\n",
    "\n",
    "The decoder has THREE sublayers (one more than encoder):\n",
    "\n",
    "```\n",
    "Input x (target sequence)\n",
    "    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â–¼                               â”‚\n",
    "LayerNorm                           â”‚\n",
    "    â”‚                               â”‚\n",
    "    â–¼                               â”‚\n",
    "Masked Self-Attention â—„â”€â”€ Causal    â”‚\n",
    "    â”‚                     Mask      â”‚\n",
    "    â–¼                               â”‚\n",
    "Dropout                             â”‚\n",
    "    â”‚                               â”‚\n",
    "    â–¼                               â”‚\n",
    "    + â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â–¼                               â”‚\n",
    "LayerNorm                           â”‚\n",
    "    â”‚                               â”‚\n",
    "    â–¼                               â”‚\n",
    "Cross-Attention â—„â”€â”€ Encoder Output  â”‚\n",
    "    â”‚                               â”‚\n",
    "    â–¼                               â”‚\n",
    "Dropout                             â”‚\n",
    "    â”‚                               â”‚\n",
    "    â–¼                               â”‚\n",
    "    + â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â–¼                               â”‚\n",
    "LayerNorm                           â”‚\n",
    "    â”‚                               â”‚\n",
    "    â–¼                               â”‚\n",
    "Position-wise FFN                   â”‚\n",
    "    â”‚                               â”‚\n",
    "    â–¼                               â”‚\n",
    "Dropout                             â”‚\n",
    "    â”‚                               â”‚\n",
    "    â–¼                               â”‚\n",
    "    + â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚\n",
    "    â–¼\n",
    "Output\n",
    "```\n",
    "\n",
    "### Key Differences from Encoder\n",
    "\n",
    "1. **Masked self-attention**: Causal mask prevents attending to future tokens\n",
    "2. **Cross-attention**: Attends to encoder output (Q from decoder, K/V from encoder)\n",
    "3. **Three sublayers**: Self-attn â†’ Cross-attn â†’ FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRANSFORMER DECODER BLOCK\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSFORMER DECODER BLOCK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Decoder Block.\n",
    "    \n",
    "    Architecture (Pre-Norm variant):\n",
    "        x -> LayerNorm -> MaskedSelfAttention -> Dropout -> + ->\n",
    "             LayerNorm -> CrossAttention(encoder_output) -> Dropout -> + ->\n",
    "             LayerNorm -> FFN -> Dropout -> + -> output\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        n_heads: Number of attention heads\n",
    "        d_ff: FFN hidden dimension\n",
    "        dropout: Dropout probability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        d_ff: int = None,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        \n",
    "        # Masked self-attention sublayer\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Cross-attention sublayer\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # FFN sublayer\n",
    "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        encoder_output: torch.Tensor,\n",
    "        self_attn_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Decoder input (batch, tgt_len, d_model)\n",
    "            encoder_output: Encoder output (batch, src_len, d_model)\n",
    "            self_attn_mask: Causal mask for self-attention\n",
    "            cross_attn_mask: Optional mask for cross-attention (e.g., padding)\n",
    "        Returns:\n",
    "            output: (batch, tgt_len, d_model)\n",
    "        \"\"\"\n",
    "        # Masked self-attention with residual\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.self_attn(x, x, x, mask=self_attn_mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        # Cross-attention with residual\n",
    "        # Q from decoder, K/V from encoder\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.cross_attn(x, encoder_output, encoder_output, mask=cross_attn_mask)\n",
    "        x = self.dropout2(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        # FFN with residual\n",
    "        residual = x\n",
    "        x = self.norm3(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test decoder block\n",
    "print(\"\\nðŸ“Œ Testing Decoder Block\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_model = 64\n",
    "n_heads = 8\n",
    "d_ff = 256\n",
    "batch_size = 2\n",
    "src_len = 10  # Encoder sequence length\n",
    "tgt_len = 8   # Decoder sequence length\n",
    "\n",
    "decoder_block = TransformerDecoderBlock(d_model, n_heads, d_ff, dropout=0.1)\n",
    "\n",
    "# Inputs\n",
    "decoder_input = torch.randn(batch_size, tgt_len, d_model)\n",
    "encoder_output = torch.randn(batch_size, src_len, d_model)\n",
    "\n",
    "# Causal mask for self-attention\n",
    "causal_mask = torch.tril(torch.ones(tgt_len, tgt_len))\n",
    "\n",
    "output = decoder_block(decoder_input, encoder_output, self_attn_mask=causal_mask)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  n_heads: {n_heads}\")\n",
    "print(f\"  d_ff: {d_ff}\")\n",
    "print(f\"\\nDecoder input shape: {decoder_input.shape}\")\n",
    "print(f\"Encoder output shape: {encoder_output.shape}\")\n",
    "print(f\"Causal mask shape: {causal_mask.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Parameter count\n",
    "total_params = sum(p.numel() for p in decoder_block.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"  (More than encoder due to cross-attention layer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717ba424",
   "metadata": {},
   "source": [
    "## 4.7 Full Encoder-Decoder Transformer\n",
    "\n",
    "### Complete Architecture (Vaswani et al., 2017)\n",
    "\n",
    "```\n",
    "Source Sequence                          Target Sequence\n",
    "      â”‚                                        â”‚\n",
    "      â–¼                                        â–¼\n",
    "Input Embedding                          Output Embedding\n",
    "      â”‚                                        â”‚\n",
    "      â–¼                                        â–¼\n",
    "   + Positional                            + Positional\n",
    "   Encoding                                Encoding\n",
    "      â”‚                                        â”‚\n",
    "      â–¼                                        â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
    "â”‚   Encoder   â”‚                                â”‚\n",
    "â”‚   Block 1   â”‚                                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚\n",
    "      â”‚                                        â”‚\n",
    "      â–¼                                        â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
    "â”‚   Encoder   â”‚                                â”‚\n",
    "â”‚   Block 2   â”‚                                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚\n",
    "      â”‚                                        â”‚\n",
    "      â‹® (N layers)                             â”‚\n",
    "      â”‚                                        â”‚\n",
    "      â–¼                                        â–¼\n",
    "Encoder Output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                      â”‚   Decoder   â”‚\n",
    "                                      â”‚   Block 1   â”‚\n",
    "                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                            â”‚\n",
    "                                            â–¼\n",
    "                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                      â”‚   Decoder   â”‚\n",
    "                                      â”‚   Block 2   â”‚\n",
    "                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                            â”‚\n",
    "                                            â‹® (N layers)\n",
    "                                            â”‚\n",
    "                                            â–¼\n",
    "                                       LayerNorm\n",
    "                                            â”‚\n",
    "                                            â–¼\n",
    "                                    Linear (vocab_size)\n",
    "                                            â”‚\n",
    "                                            â–¼\n",
    "                                        Softmax\n",
    "                                            â”‚\n",
    "                                            â–¼\n",
    "                                    Output Probabilities\n",
    "```\n",
    "\n",
    "### Original Transformer Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| $d_{model}$ | 512 |\n",
    "| $d_{ff}$ | 2048 |\n",
    "| $h$ (heads) | 8 |\n",
    "| $N$ (layers) | 6 |\n",
    "| $d_k = d_v$ | 64 |\n",
    "| Dropout | 0.1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de985dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FULL ENCODER-DECODER TRANSFORMER\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"FULL ENCODER-DECODER TRANSFORMER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Full Encoder-Decoder Transformer from \"Attention Is All You Need\".\n",
    "    \n",
    "    Args:\n",
    "        src_vocab_size: Source vocabulary size\n",
    "        tgt_vocab_size: Target vocabulary size\n",
    "        d_model: Model dimension\n",
    "        n_heads: Number of attention heads\n",
    "        n_encoder_layers: Number of encoder layers\n",
    "        n_decoder_layers: Number of decoder layers\n",
    "        d_ff: FFN hidden dimension\n",
    "        max_len: Maximum sequence length\n",
    "        dropout: Dropout probability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        d_model: int = 512,\n",
    "        n_heads: int = 8,\n",
    "        n_encoder_layers: int = 6,\n",
    "        n_decoder_layers: int = 6,\n",
    "        d_ff: int = 2048,\n",
    "        max_len: int = 512,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embeddings\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = self._create_positional_encoding(max_len, d_model)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_encoder_layers)\n",
    "        ])\n",
    "        self.encoder_norm = LayerNorm(d_model)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_decoder_layers)\n",
    "        ])\n",
    "        self.decoder_norm = LayerNorm(d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "    \n",
    "    def _create_positional_encoding(self, max_len: int, d_model: int) -> torch.Tensor:\n",
    "        \"\"\"Create sinusoidal positional encoding.\"\"\"\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "    \n",
    "    def _init_parameters(self):\n",
    "        \"\"\"Initialize parameters with Xavier uniform.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def _create_causal_mask(self, size: int) -> torch.Tensor:\n",
    "        \"\"\"Create causal mask for decoder self-attention.\"\"\"\n",
    "        return torch.tril(torch.ones(size, size))\n",
    "    \n",
    "    def encode(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        src_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode source sequence.\n",
    "        \n",
    "        Args:\n",
    "            src: Source token IDs (batch, src_len)\n",
    "            src_mask: Optional padding mask\n",
    "        Returns:\n",
    "            encoder_output: (batch, src_len, d_model)\n",
    "        \"\"\"\n",
    "        # Embed and add positional encoding\n",
    "        src_len = src.size(1)\n",
    "        x = self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:, :src_len, :].to(x.device)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask=src_mask)\n",
    "        \n",
    "        return self.encoder_norm(x)\n",
    "    \n",
    "    def decode(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        encoder_output: torch.Tensor,\n",
    "        tgt_mask: Optional[torch.Tensor] = None,\n",
    "        memory_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode target sequence.\n",
    "        \n",
    "        Args:\n",
    "            tgt: Target token IDs (batch, tgt_len)\n",
    "            encoder_output: Encoder output (batch, src_len, d_model)\n",
    "            tgt_mask: Causal mask for self-attention\n",
    "            memory_mask: Optional mask for cross-attention\n",
    "        Returns:\n",
    "            decoder_output: (batch, tgt_len, d_model)\n",
    "        \"\"\"\n",
    "        # Embed and add positional encoding\n",
    "        tgt_len = tgt.size(1)\n",
    "        x = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:, :tgt_len, :].to(x.device)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Create causal mask if not provided\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = self._create_causal_mask(tgt_len).to(x.device)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output, self_attn_mask=tgt_mask, cross_attn_mask=memory_mask)\n",
    "        \n",
    "        return self.decoder_norm(x)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        tgt: torch.Tensor,\n",
    "        src_mask: Optional[torch.Tensor] = None,\n",
    "        tgt_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Full forward pass.\n",
    "        \n",
    "        Args:\n",
    "            src: Source token IDs (batch, src_len)\n",
    "            tgt: Target token IDs (batch, tgt_len)\n",
    "            src_mask: Optional source padding mask\n",
    "            tgt_mask: Optional target causal mask\n",
    "        Returns:\n",
    "            logits: (batch, tgt_len, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        decoder_output = self.decode(tgt, encoder_output, tgt_mask)\n",
    "        logits = self.output_projection(decoder_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Test the full transformer\n",
    "print(\"\\nðŸ“Œ Testing Full Transformer\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Small configuration for testing\n",
    "config = {\n",
    "    'src_vocab_size': 1000,\n",
    "    'tgt_vocab_size': 1000,\n",
    "    'd_model': 64,\n",
    "    'n_heads': 4,\n",
    "    'n_encoder_layers': 2,\n",
    "    'n_decoder_layers': 2,\n",
    "    'd_ff': 256,\n",
    "    'max_len': 100,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "transformer = Transformer(**config)\n",
    "\n",
    "batch_size = 2\n",
    "src_len = 10\n",
    "tgt_len = 8\n",
    "\n",
    "src = torch.randint(0, config['src_vocab_size'], (batch_size, src_len))\n",
    "tgt = torch.randint(0, config['tgt_vocab_size'], (batch_size, tgt_len))\n",
    "\n",
    "logits = transformer(src, tgt)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"\\nSource shape: {src.shape}\")\n",
    "print(f\"Target shape: {tgt.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "\n",
    "# Parameter count\n",
    "total_params = sum(p.numel() for p in transformer.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01d8a29",
   "metadata": {},
   "source": [
    "## 4.8 Weight Initialization\n",
    "\n",
    "### Why Initialization Matters\n",
    "\n",
    "Poor initialization can cause:\n",
    "- **Vanishing gradients**: Weights too small â†’ activations shrink â†’ gradients vanish\n",
    "- **Exploding gradients**: Weights too large â†’ activations explode â†’ training unstable\n",
    "- **Symmetry**: Identical weights â†’ neurons learn the same thing\n",
    "\n",
    "### Xavier/Glorot Initialization (Glorot & Bengio, 2010)\n",
    "\n",
    "For layers with tanh or sigmoid activation:\n",
    "\n",
    "$W \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}}\\right)$\n",
    "\n",
    "Or normal distribution:\n",
    "$W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{in} + n_{out}}\\right)$\n",
    "\n",
    "**Goal**: Keep variance of activations constant across layers.\n",
    "\n",
    "### He/Kaiming Initialization (He et al., 2015)\n",
    "\n",
    "For layers with ReLU activation:\n",
    "\n",
    "$W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{in}}\\right)$\n",
    "\n",
    "**Why different?** ReLU zeros out half the activations, so we need 2x variance.\n",
    "\n",
    "### Transformer-Specific Initialization\n",
    "\n",
    "The original paper uses Xavier uniform. Modern practices:\n",
    "- **Embeddings**: $\\mathcal{N}(0, 1)$ or $\\mathcal{N}(0, 1/\\sqrt{d_{model}})$\n",
    "- **Linear layers**: Xavier uniform\n",
    "- **Output projection**: Sometimes scaled by $1/\\sqrt{N}$ where N = number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc904b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# WEIGHT INITIALIZATION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"WEIGHT INITIALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def xavier_uniform_(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Xavier/Glorot uniform initialization.\n",
    "    \n",
    "    W ~ U(-sqrt(6/(fan_in + fan_out)), sqrt(6/(fan_in + fan_out)))\n",
    "    \"\"\"\n",
    "    fan_in, fan_out = tensor.size(-2), tensor.size(-1)\n",
    "    bound = math.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return tensor.uniform_(-bound, bound)\n",
    "\n",
    "def xavier_normal_(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Xavier/Glorot normal initialization.\n",
    "    \n",
    "    W ~ N(0, 2/(fan_in + fan_out))\n",
    "    \"\"\"\n",
    "    fan_in, fan_out = tensor.size(-2), tensor.size(-1)\n",
    "    std = math.sqrt(2.0 / (fan_in + fan_out))\n",
    "    return tensor.normal_(0, std)\n",
    "\n",
    "def he_normal_(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    He/Kaiming normal initialization for ReLU.\n",
    "    \n",
    "    W ~ N(0, 2/fan_in)\n",
    "    \"\"\"\n",
    "    fan_in = tensor.size(-2)\n",
    "    std = math.sqrt(2.0 / fan_in)\n",
    "    return tensor.normal_(0, std)\n",
    "\n",
    "# Compare initializations\n",
    "print(\"\\nðŸ“Œ Comparing Initialization Methods\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "fan_in, fan_out = 512, 512\n",
    "\n",
    "# Create weights with different initializations\n",
    "w_xavier_u = xavier_uniform_(torch.empty(fan_in, fan_out))\n",
    "w_xavier_n = xavier_normal_(torch.empty(fan_in, fan_out))\n",
    "w_he = he_normal_(torch.empty(fan_in, fan_out))\n",
    "w_random = torch.randn(fan_in, fan_out)  # Standard normal\n",
    "\n",
    "print(f\"Weight matrix shape: ({fan_in}, {fan_out})\")\n",
    "print(f\"\\nInitialization statistics:\")\n",
    "print(f\"  Xavier Uniform: mean={w_xavier_u.mean():.4f}, std={w_xavier_u.std():.4f}\")\n",
    "print(f\"  Xavier Normal:  mean={w_xavier_n.mean():.4f}, std={w_xavier_n.std():.4f}\")\n",
    "print(f\"  He Normal:      mean={w_he.mean():.4f}, std={w_he.std():.4f}\")\n",
    "print(f\"  Random N(0,1):  mean={w_random.mean():.4f}, std={w_random.std():.4f}\")\n",
    "\n",
    "# Expected values\n",
    "print(f\"\\nExpected standard deviations:\")\n",
    "print(f\"  Xavier: sqrt(2/{fan_in + fan_out}) = {math.sqrt(2/(fan_in + fan_out)):.4f}\")\n",
    "print(f\"  He:     sqrt(2/{fan_in}) = {math.sqrt(2/fan_in):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f31068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EFFECT OF INITIALIZATION ON FORWARD PASS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INITIALIZATION EFFECT ON ACTIVATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def test_forward_pass(init_fn, name, n_layers=10):\n",
    "    \"\"\"Test how activations evolve through layers with given initialization.\"\"\"\n",
    "    d = 512\n",
    "    x = torch.randn(32, d)  # Batch of 32\n",
    "    \n",
    "    activation_stds = [x.std().item()]\n",
    "    \n",
    "    for _ in range(n_layers):\n",
    "        w = init_fn(torch.empty(d, d))\n",
    "        x = torch.relu(x @ w)\n",
    "        activation_stds.append(x.std().item())\n",
    "    \n",
    "    return activation_stds\n",
    "\n",
    "# Test different initializations\n",
    "results = {\n",
    "    'Xavier Uniform': test_forward_pass(xavier_uniform_, 'Xavier Uniform'),\n",
    "    'Xavier Normal': test_forward_pass(xavier_normal_, 'Xavier Normal'),\n",
    "    'He Normal': test_forward_pass(he_normal_, 'He Normal'),\n",
    "    'Random N(0,1)': test_forward_pass(lambda t: t.normal_(0, 1), 'Random'),\n",
    "    'Too Small': test_forward_pass(lambda t: t.normal_(0, 0.01), 'Too Small'),\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for name, stds in results.items():\n",
    "    plt.plot(stds, label=name, marker='o')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Activation Std')\n",
    "plt.title('Activation Standard Deviation Through Layers')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for name, stds in results.items():\n",
    "    plt.semilogy(stds, label=name, marker='o')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Activation Std (log scale)')\n",
    "plt.title('Activation Std (Log Scale)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… He initialization maintains activation scale with ReLU!\")\n",
    "print(\"   Xavier works well for tanh/sigmoid but activations shrink with ReLU.\")\n",
    "print(\"   Random N(0,1) causes exploding activations.\")\n",
    "print(\"   Too small initialization causes vanishing activations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4463de5e",
   "metadata": {},
   "source": [
    "## 4.9 Training on the Copy Task\n",
    "\n",
    "### The Copy Task\n",
    "\n",
    "A simple sanity check for sequence-to-sequence models:\n",
    "- **Input**: A sequence of tokens, e.g., `[1, 5, 3, 7, 2]`\n",
    "- **Output**: The same sequence, e.g., `[1, 5, 3, 7, 2]`\n",
    "\n",
    "If the model can't learn to copy, something is fundamentally wrong!\n",
    "\n",
    "### Training Setup\n",
    "\n",
    "- **Loss**: Cross-entropy between predicted and target tokens\n",
    "- **Teacher forcing**: During training, feed ground truth tokens to decoder\n",
    "- **Evaluation**: Generate autoregressively (one token at a time)\n",
    "\n",
    "### What We're Testing\n",
    "\n",
    "1. Encoder correctly encodes the input sequence\n",
    "2. Decoder attends to encoder output via cross-attention\n",
    "3. Model learns to reproduce the input exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COPY TASK TRAINING\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING TRANSFORMER ON COPY TASK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Special tokens\n",
    "PAD_IDX = 0\n",
    "BOS_IDX = 1  # Beginning of sequence\n",
    "EOS_IDX = 2  # End of sequence\n",
    "\n",
    "def generate_copy_data(batch_size: int, seq_len: int, vocab_size: int):\n",
    "    \"\"\"\n",
    "    Generate copy task data.\n",
    "    \n",
    "    Source: [BOS, x1, x2, ..., xn, EOS, PAD, ...]\n",
    "    Target: [BOS, x1, x2, ..., xn, EOS, PAD, ...]\n",
    "    \n",
    "    Returns:\n",
    "        src: (batch_size, seq_len + 2)\n",
    "        tgt: (batch_size, seq_len + 2)\n",
    "    \"\"\"\n",
    "    # Random tokens (excluding special tokens)\n",
    "    data = torch.randint(3, vocab_size, (batch_size, seq_len))\n",
    "    \n",
    "    # Add BOS and EOS\n",
    "    bos = torch.full((batch_size, 1), BOS_IDX)\n",
    "    eos = torch.full((batch_size, 1), EOS_IDX)\n",
    "    \n",
    "    src = torch.cat([bos, data, eos], dim=1)\n",
    "    tgt = torch.cat([bos, data, eos], dim=1)\n",
    "    \n",
    "    return src, tgt\n",
    "\n",
    "# Create a small transformer for the copy task\n",
    "vocab_size = 20\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "d_ff = 128\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_encoder_layers=n_layers,\n",
    "    n_decoder_layers=n_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_len=50,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  Vocab size: {vocab_size}\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  n_heads: {n_heads}\")\n",
    "print(f\"  n_layers: {n_layers}\")\n",
    "print(f\"  Total params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nðŸ“Œ Training...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "seq_len = 8\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Generate batch\n",
    "    src, tgt = generate_copy_data(batch_size, seq_len, vocab_size)\n",
    "    \n",
    "    # Forward pass\n",
    "    # Input to decoder: all tokens except last (teacher forcing)\n",
    "    # Target: all tokens except first (shifted by 1)\n",
    "    tgt_input = tgt[:, :-1]\n",
    "    tgt_output = tgt[:, 1:]\n",
    "    \n",
    "    logits = model(src, tgt_input)\n",
    "    \n",
    "    # Compute loss\n",
    "    # logits: (batch, seq_len, vocab_size) -> (batch * seq_len, vocab_size)\n",
    "    # tgt_output: (batch, seq_len) -> (batch * seq_len,)\n",
    "    loss = criterion(logits.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch + 1:3d}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34916aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVALUATE COPY TASK\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATING COPY TASK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def greedy_decode(model, src, max_len=20):\n",
    "    \"\"\"\n",
    "    Greedy decoding for sequence generation.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained transformer\n",
    "        src: Source sequence (1, src_len)\n",
    "        max_len: Maximum output length\n",
    "    \n",
    "    Returns:\n",
    "        output: Generated sequence\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode source\n",
    "        encoder_output = model.encode(src)\n",
    "        \n",
    "        # Start with BOS token\n",
    "        tgt = torch.tensor([[BOS_IDX]])\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            # Decode\n",
    "            decoder_output = model.decode(tgt, encoder_output)\n",
    "            \n",
    "            # Get next token (greedy: take argmax)\n",
    "            logits = model.output_projection(decoder_output[:, -1, :])\n",
    "            next_token = logits.argmax(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Append to sequence\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "            \n",
    "            # Stop if EOS\n",
    "            if next_token.item() == EOS_IDX:\n",
    "                break\n",
    "    \n",
    "    return tgt[0].tolist()\n",
    "\n",
    "# Test on new sequences\n",
    "print(\"\\nðŸ“Œ Testing on New Sequences\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "model.eval()\n",
    "n_tests = 5\n",
    "correct = 0\n",
    "\n",
    "for i in range(n_tests):\n",
    "    # Generate test sequence\n",
    "    src, tgt = generate_copy_data(1, seq_len, vocab_size)\n",
    "    \n",
    "    # Decode\n",
    "    output = greedy_decode(model, src)\n",
    "    \n",
    "    # Compare (excluding BOS)\n",
    "    src_tokens = src[0].tolist()\n",
    "    tgt_tokens = tgt[0].tolist()\n",
    "    \n",
    "    # Check if output matches target\n",
    "    match = output == tgt_tokens\n",
    "    if match:\n",
    "        correct += 1\n",
    "    \n",
    "    print(f\"\\nTest {i + 1}:\")\n",
    "    print(f\"  Source:    {src_tokens}\")\n",
    "    print(f\"  Expected:  {tgt_tokens}\")\n",
    "    print(f\"  Generated: {output}\")\n",
    "    print(f\"  Match: {'âœ“' if match else 'âœ—'}\")\n",
    "\n",
    "print(f\"\\nAccuracy: {correct}/{n_tests} = {100*correct/n_tests:.0f}%\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Copy Task Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… If accuracy is high, the transformer is working correctly!\")\n",
    "print(\"   The model learned to encode input and decode it back.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad07e30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 4: Key Takeaways\n",
    "\n",
    "### Transformer Components\n",
    "\n",
    "| Component | Formula | Purpose |\n",
    "|-----------|---------|---------|\n",
    "| **FFN** | $\\text{ReLU}(xW_1)W_2$ | Non-linear transformation per position |\n",
    "| **LayerNorm** | $\\gamma \\frac{x-\\mu}{\\sigma} + \\beta$ | Stabilize activations |\n",
    "| **Residual** | $x + \\text{Sublayer}(x)$ | Enable gradient flow |\n",
    "| **Dropout** | $\\frac{x \\cdot m}{1-p}$ | Regularization |\n",
    "\n",
    "### Encoder vs Decoder\n",
    "\n",
    "| Aspect | Encoder | Decoder |\n",
    "|--------|---------|---------|\n",
    "| Attention | Bidirectional | Causal (masked) |\n",
    "| Cross-attention | No | Yes (to encoder) |\n",
    "| Sublayers | 2 | 3 |\n",
    "| Use case | Understanding | Generation |\n",
    "\n",
    "### Architecture Choices\n",
    "\n",
    "**Pre-Norm (Modern):**\n",
    "$x + \\text{Sublayer}(\\text{LayerNorm}(x))$\n",
    "- More stable training\n",
    "- Used in GPT-2, GPT-3, LLaMA\n",
    "\n",
    "**Post-Norm (Original):**\n",
    "$\\text{LayerNorm}(x + \\text{Sublayer}(x))$\n",
    "- Original transformer\n",
    "- Requires careful learning rate warmup\n",
    "\n",
    "### Parameter Count (per layer)\n",
    "\n",
    "- **Attention**: $4 \\times d_{model}^2$ (Q, K, V, O projections)\n",
    "- **FFN**: $8 \\times d_{model}^2$ (two linear layers with 4x expansion)\n",
    "- **Total per layer**: $12 \\times d_{model}^2$\n",
    "\n",
    "### Initialization\n",
    "\n",
    "- **Xavier**: For tanh/sigmoid activations\n",
    "- **He**: For ReLU activations\n",
    "- **Transformers**: Typically Xavier uniform\n",
    "\n",
    "### Training Tips\n",
    "\n",
    "1. **Learning rate warmup**: Gradually increase LR at start\n",
    "2. **Gradient clipping**: Prevent exploding gradients\n",
    "3. **Label smoothing**: Soft targets for better generalization\n",
    "4. **Dropout**: 0.1 for large models, higher for small\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [BERT (Encoder-Only)](#module-5-bert) - Bidirectional understanding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ba487",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-5-bert'></a>\n",
    "# Module 5: BERT - Encoder-Only Architecture\n",
    "\n",
    "**Prerequisites:** Modules 1-4 (PyTorch, embeddings, attention, transformer architecture)\n",
    "\n",
    "**Learning Objectives:**\n",
    "By the end of this module, you will be able to:\n",
    "- Understand the BERT architecture and its innovations\n",
    "- Implement Masked Language Modeling (MLM) from scratch\n",
    "- Implement Next Sentence Prediction (NSP)\n",
    "- Build classification and token classification heads\n",
    "- Understand bidirectional vs causal attention\n",
    "- Fine-tune BERT for downstream tasks\n",
    "\n",
    "**BERT: Bidirectional Encoder Representations from Transformers**\n",
    "(Devlin et al., 2018)\n",
    "\n",
    "BERT revolutionized NLP by showing that pre-training bidirectional representations \n",
    "leads to massive improvements on downstream tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcaef65",
   "metadata": {},
   "source": [
    "## 5.1 BERT Architecture\n",
    "\n",
    "### Key Innovation: Bidirectional Pre-training\n",
    "\n",
    "Before BERT, language models were either:\n",
    "- **Left-to-right** (GPT): Can only see previous tokens\n",
    "- **Shallow bidirectional** (ELMo): Separate left-to-right and right-to-left\n",
    "\n",
    "BERT uses **deep bidirectional** attention - every token can attend to every other token.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "BERT is simply a **stack of Transformer encoder blocks**:\n",
    "\n",
    "```\n",
    "Input: [CLS] Token1 Token2 ... TokenN [SEP]\n",
    "         â”‚      â”‚      â”‚          â”‚     â”‚\n",
    "         â–¼      â–¼      â–¼          â–¼     â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚         Embedding Layer             â”‚\n",
    "    â”‚   (Token + Position + Segment)      â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚      â”‚      â”‚          â”‚     â”‚\n",
    "         â–¼      â–¼      â–¼          â–¼     â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚      Transformer Encoder Ã— N        â”‚\n",
    "    â”‚   (Bidirectional Self-Attention)    â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚      â”‚      â”‚          â”‚     â”‚\n",
    "         â–¼      â–¼      â–¼          â–¼     â–¼\n",
    "    [CLS]  H1    H2   ...       HN   [SEP]\n",
    "      â”‚\n",
    "      â–¼\n",
    "  Classification\n",
    "```\n",
    "\n",
    "### BERT Configurations\n",
    "\n",
    "| Model | Layers | Hidden | Heads | Params |\n",
    "|-------|--------|--------|-------|--------|\n",
    "| BERT-Base | 12 | 768 | 12 | 110M |\n",
    "| BERT-Large | 24 | 1024 | 16 | 340M |\n",
    "\n",
    "### Special Tokens\n",
    "\n",
    "- **[CLS]**: Classification token (first position)\n",
    "- **[SEP]**: Separator between sentences\n",
    "- **[MASK]**: Placeholder for masked tokens\n",
    "- **[PAD]**: Padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bedfad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BERT MODEL IMPLEMENTATION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"BERT ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Embeddings: Token + Position + Segment embeddings.\n",
    "    \n",
    "    Unlike the original transformer, BERT adds segment embeddings\n",
    "    to distinguish between sentence A and sentence B.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int,\n",
    "        max_len: int = 512,\n",
    "        n_segments: int = 2,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_len, d_model)\n",
    "        self.segment_embedding = nn.Embedding(n_segments, d_model)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Register position IDs as buffer\n",
    "        self.register_buffer(\n",
    "            'position_ids',\n",
    "            torch.arange(max_len).unsqueeze(0)\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        segment_ids: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Token IDs (batch, seq_len)\n",
    "            segment_ids: Segment IDs (batch, seq_len), 0 for sentence A, 1 for B\n",
    "        Returns:\n",
    "            embeddings: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        seq_len = input_ids.size(1)\n",
    "        \n",
    "        # Token embeddings\n",
    "        token_emb = self.token_embedding(input_ids)\n",
    "        \n",
    "        # Position embeddings\n",
    "        position_ids = self.position_ids[:, :seq_len]\n",
    "        position_emb = self.position_embedding(position_ids)\n",
    "        \n",
    "        # Segment embeddings (default to all zeros = sentence A)\n",
    "        if segment_ids is None:\n",
    "            segment_ids = torch.zeros_like(input_ids)\n",
    "        segment_emb = self.segment_embedding(segment_ids)\n",
    "        \n",
    "        # Combine\n",
    "        embeddings = token_emb + position_emb + segment_emb\n",
    "        embeddings = self.norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    \"\"\"Stack of Transformer encoder blocks.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        n_layers: int,\n",
    "        d_ff: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT: Bidirectional Encoder Representations from Transformers.\n",
    "    \n",
    "    This is the base BERT model without task-specific heads.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 768,\n",
    "        n_heads: int = 12,\n",
    "        n_layers: int = 12,\n",
    "        d_ff: int = 3072,\n",
    "        max_len: int = 512,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embeddings = BertEmbeddings(vocab_size, d_model, max_len, dropout=dropout)\n",
    "        self.encoder = BertEncoder(d_model, n_heads, n_layers, d_ff, dropout)\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        segment_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Token IDs (batch, seq_len)\n",
    "            segment_ids: Segment IDs for sentence pairs\n",
    "            attention_mask: Mask for padding (1 = attend, 0 = ignore)\n",
    "        Returns:\n",
    "            hidden_states: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Get embeddings\n",
    "        x = self.embeddings(input_ids, segment_ids)\n",
    "        \n",
    "        # Encode\n",
    "        x = self.encoder(x, mask=attention_mask)\n",
    "        \n",
    "        # Final norm\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test BERT\n",
    "print(\"\\nðŸ“Œ Testing BERT Model\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Small BERT for testing\n",
    "bert = BERT(\n",
    "    vocab_size=30000,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    n_layers=4,\n",
    "    d_ff=1024,\n",
    "    max_len=128\n",
    ")\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 20\n",
    "\n",
    "input_ids = torch.randint(0, 30000, (batch_size, seq_len))\n",
    "segment_ids = torch.zeros_like(input_ids)\n",
    "segment_ids[:, 10:] = 1  # Second half is sentence B\n",
    "\n",
    "output = bert(input_ids, segment_ids)\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in bert.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c97094",
   "metadata": {},
   "source": [
    "## 5.2 Masked Language Modeling (MLM)\n",
    "\n",
    "### The MLM Pre-training Objective\n",
    "\n",
    "BERT's key innovation is **Masked Language Modeling**:\n",
    "\n",
    "1. Randomly select 15% of tokens\n",
    "2. Of those selected:\n",
    "   - 80%: Replace with [MASK]\n",
    "   - 10%: Replace with random token\n",
    "   - 10%: Keep unchanged\n",
    "3. Predict the original tokens\n",
    "\n",
    "### Why This Masking Strategy?\n",
    "\n",
    "- **80% [MASK]**: Main signal for learning\n",
    "- **10% random**: Prevents model from assuming [MASK] always means \"predict here\"\n",
    "- **10% unchanged**: Teaches model that any token might need prediction\n",
    "\n",
    "### MLM Loss\n",
    "\n",
    "$\\mathcal{L}_{MLM} = -\\sum_{i \\in \\text{masked}} \\log P(x_i | \\mathbf{x}_{\\backslash i})$\n",
    "\n",
    "Only compute loss on masked positions (not all tokens).\n",
    "\n",
    "### Bidirectional Context\n",
    "\n",
    "Unlike GPT (left-to-right), BERT sees context from BOTH directions:\n",
    "\n",
    "```\n",
    "GPT:  \"The cat sat on the [?]\"  â†’ only sees left context\n",
    "BERT: \"The cat sat on the [MASK]\" â†’ sees \"The cat sat on the\" AND nothing after\n",
    "      \"The [MASK] sat on the mat\" â†’ sees \"The\" AND \"sat on the mat\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MASKED LANGUAGE MODELING\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"MASKED LANGUAGE MODELING (MLM)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class MLMHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked Language Modeling head for BERT.\n",
    "    \n",
    "    Predicts original tokens at masked positions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.GELU()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            logits: (batch, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        x = self.dense(hidden_states)\n",
    "        x = self.activation(x)\n",
    "        x = self.norm(x)\n",
    "        logits = self.decoder(x)\n",
    "        return logits\n",
    "\n",
    "def create_mlm_masks(\n",
    "    input_ids: torch.Tensor,\n",
    "    vocab_size: int,\n",
    "    mask_token_id: int,\n",
    "    special_token_ids: set,\n",
    "    mlm_probability: float = 0.15\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Create MLM masks following BERT's strategy.\n",
    "    \n",
    "    Args:\n",
    "        input_ids: Original token IDs (batch, seq_len)\n",
    "        vocab_size: Size of vocabulary\n",
    "        mask_token_id: ID of [MASK] token\n",
    "        special_token_ids: Set of special token IDs to never mask\n",
    "        mlm_probability: Probability of masking a token\n",
    "    \n",
    "    Returns:\n",
    "        masked_input_ids: Input with masks applied\n",
    "        labels: Original tokens at masked positions, -100 elsewhere\n",
    "        mask_positions: Boolean mask of which positions are masked\n",
    "    \"\"\"\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create probability matrix\n",
    "    probability_matrix = torch.full(input_ids.shape, mlm_probability)\n",
    "    \n",
    "    # Don't mask special tokens\n",
    "    for special_id in special_token_ids:\n",
    "        probability_matrix.masked_fill_(input_ids == special_id, 0.0)\n",
    "    \n",
    "    # Sample which tokens to mask\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    \n",
    "    # Set labels to -100 for non-masked tokens (ignored in loss)\n",
    "    labels[~masked_indices] = -100\n",
    "    \n",
    "    # 80% of masked tokens -> [MASK]\n",
    "    indices_replaced = torch.bernoulli(torch.full(input_ids.shape, 0.8)).bool() & masked_indices\n",
    "    input_ids_masked = input_ids.clone()\n",
    "    input_ids_masked[indices_replaced] = mask_token_id\n",
    "    \n",
    "    # 10% of masked tokens -> random token\n",
    "    indices_random = torch.bernoulli(torch.full(input_ids.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_tokens = torch.randint(vocab_size, input_ids.shape, dtype=input_ids.dtype)\n",
    "    input_ids_masked[indices_random] = random_tokens[indices_random]\n",
    "    \n",
    "    # 10% remain unchanged (already handled - we just don't modify them)\n",
    "    \n",
    "    return input_ids_masked, labels, masked_indices\n",
    "\n",
    "# Demonstrate MLM masking\n",
    "print(\"\\nðŸ“Œ MLM Masking Strategy\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Simulate a sentence\n",
    "vocab_size = 1000\n",
    "mask_token_id = 103  # [MASK]\n",
    "special_tokens = {0, 101, 102}  # [PAD], [CLS], [SEP]\n",
    "\n",
    "# Create sample input: [CLS] token1 token2 ... tokenN [SEP]\n",
    "input_ids = torch.tensor([[101, 45, 67, 89, 123, 456, 789, 234, 567, 102]])\n",
    "\n",
    "masked_ids, labels, mask_positions = create_mlm_masks(\n",
    "    input_ids, vocab_size, mask_token_id, special_tokens, mlm_probability=0.3\n",
    ")\n",
    "\n",
    "print(f\"Original:     {input_ids[0].tolist()}\")\n",
    "print(f\"Masked input: {masked_ids[0].tolist()}\")\n",
    "print(f\"Labels:       {labels[0].tolist()}\")\n",
    "print(f\"Mask positions: {mask_positions[0].tolist()}\")\n",
    "print(f\"\\n[MASK] token ID: {mask_token_id}\")\n",
    "print(\"Labels show -100 for non-masked positions (ignored in loss)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7106dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BERT FOR MLM TRAINING\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BERT FOR MLM PRE-TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class BertForMLM(nn.Module):\n",
    "    \"\"\"BERT with MLM head for pre-training.\"\"\"\n",
    "    \n",
    "    def __init__(self, bert: BERT, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.mlm_head = MLMHead(bert.d_model, vocab_size)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        segment_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Masked input tokens\n",
    "            labels: Original tokens at masked positions\n",
    "        Returns:\n",
    "            dict with 'logits' and optionally 'loss'\n",
    "        \"\"\"\n",
    "        hidden_states = self.bert(input_ids, segment_ids, attention_mask)\n",
    "        logits = self.mlm_head(hidden_states)\n",
    "        \n",
    "        output = {'logits': logits}\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()  # Ignores -100 by default\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            output['loss'] = loss\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test MLM training\n",
    "print(\"\\nðŸ“Œ Testing MLM Forward Pass\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "vocab_size = 30000\n",
    "bert = BERT(vocab_size=vocab_size, d_model=256, n_heads=4, n_layers=2, d_ff=512)\n",
    "bert_mlm = BertForMLM(bert, vocab_size)\n",
    "\n",
    "# Create masked input\n",
    "input_ids = torch.randint(100, vocab_size, (2, 20))\n",
    "masked_ids, labels, _ = create_mlm_masks(\n",
    "    input_ids, vocab_size, mask_token_id=103, \n",
    "    special_token_ids={0, 101, 102}\n",
    ")\n",
    "\n",
    "output = bert_mlm(masked_ids, labels=labels)\n",
    "\n",
    "print(f\"Input shape: {masked_ids.shape}\")\n",
    "print(f\"Logits shape: {output['logits'].shape}\")\n",
    "print(f\"MLM Loss: {output['loss'].item():.4f}\")\n",
    "\n",
    "# Show prediction at a masked position\n",
    "masked_pos = (labels[0] != -100).nonzero(as_tuple=True)[0]\n",
    "if len(masked_pos) > 0:\n",
    "    pos = masked_pos[0].item()\n",
    "    pred = output['logits'][0, pos].argmax().item()\n",
    "    true = labels[0, pos].item()\n",
    "    print(f\"\\nAt position {pos}:\")\n",
    "    print(f\"  True token: {true}\")\n",
    "    print(f\"  Predicted:  {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaf5adc",
   "metadata": {},
   "source": [
    "## 5.3 Next Sentence Prediction (NSP)\n",
    "\n",
    "### The NSP Task\n",
    "\n",
    "BERT's second pre-training objective:\n",
    "- Given two sentences A and B\n",
    "- Predict: Is B the actual next sentence after A?\n",
    "\n",
    "### Training Data\n",
    "\n",
    "- **50% positive**: B is the actual next sentence\n",
    "- **50% negative**: B is a random sentence from corpus\n",
    "\n",
    "### Input Format\n",
    "\n",
    "```\n",
    "[CLS] Sentence A tokens [SEP] Sentence B tokens [SEP]\n",
    "  0    0  0  0  0  0  0   0    1  1  1  1  1  1   1   <- Segment IDs\n",
    "```\n",
    "\n",
    "### NSP Loss\n",
    "\n",
    "Binary classification using the [CLS] token representation:\n",
    "\n",
    "$\\mathcal{L}_{NSP} = -\\log P(\\text{IsNext} | \\mathbf{h}_{[CLS]})$\n",
    "\n",
    "### Controversy\n",
    "\n",
    "Later work (RoBERTa, ALBERT) showed NSP may not be necessary:\n",
    "- Removing NSP often improves downstream performance\n",
    "- MLM alone is sufficient for learning good representations\n",
    "\n",
    "However, NSP helps for tasks requiring sentence-pair understanding (QA, NLI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# NEXT SENTENCE PREDICTION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"NEXT SENTENCE PREDICTION (NSP)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class NSPHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Next Sentence Prediction head for BERT.\n",
    "    \n",
    "    Binary classification: Is sentence B the next sentence after A?\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(d_model, 2)  # Binary: IsNext / NotNext\n",
    "    \n",
    "    def forward(self, pooled_output: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pooled_output: [CLS] token representation (batch, d_model)\n",
    "        Returns:\n",
    "            logits: (batch, 2)\n",
    "        \"\"\"\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    \"\"\"\n",
    "    Pool the [CLS] token for classification tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            pooled: (batch, d_model) - [CLS] representation\n",
    "        \"\"\"\n",
    "        # Take [CLS] token (first position)\n",
    "        cls_token = hidden_states[:, 0]\n",
    "        pooled = self.dense(cls_token)\n",
    "        pooled = self.activation(pooled)\n",
    "        return pooled\n",
    "\n",
    "class BertForPreTraining(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT with both MLM and NSP heads for pre-training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bert: BERT, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.pooler = BertPooler(bert.d_model)\n",
    "        self.mlm_head = MLMHead(bert.d_model, vocab_size)\n",
    "        self.nsp_head = NSPHead(bert.d_model)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        segment_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        mlm_labels: Optional[torch.Tensor] = None,\n",
    "        nsp_labels: Optional[torch.Tensor] = None\n",
    "    ) -> dict:\n",
    "        hidden_states = self.bert(input_ids, segment_ids, attention_mask)\n",
    "        \n",
    "        # MLM predictions (all positions)\n",
    "        mlm_logits = self.mlm_head(hidden_states)\n",
    "        \n",
    "        # NSP prediction ([CLS] token)\n",
    "        pooled = self.pooler(hidden_states)\n",
    "        nsp_logits = self.nsp_head(pooled)\n",
    "        \n",
    "        output = {\n",
    "            'mlm_logits': mlm_logits,\n",
    "            'nsp_logits': nsp_logits,\n",
    "            'pooled_output': pooled\n",
    "        }\n",
    "        \n",
    "        # Compute losses if labels provided\n",
    "        if mlm_labels is not None:\n",
    "            mlm_loss = F.cross_entropy(\n",
    "                mlm_logits.view(-1, mlm_logits.size(-1)),\n",
    "                mlm_labels.view(-1)\n",
    "            )\n",
    "            output['mlm_loss'] = mlm_loss\n",
    "        \n",
    "        if nsp_labels is not None:\n",
    "            nsp_loss = F.cross_entropy(nsp_logits, nsp_labels)\n",
    "            output['nsp_loss'] = nsp_loss\n",
    "        \n",
    "        if mlm_labels is not None and nsp_labels is not None:\n",
    "            output['loss'] = output['mlm_loss'] + output['nsp_loss']\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test pre-training model\n",
    "print(\"\\nðŸ“Œ Testing BERT Pre-training\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "vocab_size = 30000\n",
    "bert = BERT(vocab_size=vocab_size, d_model=256, n_heads=4, n_layers=2, d_ff=512)\n",
    "bert_pretrain = BertForPreTraining(bert, vocab_size)\n",
    "\n",
    "# Create sample data\n",
    "batch_size = 4\n",
    "seq_len = 30\n",
    "\n",
    "input_ids = torch.randint(100, vocab_size, (batch_size, seq_len))\n",
    "segment_ids = torch.zeros_like(input_ids)\n",
    "segment_ids[:, 15:] = 1  # Second half is sentence B\n",
    "\n",
    "# MLM labels\n",
    "masked_ids, mlm_labels, _ = create_mlm_masks(\n",
    "    input_ids, vocab_size, 103, {0, 101, 102}\n",
    ")\n",
    "\n",
    "# NSP labels (0 = IsNext, 1 = NotNext)\n",
    "nsp_labels = torch.randint(0, 2, (batch_size,))\n",
    "\n",
    "output = bert_pretrain(masked_ids, segment_ids, mlm_labels=mlm_labels, nsp_labels=nsp_labels)\n",
    "\n",
    "print(f\"MLM logits shape: {output['mlm_logits'].shape}\")\n",
    "print(f\"NSP logits shape: {output['nsp_logits'].shape}\")\n",
    "print(f\"MLM Loss: {output['mlm_loss'].item():.4f}\")\n",
    "print(f\"NSP Loss: {output['nsp_loss'].item():.4f}\")\n",
    "print(f\"Total Loss: {output['loss'].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a6e04",
   "metadata": {},
   "source": [
    "## 5.4 Sequence Classification with BERT\n",
    "\n",
    "### Using [CLS] for Classification\n",
    "\n",
    "The [CLS] token aggregates information from the entire sequence:\n",
    "\n",
    "```\n",
    "[CLS] This movie was great! [SEP]\n",
    "  â”‚\n",
    "  â–¼\n",
    "BERT Encoder\n",
    "  â”‚\n",
    "  â–¼\n",
    "[CLS] representation â†’ Linear â†’ Softmax â†’ Sentiment\n",
    "```\n",
    "\n",
    "### Fine-tuning Process\n",
    "\n",
    "1. Initialize with pre-trained BERT weights\n",
    "2. Add task-specific classification head\n",
    "3. Fine-tune entire model on labeled data\n",
    "\n",
    "### Common Classification Tasks\n",
    "\n",
    "| Task | Input | Output |\n",
    "|------|-------|--------|\n",
    "| Sentiment | Single sentence | Positive/Negative |\n",
    "| NLI | Sentence pair | Entailment/Contradiction/Neutral |\n",
    "| QA | Question + Context | Answer span |\n",
    "| Topic | Document | Category |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BERT FOR SEQUENCE CLASSIFICATION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"BERT FOR SEQUENCE CLASSIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class BertForSequenceClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT with a classification head for tasks like sentiment analysis.\n",
    "    \n",
    "    Uses the [CLS] token representation for classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bert: BERT, num_classes: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.pooler = BertPooler(bert.d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(bert.d_model, num_classes)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        segment_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Token IDs (batch, seq_len)\n",
    "            labels: Class labels (batch,)\n",
    "        Returns:\n",
    "            dict with 'logits' and optionally 'loss'\n",
    "        \"\"\"\n",
    "        hidden_states = self.bert(input_ids, segment_ids, attention_mask)\n",
    "        pooled = self.pooler(hidden_states)\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        output = {'logits': logits}\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            output['loss'] = loss\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test classification\n",
    "print(\"\\nðŸ“Œ Testing Sequence Classification\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "vocab_size = 30000\n",
    "num_classes = 3  # e.g., Positive, Negative, Neutral\n",
    "\n",
    "bert = BERT(vocab_size=vocab_size, d_model=256, n_heads=4, n_layers=2, d_ff=512)\n",
    "classifier = BertForSequenceClassification(bert, num_classes)\n",
    "\n",
    "# Sample input\n",
    "batch_size = 4\n",
    "seq_len = 20\n",
    "input_ids = torch.randint(100, vocab_size, (batch_size, seq_len))\n",
    "labels = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "output = classifier(input_ids, labels=labels)\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Logits shape: {output['logits'].shape}\")\n",
    "print(f\"Loss: {output['loss'].item():.4f}\")\n",
    "\n",
    "# Predictions\n",
    "predictions = output['logits'].argmax(dim=-1)\n",
    "print(f\"\\nPredictions: {predictions.tolist()}\")\n",
    "print(f\"True labels: {labels.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd7b97a",
   "metadata": {},
   "source": [
    "## 5.5 Token Classification (NER)\n",
    "\n",
    "### Per-Token Predictions\n",
    "\n",
    "Unlike sequence classification, token classification predicts a label for EACH token:\n",
    "\n",
    "```\n",
    "Input:  [CLS] John works at Google [SEP]\n",
    "Labels:   O   B-PER  O    O  B-ORG   O\n",
    "\n",
    "B-PER = Beginning of Person entity\n",
    "B-ORG = Beginning of Organization entity\n",
    "O = Outside (not an entity)\n",
    "```\n",
    "\n",
    "### Named Entity Recognition (NER)\n",
    "\n",
    "Common entity types:\n",
    "- **PER**: Person names\n",
    "- **ORG**: Organizations\n",
    "- **LOC**: Locations\n",
    "- **DATE**: Dates\n",
    "- **MISC**: Miscellaneous\n",
    "\n",
    "### BIO Tagging Scheme\n",
    "\n",
    "- **B-X**: Beginning of entity type X\n",
    "- **I-X**: Inside entity type X (continuation)\n",
    "- **O**: Outside any entity\n",
    "\n",
    "Example:\n",
    "```\n",
    "\"New York City is great\"\n",
    " B-LOC I-LOC I-LOC O O\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1349a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BERT FOR TOKEN CLASSIFICATION (NER)\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"BERT FOR TOKEN CLASSIFICATION (NER)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class BertForTokenClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT with a token classification head for tasks like NER.\n",
    "    \n",
    "    Predicts a label for each token in the sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bert: BERT, num_labels: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(bert.d_model, num_labels)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        segment_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Token IDs (batch, seq_len)\n",
    "            labels: Per-token labels (batch, seq_len)\n",
    "        Returns:\n",
    "            dict with 'logits' and optionally 'loss'\n",
    "        \"\"\"\n",
    "        hidden_states = self.bert(input_ids, segment_ids, attention_mask)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        logits = self.classifier(hidden_states)\n",
    "        \n",
    "        output = {'logits': logits}\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                labels.view(-1),\n",
    "                ignore_index=-100  # Ignore padding\n",
    "            )\n",
    "            output['loss'] = loss\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test token classification\n",
    "print(\"\\nðŸ“Œ Testing Token Classification (NER)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# NER labels: O, B-PER, I-PER, B-ORG, I-ORG, B-LOC, I-LOC\n",
    "label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "num_labels = len(label_names)\n",
    "\n",
    "vocab_size = 30000\n",
    "bert = BERT(vocab_size=vocab_size, d_model=256, n_heads=4, n_layers=2, d_ff=512)\n",
    "ner_model = BertForTokenClassification(bert, num_labels)\n",
    "\n",
    "# Sample input\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "input_ids = torch.randint(100, vocab_size, (batch_size, seq_len))\n",
    "labels = torch.randint(0, num_labels, (batch_size, seq_len))\n",
    "\n",
    "output = ner_model(input_ids, labels=labels)\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Logits shape: {output['logits'].shape}\")\n",
    "print(f\"Loss: {output['loss'].item():.4f}\")\n",
    "\n",
    "# Show predictions for first sequence\n",
    "predictions = output['logits'][0].argmax(dim=-1)\n",
    "print(f\"\\nFirst sequence predictions:\")\n",
    "print(f\"  Predicted: {[label_names[p] for p in predictions.tolist()]}\")\n",
    "print(f\"  True:      {[label_names[l] for l in labels[0].tolist()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f0d3f",
   "metadata": {},
   "source": [
    "## 5.6 Bidirectional vs Causal Attention\n",
    "\n",
    "### The Key Difference\n",
    "\n",
    "| Aspect | BERT (Bidirectional) | GPT (Causal) |\n",
    "|--------|---------------------|--------------|\n",
    "| Attention | Full attention matrix | Lower triangular |\n",
    "| Context | Both left and right | Only left |\n",
    "| Pre-training | MLM (predict masked) | LM (predict next) |\n",
    "| Best for | Understanding | Generation |\n",
    "\n",
    "### Attention Patterns\n",
    "\n",
    "**Bidirectional (BERT):**\n",
    "```\n",
    "     T1  T2  T3  T4\n",
    "T1 [ 1   1   1   1 ]  â† T1 sees all tokens\n",
    "T2 [ 1   1   1   1 ]  â† T2 sees all tokens\n",
    "T3 [ 1   1   1   1 ]  â† T3 sees all tokens\n",
    "T4 [ 1   1   1   1 ]  â† T4 sees all tokens\n",
    "```\n",
    "\n",
    "**Causal (GPT):**\n",
    "```\n",
    "     T1  T2  T3  T4\n",
    "T1 [ 1   0   0   0 ]  â† T1 sees only T1\n",
    "T2 [ 1   1   0   0 ]  â† T2 sees T1, T2\n",
    "T3 [ 1   1   1   0 ]  â† T3 sees T1, T2, T3\n",
    "T4 [ 1   1   1   1 ]  â† T4 sees all\n",
    "```\n",
    "\n",
    "### Why BERT Can't Generate\n",
    "\n",
    "BERT sees the entire sequence during training, so it can't generate autoregressively:\n",
    "- At generation time, future tokens don't exist\n",
    "- BERT would need to see [MASK] tokens and fill them in\n",
    "- This is inefficient and doesn't work well for long generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488c105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZE BIDIRECTIONAL VS CAUSAL ATTENTION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"BIDIRECTIONAL VS CAUSAL ATTENTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def visualize_attention_patterns():\n",
    "    seq_len = 6\n",
    "    tokens = ['[CLS]', 'The', 'cat', 'sat', 'down', '[SEP]']\n",
    "    \n",
    "    # Bidirectional attention (BERT)\n",
    "    bidirectional = torch.ones(seq_len, seq_len)\n",
    "    \n",
    "    # Causal attention (GPT)\n",
    "    causal = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # BERT attention\n",
    "    ax = axes[0]\n",
    "    im = ax.imshow(bidirectional.numpy(), cmap='Blues', aspect='auto')\n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(tokens)\n",
    "    ax.set_xlabel('Key (attending to)')\n",
    "    ax.set_ylabel('Query (from)')\n",
    "    ax.set_title('BERT: Bidirectional Attention\\n(Each token sees ALL tokens)')\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            ax.text(j, i, 'âœ“', ha='center', va='center', fontsize=12, color='darkblue')\n",
    "    \n",
    "    # GPT attention\n",
    "    ax = axes[1]\n",
    "    im = ax.imshow(causal.numpy(), cmap='Oranges', aspect='auto')\n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(tokens)\n",
    "    ax.set_xlabel('Key (attending to)')\n",
    "    ax.set_ylabel('Query (from)')\n",
    "    ax.set_title('GPT: Causal Attention\\n(Each token sees only PAST tokens)')\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if causal[i, j] == 1:\n",
    "                ax.text(j, i, 'âœ“', ha='center', va='center', fontsize=12, color='darkorange')\n",
    "            else:\n",
    "                ax.text(j, i, 'âœ—', ha='center', va='center', fontsize=12, color='gray')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention_patterns()\n",
    "\n",
    "print(\"\\nðŸ“Œ Key Insight:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"BERT: 'cat' can see 'sat', 'down' â†’ understands full context\")\n",
    "print(\"GPT:  'cat' cannot see 'sat', 'down' â†’ must predict without future\")\n",
    "print(\"\\nThis is why BERT excels at understanding tasks (classification, QA)\")\n",
    "print(\"while GPT excels at generation tasks (text completion, chat)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6b497",
   "metadata": {},
   "source": [
    "## 5.7 Fine-tuning BERT for Sentiment Classification\n",
    "\n",
    "### Fine-tuning Strategy\n",
    "\n",
    "1. **Load pre-trained weights**: Start with BERT trained on MLM/NSP\n",
    "2. **Add task head**: Classification layer on top\n",
    "3. **Fine-tune all layers**: Update entire model on task data\n",
    "4. **Use lower learning rate**: Typically 2e-5 to 5e-5\n",
    "\n",
    "### Training Tips\n",
    "\n",
    "- **Warmup**: Gradually increase LR at start\n",
    "- **Epochs**: Usually 2-4 epochs is enough\n",
    "- **Batch size**: 16-32 works well\n",
    "- **Max length**: 128-512 depending on task\n",
    "\n",
    "### Why Fine-tuning Works\n",
    "\n",
    "Pre-training teaches:\n",
    "- Syntax and grammar\n",
    "- Word meanings and relationships\n",
    "- Common patterns in language\n",
    "\n",
    "Fine-tuning adapts this knowledge to specific tasks with minimal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325fc9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FINE-TUNING BERT ON SENTIMENT DATA\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"FINE-TUNING BERT FOR SENTIMENT CLASSIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create synthetic sentiment dataset\n",
    "def create_sentiment_data(n_samples: int, seq_len: int, vocab_size: int):\n",
    "    \"\"\"\n",
    "    Create synthetic sentiment data.\n",
    "    In practice, you'd use real tokenized text.\n",
    "    \"\"\"\n",
    "    # Random token IDs (simulating tokenized text)\n",
    "    input_ids = torch.randint(100, vocab_size, (n_samples, seq_len))\n",
    "    \n",
    "    # Add [CLS] at start and [SEP] at end\n",
    "    input_ids[:, 0] = 101  # [CLS]\n",
    "    input_ids[:, -1] = 102  # [SEP]\n",
    "    \n",
    "    # Random labels: 0 = negative, 1 = neutral, 2 = positive\n",
    "    labels = torch.randint(0, 3, (n_samples,))\n",
    "    \n",
    "    return input_ids, labels\n",
    "\n",
    "# Create model\n",
    "vocab_size = 30000\n",
    "num_classes = 3\n",
    "\n",
    "bert = BERT(vocab_size=vocab_size, d_model=128, n_heads=4, n_layers=2, d_ff=256)\n",
    "model = BertForSequenceClassification(bert, num_classes)\n",
    "\n",
    "# Create data\n",
    "n_train = 200\n",
    "n_test = 50\n",
    "seq_len = 32\n",
    "\n",
    "train_ids, train_labels = create_sentiment_data(n_train, seq_len, vocab_size)\n",
    "test_ids, test_labels = create_sentiment_data(n_test, seq_len, vocab_size)\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "n_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Training samples: {n_train}\")\n",
    "print(f\"Test samples: {n_test}\")\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nðŸ“Œ Training...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "model.train()\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for i in range(0, n_train, batch_size):\n",
    "        batch_ids = train_ids[i:i+batch_size]\n",
    "        batch_labels = train_labels[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_ids, labels=batch_labels)\n",
    "        loss = output['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nðŸ“Œ Evaluation\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(test_ids)\n",
    "    predictions = output['logits'].argmax(dim=-1)\n",
    "    accuracy = (predictions == test_labels).float().mean()\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy.item():.2%}\")\n",
    "print(\"(Note: Random baseline = 33.3% for 3 classes)\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Fine-tuning Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23dce87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 5: Key Takeaways\n",
    "\n",
    "### BERT Architecture\n",
    "\n",
    "- **Encoder-only**: Stack of transformer encoder blocks\n",
    "- **Bidirectional**: Each token attends to all other tokens\n",
    "- **Special tokens**: [CLS] for classification, [SEP] for separation, [MASK] for MLM\n",
    "\n",
    "### Pre-training Objectives\n",
    "\n",
    "| Objective | Task | Purpose |\n",
    "|-----------|------|---------|\n",
    "| **MLM** | Predict masked tokens | Learn bidirectional representations |\n",
    "| **NSP** | Predict if B follows A | Learn sentence relationships |\n",
    "\n",
    "### MLM Masking Strategy\n",
    "\n",
    "- 15% of tokens selected for prediction\n",
    "- 80% â†’ [MASK], 10% â†’ random, 10% â†’ unchanged\n",
    "\n",
    "### Task-Specific Heads\n",
    "\n",
    "| Task | Head | Input |\n",
    "|------|------|-------|\n",
    "| Classification | Linear([CLS]) | [CLS] representation |\n",
    "| Token Classification | Linear(all tokens) | All token representations |\n",
    "| QA | Linear(start, end) | Token representations |\n",
    "\n",
    "### BERT vs GPT\n",
    "\n",
    "| Aspect | BERT | GPT |\n",
    "|--------|------|-----|\n",
    "| Architecture | Encoder-only | Decoder-only |\n",
    "| Attention | Bidirectional | Causal |\n",
    "| Pre-training | MLM + NSP | Next token prediction |\n",
    "| Strength | Understanding | Generation |\n",
    "\n",
    "### Fine-tuning Best Practices\n",
    "\n",
    "1. **Learning rate**: 2e-5 to 5e-5\n",
    "2. **Epochs**: 2-4 usually sufficient\n",
    "3. **Warmup**: 10% of training steps\n",
    "4. **Batch size**: 16-32\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "BERT showed that **pre-training + fine-tuning** is incredibly effective:\n",
    "- Pre-train once on large corpus\n",
    "- Fine-tune quickly on small task-specific data\n",
    "- Achieves state-of-the-art on many NLP tasks\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [GPT (Decoder-Only)](#module-6-gpt) - Autoregressive language modeling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3165d44e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-6-gpt'></a>\n",
    "# Module 6: GPT - Decoder-Only Architecture\n",
    "\n",
    "**Prerequisites:** Modules 1-5 (PyTorch, embeddings, attention, transformer, BERT)\n",
    "\n",
    "**Learning Objectives:**\n",
    "By the end of this module, you will be able to:\n",
    "- Understand the GPT architecture and autoregressive language modeling\n",
    "- Implement a minimal GPT model from scratch\n",
    "- Implement various text generation strategies\n",
    "- Train a small GPT on text data\n",
    "- Understand the differences between GPT-1, GPT-2, and GPT-3\n",
    "\n",
    "**The GPT Family**\n",
    "\n",
    "| Model | Year | Parameters | Key Innovation |\n",
    "|-------|------|------------|----------------|\n",
    "| GPT-1 | 2018 | 117M | Decoder-only pre-training |\n",
    "| GPT-2 | 2019 | 1.5B | Zero-shot task transfer |\n",
    "| GPT-3 | 2020 | 175B | In-context learning |\n",
    "| GPT-4 | 2023 | ~1T? | Multimodal, RLHF |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5dcd24",
   "metadata": {},
   "source": [
    "## 6.1 GPT Architecture\n",
    "\n",
    "### The Key Idea: Autoregressive Language Modeling\n",
    "\n",
    "GPT predicts the next token given all previous tokens:\n",
    "\n",
    "$P(x_1, x_2, ..., x_n) = \\prod_{i=1}^{n} P(x_i | x_1, ..., x_{i-1})$\n",
    "\n",
    "This is fundamentally different from BERT's masked prediction.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "GPT uses **decoder-only** transformer blocks (no encoder, no cross-attention):\n",
    "\n",
    "```\n",
    "Input: Token1 Token2 Token3 ... TokenN\n",
    "         â”‚      â”‚      â”‚          â”‚\n",
    "         â–¼      â–¼      â–¼          â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚      Token + Position           â”‚\n",
    "    â”‚         Embeddings              â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚      â”‚      â”‚          â”‚\n",
    "         â–¼      â–¼      â–¼          â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚   Transformer Decoder Block Ã— N â”‚\n",
    "    â”‚   (Causal Self-Attention only)  â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚      â”‚      â”‚          â”‚\n",
    "         â–¼      â–¼      â–¼          â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚         Layer Norm              â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚      â”‚      â”‚          â”‚\n",
    "         â–¼      â–¼      â–¼          â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚    Output Projection (vocab)    â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚      â”‚      â”‚          â”‚\n",
    "         â–¼      â–¼      â–¼          â–¼\n",
    "      P(T2)  P(T3)  P(T4)  ...  P(TN+1)\n",
    "```\n",
    "\n",
    "### GPT-2 Configuration\n",
    "\n",
    "| Parameter | Small | Medium | Large | XL |\n",
    "|-----------|-------|--------|-------|-----|\n",
    "| Layers | 12 | 24 | 36 | 48 |\n",
    "| d_model | 768 | 1024 | 1280 | 1600 |\n",
    "| Heads | 12 | 16 | 20 | 25 |\n",
    "| Params | 117M | 345M | 762M | 1.5B |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3a24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GPT MODEL IMPLEMENTATION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"GPT ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single GPT Decoder Block.\n",
    "    \n",
    "    Unlike the full transformer decoder, GPT blocks have:\n",
    "    - Causal self-attention (no cross-attention)\n",
    "    - Pre-norm architecture (LayerNorm before sublayers)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        d_ff: int = None,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        \n",
    "        # Causal self-attention\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # FFN\n",
    "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout, activation='gelu')\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        causal_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "            causal_mask: Lower triangular mask\n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Self-attention with residual (Pre-Norm)\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x, x, x, mask=causal_mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        # FFN with residual (Pre-Norm)\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT: Generative Pre-trained Transformer.\n",
    "    \n",
    "    Decoder-only transformer for autoregressive language modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 768,\n",
    "        n_heads: int = 12,\n",
    "        n_layers: int = 12,\n",
    "        d_ff: int = None,\n",
    "        max_len: int = 1024,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GPTBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output projection (tied with token embeddings in practice)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying (optional but common)\n",
    "        self.lm_head.weight = self.token_embedding.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def _create_causal_mask(self, seq_len: int, device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"Create causal attention mask.\"\"\"\n",
    "        return torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        labels: Optional[torch.Tensor] = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: Token IDs (batch, seq_len)\n",
    "            labels: Target token IDs for loss computation\n",
    "        Returns:\n",
    "            dict with 'logits' and optionally 'loss'\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        \n",
    "        # Get embeddings\n",
    "        token_emb = self.token_embedding(input_ids)\n",
    "        position_ids = torch.arange(seq_len, device=device).unsqueeze(0)\n",
    "        position_emb = self.position_embedding(position_ids)\n",
    "        \n",
    "        x = self.dropout(token_emb + position_emb)\n",
    "        \n",
    "        # Create causal mask\n",
    "        causal_mask = self._create_causal_mask(seq_len, device)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, causal_mask)\n",
    "        \n",
    "        # Final norm and projection\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        output = {'logits': logits}\n",
    "        \n",
    "        # Compute loss if labels provided\n",
    "        if labels is not None:\n",
    "            # Shift logits and labels for next-token prediction\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            \n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "            output['loss'] = loss\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test GPT\n",
    "print(\"\\nðŸ“Œ Testing GPT Model\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "gpt = GPT(\n",
    "    vocab_size=50257,  # GPT-2 vocab size\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    n_layers=4,\n",
    "    max_len=512\n",
    ")\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 20\n",
    "\n",
    "input_ids = torch.randint(0, 50257, (batch_size, seq_len))\n",
    "output = gpt(input_ids, labels=input_ids)\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Logits shape: {output['logits'].shape}\")\n",
    "print(f\"Loss: {output['loss'].item():.4f}\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in gpt.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed3ec0a",
   "metadata": {},
   "source": [
    "## 6.2 Autoregressive Text Generation\n",
    "\n",
    "### The Generation Process\n",
    "\n",
    "GPT generates text one token at a time:\n",
    "\n",
    "```\n",
    "Input:  \"The cat\"\n",
    "Step 1: P(next | \"The cat\") â†’ \"sat\"\n",
    "Step 2: P(next | \"The cat sat\") â†’ \"on\"\n",
    "Step 3: P(next | \"The cat sat on\") â†’ \"the\"\n",
    "...\n",
    "```\n",
    "\n",
    "### Generation Loop\n",
    "\n",
    "```python\n",
    "while not done:\n",
    "    logits = model(tokens)\n",
    "    next_token = sample(logits[:, -1, :])  # Only use last position\n",
    "    tokens = concat(tokens, next_token)\n",
    "```\n",
    "\n",
    "### Stopping Conditions\n",
    "\n",
    "1. **Max length**: Stop after N tokens\n",
    "2. **EOS token**: Stop when model generates end-of-sequence\n",
    "3. **Custom**: Stop on specific patterns (e.g., newline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917df036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# AUTOREGRESSIVE GENERATION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"AUTOREGRESSIVE TEXT GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_greedy(\n",
    "    model: GPT,\n",
    "    input_ids: torch.Tensor,\n",
    "    max_new_tokens: int = 50,\n",
    "    eos_token_id: Optional[int] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Greedy decoding: always pick the most likely next token.\n",
    "    \n",
    "    Args:\n",
    "        model: GPT model\n",
    "        input_ids: Starting tokens (batch, seq_len)\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        eos_token_id: Stop when this token is generated\n",
    "    \n",
    "    Returns:\n",
    "        Generated token IDs (batch, seq_len + new_tokens)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Get logits for current sequence\n",
    "        output = model(input_ids)\n",
    "        logits = output['logits']\n",
    "        \n",
    "        # Get logits for last position only\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        \n",
    "        # Greedy: take argmax\n",
    "        next_token = next_token_logits.argmax(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Append to sequence\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        # Check for EOS\n",
    "        if eos_token_id is not None and (next_token == eos_token_id).all():\n",
    "            break\n",
    "    \n",
    "    return input_ids\n",
    "\n",
    "# Test greedy generation\n",
    "print(\"\\nðŸ“Œ Testing Greedy Generation\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create a small GPT\n",
    "gpt = GPT(vocab_size=100, d_model=64, n_heads=2, n_layers=2, max_len=100)\n",
    "\n",
    "# Start with some tokens\n",
    "prompt = torch.tensor([[1, 5, 10, 15]])  # Batch of 1\n",
    "print(f\"Prompt: {prompt[0].tolist()}\")\n",
    "\n",
    "generated = generate_greedy(gpt, prompt, max_new_tokens=10)\n",
    "print(f\"Generated: {generated[0].tolist()}\")\n",
    "print(f\"New tokens: {generated[0, len(prompt[0]):].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0471fe",
   "metadata": {},
   "source": [
    "## 6.3 Sampling Strategies\n",
    "\n",
    "### The Problem with Greedy Decoding\n",
    "\n",
    "Greedy decoding always picks the most likely token, leading to:\n",
    "- Repetitive, boring text\n",
    "- Getting stuck in loops\n",
    "- Missing creative alternatives\n",
    "\n",
    "### Temperature Sampling\n",
    "\n",
    "Scale logits before softmax to control randomness:\n",
    "\n",
    "$P(x_i) = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}$\n",
    "\n",
    "- **T < 1**: Sharper distribution (more confident)\n",
    "- **T = 1**: Original distribution\n",
    "- **T > 1**: Flatter distribution (more random)\n",
    "\n",
    "### Top-k Sampling (Fan et al., 2018)\n",
    "\n",
    "Only sample from the k most likely tokens:\n",
    "\n",
    "1. Sort tokens by probability\n",
    "2. Keep only top k\n",
    "3. Renormalize and sample\n",
    "\n",
    "### Top-p (Nucleus) Sampling (Holtzman et al., 2019)\n",
    "\n",
    "Sample from smallest set of tokens whose cumulative probability â‰¥ p:\n",
    "\n",
    "1. Sort tokens by probability\n",
    "2. Find smallest set with cumulative prob â‰¥ p\n",
    "3. Renormalize and sample\n",
    "\n",
    "**Advantage**: Adapts to the distribution - uses more tokens when uncertain, fewer when confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8883a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SAMPLING STRATEGIES\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLING STRATEGIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def sample_with_temperature(logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample from logits with temperature scaling.\n",
    "    \n",
    "    Args:\n",
    "        logits: (batch, vocab_size)\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Sampled token IDs (batch, 1)\n",
    "    \"\"\"\n",
    "    if temperature == 0:\n",
    "        return logits.argmax(dim=-1, keepdim=True)\n",
    "    \n",
    "    scaled_logits = logits / temperature\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "def sample_top_k(logits: torch.Tensor, k: int, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Top-k sampling: only consider the k most likely tokens.\n",
    "    \n",
    "    Args:\n",
    "        logits: (batch, vocab_size)\n",
    "        k: Number of top tokens to consider\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Sampled token IDs (batch, 1)\n",
    "    \"\"\"\n",
    "    # Get top k logits and indices\n",
    "    top_k_logits, top_k_indices = torch.topk(logits, k, dim=-1)\n",
    "    \n",
    "    # Apply temperature and sample\n",
    "    scaled_logits = top_k_logits / temperature\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "    sampled_idx = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    # Map back to original vocabulary\n",
    "    return torch.gather(top_k_indices, -1, sampled_idx)\n",
    "\n",
    "def sample_top_p(logits: torch.Tensor, p: float, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Top-p (nucleus) sampling: sample from smallest set with cumulative prob >= p.\n",
    "    \n",
    "    Args:\n",
    "        logits: (batch, vocab_size)\n",
    "        p: Cumulative probability threshold\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Sampled token IDs (batch, 1)\n",
    "    \"\"\"\n",
    "    # Apply temperature\n",
    "    scaled_logits = logits / temperature\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "    \n",
    "    # Sort by probability (descending)\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "    \n",
    "    # Compute cumulative probabilities\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    \n",
    "    # Find cutoff: first position where cumulative prob >= p\n",
    "    # Shift by 1 to include the token that crosses the threshold\n",
    "    sorted_indices_to_remove = cumulative_probs > p\n",
    "    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "    sorted_indices_to_remove[:, 0] = False\n",
    "    \n",
    "    # Set removed tokens to 0 probability\n",
    "    sorted_probs[sorted_indices_to_remove] = 0\n",
    "    \n",
    "    # Renormalize\n",
    "    sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Sample from filtered distribution\n",
    "    sampled_idx = torch.multinomial(sorted_probs, num_samples=1)\n",
    "    \n",
    "    # Map back to original vocabulary\n",
    "    return torch.gather(sorted_indices, -1, sampled_idx)\n",
    "\n",
    "# Demonstrate sampling strategies\n",
    "print(\"\\nðŸ“Œ Comparing Sampling Strategies\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create example logits (simulating model output)\n",
    "torch.manual_seed(42)\n",
    "vocab_size = 100\n",
    "logits = torch.randn(1, vocab_size)\n",
    "\n",
    "# Make a few tokens much more likely\n",
    "logits[0, 5] = 5.0   # Very likely\n",
    "logits[0, 10] = 3.0  # Likely\n",
    "logits[0, 15] = 2.0  # Somewhat likely\n",
    "\n",
    "print(\"Sampling 10 tokens with each strategy:\\n\")\n",
    "\n",
    "# Greedy\n",
    "greedy_samples = [logits.argmax(dim=-1).item() for _ in range(10)]\n",
    "print(f\"Greedy:        {greedy_samples}\")\n",
    "print(\"  (Always picks the same token)\\n\")\n",
    "\n",
    "# Temperature sampling\n",
    "for temp in [0.5, 1.0, 2.0]:\n",
    "    samples = [sample_with_temperature(logits, temp).item() for _ in range(10)]\n",
    "    print(f\"Temp={temp}:      {samples}\")\n",
    "print(\"  (Higher temp = more variety)\\n\")\n",
    "\n",
    "# Top-k\n",
    "for k in [3, 10, 50]:\n",
    "    samples = [sample_top_k(logits, k).item() for _ in range(10)]\n",
    "    print(f\"Top-k={k}:      {samples}\")\n",
    "print(\"  (Smaller k = less variety)\\n\")\n",
    "\n",
    "# Top-p\n",
    "for p in [0.5, 0.9, 0.95]:\n",
    "    samples = [sample_top_p(logits, p).item() for _ in range(10)]\n",
    "    print(f\"Top-p={p}:    {samples}\")\n",
    "print(\"  (Smaller p = less variety)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZE SAMPLING DISTRIBUTIONS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VISUALIZING SAMPLING DISTRIBUTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create logits with clear structure\n",
    "vocab_size = 20\n",
    "logits = torch.tensor([5.0, 3.0, 2.0, 1.5, 1.0] + [0.0] * 15).unsqueeze(0)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Original distribution\n",
    "probs = F.softmax(logits, dim=-1)[0].numpy()\n",
    "ax = axes[0, 0]\n",
    "ax.bar(range(vocab_size), probs)\n",
    "ax.set_title('Original Distribution (T=1)')\n",
    "ax.set_xlabel('Token ID')\n",
    "ax.set_ylabel('Probability')\n",
    "\n",
    "# Temperature effects\n",
    "for idx, temp in enumerate([0.5, 2.0]):\n",
    "    scaled_probs = F.softmax(logits / temp, dim=-1)[0].numpy()\n",
    "    ax = axes[0, idx + 1]\n",
    "    ax.bar(range(vocab_size), scaled_probs)\n",
    "    ax.set_title(f'Temperature = {temp}')\n",
    "    ax.set_xlabel('Token ID')\n",
    "    ax.set_ylabel('Probability')\n",
    "\n",
    "# Top-k\n",
    "k = 3\n",
    "top_k_probs = probs.copy()\n",
    "top_k_probs[k:] = 0\n",
    "top_k_probs = top_k_probs / top_k_probs.sum()\n",
    "ax = axes[1, 0]\n",
    "ax.bar(range(vocab_size), top_k_probs)\n",
    "ax.set_title(f'Top-k (k={k})')\n",
    "ax.set_xlabel('Token ID')\n",
    "ax.set_ylabel('Probability')\n",
    "\n",
    "# Top-p\n",
    "p = 0.9\n",
    "sorted_probs = np.sort(probs)[::-1]\n",
    "cumsum = np.cumsum(sorted_probs)\n",
    "cutoff_idx = np.searchsorted(cumsum, p) + 1\n",
    "top_p_probs = probs.copy()\n",
    "threshold = sorted_probs[min(cutoff_idx, len(sorted_probs)-1)]\n",
    "top_p_probs[probs < threshold] = 0\n",
    "top_p_probs = top_p_probs / top_p_probs.sum()\n",
    "ax = axes[1, 1]\n",
    "ax.bar(range(vocab_size), top_p_probs)\n",
    "ax.set_title(f'Top-p (p={p})')\n",
    "ax.set_xlabel('Token ID')\n",
    "ax.set_ylabel('Probability')\n",
    "\n",
    "# Combined: Top-p + Temperature\n",
    "ax = axes[1, 2]\n",
    "combined_probs = F.softmax(logits / 0.8, dim=-1)[0].numpy()\n",
    "combined_probs[combined_probs < threshold] = 0\n",
    "combined_probs = combined_probs / combined_probs.sum()\n",
    "ax.bar(range(vocab_size), combined_probs)\n",
    "ax.set_title('Top-p + Temperature (common combo)')\n",
    "ax.set_xlabel('Token ID')\n",
    "ax.set_ylabel('Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… In practice, top-p=0.9 with temperature=0.7-0.9 works well for most tasks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5209c08",
   "metadata": {},
   "source": [
    "## 6.4 Complete Generation with All Options\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "A production-ready generation function combines:\n",
    "- Temperature scaling\n",
    "- Top-k filtering\n",
    "- Top-p (nucleus) filtering\n",
    "- Repetition penalty\n",
    "- Length penalty\n",
    "- Beam search (optional)\n",
    "\n",
    "### Repetition Penalty\n",
    "\n",
    "Reduce probability of tokens that already appeared:\n",
    "\n",
    "$\\text{logits}[t] = \\text{logits}[t] / \\theta$ if $t$ in generated tokens\n",
    "\n",
    "Where $\\theta > 1$ penalizes repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f077a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPLETE GENERATION FUNCTION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE TEXT GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: GPT,\n",
    "    input_ids: torch.Tensor,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    top_p: Optional[float] = None,\n",
    "    repetition_penalty: float = 1.0,\n",
    "    eos_token_id: Optional[int] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate text with various sampling strategies.\n",
    "    \n",
    "    Args:\n",
    "        model: GPT model\n",
    "        input_ids: Starting tokens (batch, seq_len)\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0 = greedy)\n",
    "        top_k: If set, only sample from top k tokens\n",
    "        top_p: If set, use nucleus sampling with this threshold\n",
    "        repetition_penalty: Penalty for repeating tokens (>1 = penalize)\n",
    "        eos_token_id: Stop when this token is generated\n",
    "    \n",
    "    Returns:\n",
    "        Generated token IDs\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    batch_size = input_ids.size(0)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Get logits\n",
    "        output = model(input_ids)\n",
    "        logits = output['logits'][:, -1, :]  # (batch, vocab_size)\n",
    "        \n",
    "        # Apply repetition penalty\n",
    "        if repetition_penalty != 1.0:\n",
    "            for batch_idx in range(batch_size):\n",
    "                for token_id in input_ids[batch_idx].unique():\n",
    "                    logits[batch_idx, token_id] /= repetition_penalty\n",
    "        \n",
    "        # Apply temperature\n",
    "        if temperature != 1.0 and temperature > 0:\n",
    "            logits = logits / temperature\n",
    "        \n",
    "        # Apply top-k filtering\n",
    "        if top_k is not None and top_k > 0:\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][:, -1, None]\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # Apply top-p filtering\n",
    "        if top_p is not None and top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            \n",
    "            # Remove tokens with cumulative prob above threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "            sorted_indices_to_remove[:, 0] = False\n",
    "            \n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                1, sorted_indices, sorted_indices_to_remove\n",
    "            )\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # Sample or greedy\n",
    "        if temperature == 0:\n",
    "            next_token = logits.argmax(dim=-1, keepdim=True)\n",
    "        else:\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Append to sequence\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        # Check for EOS\n",
    "        if eos_token_id is not None and (next_token == eos_token_id).all():\n",
    "            break\n",
    "    \n",
    "    return input_ids\n",
    "\n",
    "# Test complete generation\n",
    "print(\"\\nðŸ“Œ Testing Complete Generation\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "gpt = GPT(vocab_size=100, d_model=64, n_heads=2, n_layers=2, max_len=100)\n",
    "prompt = torch.tensor([[1, 5, 10]])\n",
    "\n",
    "print(\"Prompt:\", prompt[0].tolist())\n",
    "print()\n",
    "\n",
    "# Different configurations\n",
    "configs = [\n",
    "    {\"temperature\": 0, \"name\": \"Greedy\"},\n",
    "    {\"temperature\": 1.0, \"name\": \"T=1.0\"},\n",
    "    {\"temperature\": 0.7, \"top_p\": 0.9, \"name\": \"T=0.7, top_p=0.9\"},\n",
    "    {\"temperature\": 1.0, \"top_k\": 10, \"name\": \"T=1.0, top_k=10\"},\n",
    "    {\"temperature\": 0.8, \"repetition_penalty\": 1.2, \"name\": \"T=0.8, rep_pen=1.2\"},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    name = config.pop(\"name\")\n",
    "    generated = generate(gpt, prompt.clone(), max_new_tokens=15, **config)\n",
    "    print(f\"{name:30s}: {generated[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11621cd6",
   "metadata": {},
   "source": [
    "## 6.5 Training GPT on Text Data\n",
    "\n",
    "### Language Modeling Objective\n",
    "\n",
    "GPT is trained to predict the next token:\n",
    "\n",
    "$\\mathcal{L} = -\\sum_{t=1}^{T} \\log P(x_t | x_1, ..., x_{t-1})$\n",
    "\n",
    "### Training Data Preparation\n",
    "\n",
    "1. **Tokenize** text into token IDs\n",
    "2. **Chunk** into fixed-length sequences\n",
    "3. **Create labels** by shifting input by 1\n",
    "\n",
    "```\n",
    "Input:  [The, cat, sat, on, the]\n",
    "Labels: [cat, sat, on, the, mat]\n",
    "```\n",
    "\n",
    "### Training Tips\n",
    "\n",
    "- **Learning rate**: 6e-4 with warmup and cosine decay\n",
    "- **Batch size**: As large as memory allows\n",
    "- **Gradient clipping**: Max norm 1.0\n",
    "- **Weight decay**: 0.1 (but not on biases/LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRAINING GPT ON TEXT\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING GPT ON TEXT DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple character-level dataset\n",
    "text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "A journey of a thousand miles begins with a single step.\n",
    "To be or not to be, that is the question.\n",
    "All that glitters is not gold.\n",
    "The only thing we have to fear is fear itself.\n",
    "In the beginning was the Word.\n",
    "\"\"\" * 10  # Repeat for more data\n",
    "\n",
    "# Character-level tokenization\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "def encode(s):\n",
    "    return [char_to_idx[c] for c in s]\n",
    "\n",
    "def decode(ids):\n",
    "    return ''.join([idx_to_char[i] for i in ids])\n",
    "\n",
    "# Encode entire text\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Dataset size: {len(data)} characters\")\n",
    "print(f\"Sample characters: {chars[:20]}\")\n",
    "\n",
    "# Create training batches\n",
    "def get_batch(data, batch_size, seq_len):\n",
    "    \"\"\"Get a random batch of sequences.\"\"\"\n",
    "    ix = torch.randint(len(data) - seq_len - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# Create model\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=4,\n",
    "    max_len=128,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "batch_size = 32\n",
    "seq_len = 64\n",
    "n_steps = 500\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nðŸ“Œ Training...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # Get batch\n",
    "    x, y = get_batch(data, batch_size, seq_len)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x, labels=y)\n",
    "    loss = output['loss']\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (step + 1) % 100 == 0:\n",
    "        print(f\"Step {step + 1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7265d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GENERATE TEXT FROM TRAINED MODEL\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING TEXT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate from different prompts\n",
    "prompts = [\n",
    "    \"The \",\n",
    "    \"To be\",\n",
    "    \"In the\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for prompt_text in prompts:\n",
    "    prompt_ids = torch.tensor([encode(prompt_text)])\n",
    "    \n",
    "    generated_ids = generate(\n",
    "        model, \n",
    "        prompt_ids, \n",
    "        max_new_tokens=100,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    generated_text = decode(generated_ids[0].tolist())\n",
    "    \n",
    "    print(f\"\\nPrompt: '{prompt_text}'\")\n",
    "    print(f\"Generated: '{generated_text}'\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('GPT Training Loss (Character-Level)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… The model learned to generate text that resembles the training data!\")\n",
    "print(\"   With more data and training, it would generate more coherent text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17df30f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 6: Key Takeaways\n",
    "\n",
    "### GPT Architecture\n",
    "\n",
    "- **Decoder-only**: No encoder, no cross-attention\n",
    "- **Causal attention**: Each token only sees previous tokens\n",
    "- **Pre-norm**: LayerNorm before sublayers (GPT-2 onwards)\n",
    "- **Weight tying**: Output projection shares weights with embeddings\n",
    "\n",
    "### Autoregressive Language Modeling\n",
    "\n",
    "$P(x_1, ..., x_n) = \\prod_{i=1}^{n} P(x_i | x_1, ..., x_{i-1})$\n",
    "\n",
    "- Train to predict next token\n",
    "- Generate one token at a time\n",
    "- Can generate arbitrary length text\n",
    "\n",
    "### Sampling Strategies\n",
    "\n",
    "| Strategy | Description | Use Case |\n",
    "|----------|-------------|----------|\n",
    "| **Greedy** | Always pick most likely | Deterministic output |\n",
    "| **Temperature** | Scale logits before softmax | Control randomness |\n",
    "| **Top-k** | Sample from k most likely | Limit vocabulary |\n",
    "| **Top-p** | Sample from nucleus | Adaptive vocabulary |\n",
    "\n",
    "### Recommended Settings\n",
    "\n",
    "- **Creative writing**: T=0.9, top_p=0.95\n",
    "- **Code generation**: T=0.2, top_p=0.95\n",
    "- **Factual QA**: T=0.0 (greedy) or T=0.3\n",
    "\n",
    "### GPT vs BERT\n",
    "\n",
    "| Aspect | GPT | BERT |\n",
    "|--------|-----|------|\n",
    "| Architecture | Decoder-only | Encoder-only |\n",
    "| Attention | Causal | Bidirectional |\n",
    "| Pre-training | Next token | Masked tokens |\n",
    "| Strength | Generation | Understanding |\n",
    "| Fine-tuning | Prompt-based | Task heads |\n",
    "\n",
    "### Key Innovations by Version\n",
    "\n",
    "- **GPT-1**: Showed decoder pre-training works\n",
    "- **GPT-2**: Zero-shot transfer via prompting\n",
    "- **GPT-3**: In-context learning, few-shot prompting\n",
    "- **GPT-4**: Multimodal, instruction following, RLHF\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [Efficient Attention Variants](#module-7-efficient-attention) - Scaling to longer sequences\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf31419",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-7-efficient-attention'></a>\n",
    "# Module 7: Efficient Attention Variants\n",
    "\n",
    "**Prerequisites:** Modules 1-6 (attention mechanisms, transformer architecture)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the quadratic complexity problem of standard attention\n",
    "- Implement sparse attention patterns\n",
    "- Understand Flash Attention and its memory optimizations\n",
    "- Implement sliding window attention (Mistral-style)\n",
    "- Compare different attention variants\n",
    "\n",
    "**The Quadratic Problem**\n",
    "\n",
    "Standard attention has $O(n^2)$ complexity in sequence length:\n",
    "- Memory: Store $n \\times n$ attention matrix\n",
    "- Compute: $n^2$ dot products\n",
    "\n",
    "For a 100K token context, that's 10 billion operations per layer!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f0c90b",
   "metadata": {},
   "source": [
    "## 7.1 The Attention Complexity Problem\n",
    "\n",
    "### Standard Attention Complexity\n",
    "\n",
    "For sequence length $n$ and dimension $d$:\n",
    "\n",
    "| Operation | Time | Memory |\n",
    "|-----------|------|--------|\n",
    "| $QK^T$ | $O(n^2 d)$ | $O(n^2)$ |\n",
    "| Softmax | $O(n^2)$ | $O(n^2)$ |\n",
    "| Attention Ã— V | $O(n^2 d)$ | $O(nd)$ |\n",
    "| **Total** | $O(n^2 d)$ | $O(n^2)$ |\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "| Sequence Length | Attention Matrix Size | Memory (FP16) |\n",
    "|-----------------|----------------------|---------------|\n",
    "| 512 | 262K | 0.5 MB |\n",
    "| 2,048 | 4.2M | 8 MB |\n",
    "| 8,192 | 67M | 134 MB |\n",
    "| 32,768 | 1.07B | 2.1 GB |\n",
    "| 131,072 | 17.2B | 34 GB |\n",
    "\n",
    "Modern LLMs need 100K+ context, making standard attention impractical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a5b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ATTENTION COMPLEXITY VISUALIZATION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"ATTENTION COMPLEXITY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def attention_memory_mb(seq_len, dtype_bytes=2):\n",
    "    \"\"\"Calculate attention matrix memory in MB.\"\"\"\n",
    "    return (seq_len ** 2 * dtype_bytes) / (1024 ** 2)\n",
    "\n",
    "def attention_flops(seq_len, d_model):\n",
    "    \"\"\"Calculate attention FLOPs (approximate).\"\"\"\n",
    "    return 2 * seq_len ** 2 * d_model  # QK^T and AttnÃ—V\n",
    "\n",
    "seq_lengths = [512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "d_model = 4096\n",
    "\n",
    "print(\"\\nðŸ“Œ Attention Scaling\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Seq Length':>12} {'Memory (MB)':>15} {'GFLOPs':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    mem = attention_memory_mb(seq_len)\n",
    "    flops = attention_flops(seq_len, d_model) / 1e9\n",
    "    print(f\"{seq_len:>12,} {mem:>15.1f} {flops:>15.1f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "mems = [attention_memory_mb(s) for s in seq_lengths]\n",
    "ax.plot(seq_lengths, mems, 'b-o')\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Memory (MB)')\n",
    "ax.set_title('Attention Memory (Quadratic)')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "flops_list = [attention_flops(s, d_model) / 1e9 for s in seq_lengths]\n",
    "ax.plot(seq_lengths, flops_list, 'r-o')\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('GFLOPs')\n",
    "ax.set_title('Attention Compute (Quadratic)')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Both memory and compute grow quadratically with sequence length!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39704c8",
   "metadata": {},
   "source": [
    "## 7.2 Sparse Attention Patterns\n",
    "\n",
    "### The Idea\n",
    "\n",
    "Instead of attending to ALL positions, attend to a SUBSET:\n",
    "\n",
    "**Local Attention**: Only attend to nearby positions\n",
    "**Strided Attention**: Attend to every k-th position\n",
    "**Global Attention**: Some tokens attend to all (like [CLS])\n",
    "\n",
    "### Sparse Attention (Child et al., 2019)\n",
    "\n",
    "Combines local and strided patterns:\n",
    "\n",
    "```\n",
    "Local (window=3):     Strided (stride=3):    Combined:\n",
    "[1 1 1 0 0 0 0 0]    [1 0 0 1 0 0 1 0]     [1 1 1 1 0 0 1 0]\n",
    "[1 1 1 1 0 0 0 0]    [0 1 0 0 1 0 0 1]     [1 1 1 1 1 0 0 1]\n",
    "[0 1 1 1 1 0 0 0]    [0 0 1 0 0 1 0 0]     [0 1 1 1 1 1 0 0]\n",
    "...\n",
    "```\n",
    "\n",
    "Complexity: $O(n\\sqrt{n})$ instead of $O(n^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SPARSE ATTENTION PATTERNS\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"SPARSE ATTENTION PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_local_mask(seq_len: int, window_size: int) -> torch.Tensor:\n",
    "    \"\"\"Create local attention mask (attend to nearby positions).\"\"\"\n",
    "    mask = torch.zeros(seq_len, seq_len)\n",
    "    for i in range(seq_len):\n",
    "        start = max(0, i - window_size // 2)\n",
    "        end = min(seq_len, i + window_size // 2 + 1)\n",
    "        mask[i, start:end] = 1\n",
    "    return mask\n",
    "\n",
    "def create_strided_mask(seq_len: int, stride: int) -> torch.Tensor:\n",
    "    \"\"\"Create strided attention mask (attend to every stride-th position).\"\"\"\n",
    "    mask = torch.zeros(seq_len, seq_len)\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if j % stride == i % stride:\n",
    "                mask[i, j] = 1\n",
    "    return mask\n",
    "\n",
    "def create_global_mask(seq_len: int, n_global: int) -> torch.Tensor:\n",
    "    \"\"\"Create global attention mask (first n tokens attend to all).\"\"\"\n",
    "    mask = torch.zeros(seq_len, seq_len)\n",
    "    # Global tokens attend to all\n",
    "    mask[:n_global, :] = 1\n",
    "    # All tokens attend to global tokens\n",
    "    mask[:, :n_global] = 1\n",
    "    return mask\n",
    "\n",
    "# Visualize patterns\n",
    "seq_len = 16\n",
    "window_size = 5\n",
    "stride = 4\n",
    "n_global = 2\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Full attention\n",
    "ax = axes[0, 0]\n",
    "full_mask = torch.ones(seq_len, seq_len)\n",
    "ax.imshow(full_mask.numpy(), cmap='Blues', aspect='auto')\n",
    "ax.set_title(f'Full Attention\\n(nÂ² = {seq_len**2} connections)')\n",
    "ax.set_xlabel('Key')\n",
    "ax.set_ylabel('Query')\n",
    "\n",
    "# Local attention\n",
    "ax = axes[0, 1]\n",
    "local_mask = create_local_mask(seq_len, window_size)\n",
    "ax.imshow(local_mask.numpy(), cmap='Blues', aspect='auto')\n",
    "ax.set_title(f'Local Attention (window={window_size})\\n({int(local_mask.sum())} connections)')\n",
    "ax.set_xlabel('Key')\n",
    "ax.set_ylabel('Query')\n",
    "\n",
    "# Strided attention\n",
    "ax = axes[1, 0]\n",
    "strided_mask = create_strided_mask(seq_len, stride)\n",
    "ax.imshow(strided_mask.numpy(), cmap='Blues', aspect='auto')\n",
    "ax.set_title(f'Strided Attention (stride={stride})\\n({int(strided_mask.sum())} connections)')\n",
    "ax.set_xlabel('Key')\n",
    "ax.set_ylabel('Query')\n",
    "\n",
    "# Combined sparse\n",
    "ax = axes[1, 1]\n",
    "combined_mask = torch.clamp(local_mask + strided_mask + create_global_mask(seq_len, n_global), 0, 1)\n",
    "ax.imshow(combined_mask.numpy(), cmap='Blues', aspect='auto')\n",
    "ax.set_title(f'Combined Sparse\\n({int(combined_mask.sum())} connections)')\n",
    "ax.set_xlabel('Key')\n",
    "ax.set_ylabel('Query')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConnection counts (seq_len={seq_len}):\")\n",
    "print(f\"  Full attention: {seq_len**2}\")\n",
    "print(f\"  Local (w={window_size}): {int(local_mask.sum())}\")\n",
    "print(f\"  Strided (s={stride}): {int(strided_mask.sum())}\")\n",
    "print(f\"  Combined: {int(combined_mask.sum())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e873c5ac",
   "metadata": {},
   "source": [
    "## 7.3 Flash Attention\n",
    "\n",
    "### The Memory Bottleneck\n",
    "\n",
    "Standard attention materializes the full $n \\times n$ attention matrix in GPU memory (HBM).\n",
    "This is slow because HBM bandwidth is limited.\n",
    "\n",
    "### Flash Attention (Dao et al., 2022)\n",
    "\n",
    "Key insight: **Never materialize the full attention matrix!**\n",
    "\n",
    "Instead:\n",
    "1. Tile the computation into blocks\n",
    "2. Compute attention block-by-block in fast SRAM\n",
    "3. Use online softmax to avoid storing full matrix\n",
    "\n",
    "### Memory Hierarchy\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           GPU HBM (40GB)            â”‚  â† Slow (1.5 TB/s)\n",
    "â”‚  Stores: Q, K, V, Output            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â†•\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         GPU SRAM (20MB)             â”‚  â† Fast (19 TB/s)\n",
    "â”‚  Computes: Attention blocks         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **2-4x faster** than standard attention\n",
    "- **5-20x less memory** (linear instead of quadratic)\n",
    "- **Exact** computation (not an approximation)\n",
    "- Enables much longer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b200bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FLASH ATTENTION CONCEPT\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"FLASH ATTENTION CONCEPT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Note: Actual Flash Attention requires CUDA kernels\n",
    "# This demonstrates the CONCEPT with a simplified Python version\n",
    "\n",
    "def standard_attention(Q, K, V):\n",
    "    \"\"\"Standard attention - materializes full attention matrix.\"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # This creates the full nÃ—n matrix in memory!\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def flash_attention_concept(Q, K, V, block_size=64):\n",
    "    \"\"\"\n",
    "    Conceptual Flash Attention (simplified).\n",
    "    \n",
    "    Real Flash Attention uses CUDA kernels for efficiency.\n",
    "    This shows the tiling concept in Python.\n",
    "    \"\"\"\n",
    "    batch_size, n_heads, seq_len, d_k = Q.shape\n",
    "    \n",
    "    # Initialize output and normalization terms\n",
    "    output = torch.zeros_like(Q)\n",
    "    row_max = torch.full((batch_size, n_heads, seq_len, 1), float('-inf'))\n",
    "    row_sum = torch.zeros((batch_size, n_heads, seq_len, 1))\n",
    "    \n",
    "    # Process in blocks (tiles)\n",
    "    for j_start in range(0, seq_len, block_size):\n",
    "        j_end = min(j_start + block_size, seq_len)\n",
    "        \n",
    "        # Load K, V block\n",
    "        K_block = K[:, :, j_start:j_end, :]\n",
    "        V_block = V[:, :, j_start:j_end, :]\n",
    "        \n",
    "        # Compute attention scores for this block\n",
    "        scores_block = torch.matmul(Q, K_block.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        # Online softmax update\n",
    "        block_max = scores_block.max(dim=-1, keepdim=True)[0]\n",
    "        new_max = torch.maximum(row_max, block_max)\n",
    "        \n",
    "        # Rescale previous sum\n",
    "        exp_diff = torch.exp(row_max - new_max)\n",
    "        row_sum = row_sum * exp_diff\n",
    "        output = output * exp_diff\n",
    "        \n",
    "        # Add current block contribution\n",
    "        exp_scores = torch.exp(scores_block - new_max)\n",
    "        row_sum = row_sum + exp_scores.sum(dim=-1, keepdim=True)\n",
    "        output = output + torch.matmul(exp_scores, V_block)\n",
    "        \n",
    "        row_max = new_max\n",
    "    \n",
    "    # Normalize\n",
    "    output = output / row_sum\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Compare outputs\n",
    "print(\"\\nðŸ“Œ Comparing Standard vs Flash Attention (Concept)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "batch_size, n_heads, seq_len, d_k = 2, 4, 128, 64\n",
    "\n",
    "Q = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "K = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "V = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "\n",
    "standard_out = standard_attention(Q, K, V)\n",
    "flash_out = flash_attention_concept(Q, K, V, block_size=32)\n",
    "\n",
    "# Check if outputs match\n",
    "max_diff = (standard_out - flash_out).abs().max().item()\n",
    "print(f\"Max difference: {max_diff:.2e}\")\n",
    "print(f\"Outputs match: {max_diff < 1e-5} âœ“\")\n",
    "\n",
    "print(\"\\nðŸ“Œ Memory Comparison (Conceptual)\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Standard attention stores: {seq_len}Ã—{seq_len} = {seq_len**2:,} elements\")\n",
    "print(f\"Flash attention stores: ~{seq_len}Ã—{32} = {seq_len*32:,} elements per block\")\n",
    "print(f\"Memory reduction: ~{seq_len**2 / (seq_len*32):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba2271",
   "metadata": {},
   "source": [
    "## 7.4 Sliding Window Attention (Mistral)\n",
    "\n",
    "### The Idea\n",
    "\n",
    "Each token only attends to a fixed window of previous tokens:\n",
    "\n",
    "```\n",
    "Window size = 4:\n",
    "\n",
    "Position 0: [1 0 0 0 0 0 0 0]  â† sees only itself\n",
    "Position 1: [1 1 0 0 0 0 0 0]  â† sees 0, 1\n",
    "Position 2: [1 1 1 0 0 0 0 0]  â† sees 0, 1, 2\n",
    "Position 3: [1 1 1 1 0 0 0 0]  â† sees 0, 1, 2, 3\n",
    "Position 4: [0 1 1 1 1 0 0 0]  â† sees 1, 2, 3, 4 (window slides)\n",
    "Position 5: [0 0 1 1 1 1 0 0]  â† sees 2, 3, 4, 5\n",
    "...\n",
    "```\n",
    "\n",
    "### Effective Context\n",
    "\n",
    "With $L$ layers and window size $W$, effective context is $L \\times W$:\n",
    "- Layer 1: Token sees $W$ tokens\n",
    "- Layer 2: Those $W$ tokens each saw $W$ tokens â†’ $2W$ effective\n",
    "- Layer $L$: $L \\times W$ effective context\n",
    "\n",
    "Mistral-7B: 32 layers Ã— 4096 window = 131K effective context!\n",
    "\n",
    "### Complexity\n",
    "\n",
    "- Standard: $O(n^2)$\n",
    "- Sliding window: $O(n \\times W)$ = $O(n)$ for fixed $W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f25633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SLIDING WINDOW ATTENTION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"SLIDING WINDOW ATTENTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_sliding_window_mask(seq_len: int, window_size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create sliding window causal attention mask.\n",
    "    \n",
    "    Each position attends to at most window_size previous positions.\n",
    "    \"\"\"\n",
    "    # Start with causal mask\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    \n",
    "    # Apply window constraint\n",
    "    for i in range(seq_len):\n",
    "        if i >= window_size:\n",
    "            mask[i, :i-window_size+1] = 0\n",
    "    \n",
    "    return mask\n",
    "\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Sliding Window Attention as used in Mistral.\n",
    "    \n",
    "    Each token attends only to the previous window_size tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, window_size: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project Q, K, V\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Create sliding window mask\n",
    "        mask = create_sliding_window_mask(seq_len, self.window_size).to(x.device)\n",
    "        \n",
    "        # Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Output\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        return self.W_o(attn_output)\n",
    "\n",
    "# Visualize sliding window\n",
    "print(\"\\nðŸ“Œ Sliding Window Mask\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "seq_len = 12\n",
    "window_size = 4\n",
    "\n",
    "mask = create_sliding_window_mask(seq_len, window_size)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(mask.numpy(), cmap='Blues', aspect='auto')\n",
    "plt.colorbar(label='Can Attend')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title(f'Sliding Window Attention (window={window_size})')\n",
    "\n",
    "# Add grid\n",
    "for i in range(seq_len + 1):\n",
    "    plt.axhline(y=i-0.5, color='gray', linewidth=0.5)\n",
    "    plt.axvline(x=i-0.5, color='gray', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test the module\n",
    "swa = SlidingWindowAttention(d_model=64, n_heads=4, window_size=4)\n",
    "x = torch.randn(2, seq_len, 64)\n",
    "output = swa(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Window size: {window_size}\")\n",
    "print(f\"Effective attention per position: {window_size} tokens (vs {seq_len} for full)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da3075",
   "metadata": {},
   "source": [
    "## 7.5 Complexity Comparison\n",
    "\n",
    "### Summary of Attention Variants\n",
    "\n",
    "| Method | Time | Memory | Exact? | Notes |\n",
    "|--------|------|--------|--------|-------|\n",
    "| Standard | $O(n^2d)$ | $O(n^2)$ | âœ“ | Baseline |\n",
    "| Sparse | $O(n\\sqrt{n}d)$ | $O(n\\sqrt{n})$ | âœ“ | Fixed patterns |\n",
    "| Linear | $O(nd^2)$ | $O(nd)$ | âœ— | Kernel approximation |\n",
    "| Flash | $O(n^2d)$ | $O(n)$ | âœ“ | IO-aware |\n",
    "| Sliding Window | $O(nWd)$ | $O(nW)$ | âœ“ | Fixed window |\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "- **Standard**: Short sequences (<2K), need full attention\n",
    "- **Flash**: Always use when available (drop-in replacement)\n",
    "- **Sliding Window**: Very long sequences, local context sufficient\n",
    "- **Sparse**: Long sequences, need some global attention\n",
    "- **Linear**: Extreme lengths, can tolerate approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPLEXITY COMPARISON\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"ATTENTION COMPLEXITY COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def complexity_comparison(seq_lengths, d_model=4096, window_size=4096):\n",
    "    \"\"\"Compare complexity of different attention methods.\"\"\"\n",
    "    results = {\n",
    "        'Standard': [],\n",
    "        'Flash (same compute, less memory)': [],\n",
    "        'Sliding Window': [],\n",
    "        'Linear': []\n",
    "    }\n",
    "    \n",
    "    for n in seq_lengths:\n",
    "        # Standard: O(nÂ²d) compute, O(nÂ²) memory\n",
    "        results['Standard'].append(n ** 2 * d_model)\n",
    "        \n",
    "        # Flash: Same compute, O(n) memory\n",
    "        results['Flash (same compute, less memory)'].append(n ** 2 * d_model)\n",
    "        \n",
    "        # Sliding Window: O(n*W*d) compute\n",
    "        results['Sliding Window'].append(n * window_size * d_model)\n",
    "        \n",
    "        # Linear: O(n*dÂ²) compute\n",
    "        results['Linear'].append(n * d_model ** 2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "seq_lengths = [512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]\n",
    "results = complexity_comparison(seq_lengths)\n",
    "\n",
    "# Plot compute comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for method, values in results.items():\n",
    "    if 'memory' not in method.lower():\n",
    "        plt.plot(seq_lengths, [v/1e12 for v in values], '-o', label=method)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Compute (TFLOPs)')\n",
    "plt.title('Compute Complexity')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Memory comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "memory_standard = [n**2 * 2 / 1e9 for n in seq_lengths]  # FP16\n",
    "memory_flash = [n * 4096 * 2 / 1e9 for n in seq_lengths]  # Linear in n\n",
    "memory_sliding = [n * 4096 * 2 / 1e9 for n in seq_lengths]\n",
    "\n",
    "plt.plot(seq_lengths, memory_standard, 'r-o', label='Standard')\n",
    "plt.plot(seq_lengths, memory_flash, 'g-o', label='Flash Attention')\n",
    "plt.plot(seq_lengths, memory_sliding, 'b-o', label='Sliding Window')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Memory (GB)')\n",
    "plt.title('Memory Usage')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Œ Key Insights:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"â€¢ Flash Attention: Same compute, but much less memory\")\n",
    "print(\"â€¢ Sliding Window: Linear scaling enables 100K+ context\")\n",
    "print(\"â€¢ Linear Attention: Best scaling but approximation\")\n",
    "print(\"\\nâœ… Modern LLMs combine Flash Attention + Sliding Window for efficiency!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c072a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 7: Key Takeaways\n",
    "\n",
    "### The Quadratic Problem\n",
    "\n",
    "Standard attention: $O(n^2)$ in both time and memory\n",
    "- 8K context â†’ 64M attention weights\n",
    "- 128K context â†’ 16B attention weights\n",
    "\n",
    "### Solutions\n",
    "\n",
    "| Method | Key Idea | Complexity |\n",
    "|--------|----------|------------|\n",
    "| **Sparse** | Attend to subset | $O(n\\sqrt{n})$ |\n",
    "| **Flash** | Tile computation, avoid HBM | $O(n^2)$ time, $O(n)$ memory |\n",
    "| **Sliding Window** | Fixed local window | $O(nW)$ |\n",
    "| **Linear** | Kernel approximation | $O(nd^2)$ |\n",
    "\n",
    "### Flash Attention\n",
    "\n",
    "- **Not an approximation** - exact same output\n",
    "- **2-4x faster** due to reduced memory bandwidth\n",
    "- **5-20x less memory** - enables longer sequences\n",
    "- **Always use** when available (PyTorch 2.0+)\n",
    "\n",
    "### Sliding Window (Mistral-style)\n",
    "\n",
    "- Each token attends to $W$ previous tokens\n",
    "- Effective context: $L \\times W$ (layers Ã— window)\n",
    "- Mistral-7B: 32 layers Ã— 4096 = 131K effective context\n",
    "- Combines with Flash Attention for best of both\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "1. **Use Flash Attention** - it's a free speedup\n",
    "2. **Sliding window** for very long contexts\n",
    "3. **Sparse patterns** when you need global + local\n",
    "4. **Linear attention** only if you can tolerate approximation\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [Mixture of Experts](#module-8-moe) - Scaling model capacity efficiently\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1447fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-8-moe'></a>\n",
    "# Module 8: Mixture of Experts (MoE)\n",
    "\n",
    "**Prerequisites:** Modules 1-7 (transformer architecture, attention)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the MoE architecture and its benefits\n",
    "- Implement expert networks and gating mechanisms\n",
    "- Understand load balancing and auxiliary losses\n",
    "- See how MoE enables efficient scaling\n",
    "\n",
    "**Why MoE?**\n",
    "\n",
    "Dense models: ALL parameters used for EVERY token\n",
    "MoE models: Only SOME parameters (experts) used per token\n",
    "\n",
    "This allows much larger models with similar compute cost!\n",
    "\n",
    "| Model | Total Params | Active Params | Experts |\n",
    "|-------|-------------|---------------|---------|\n",
    "| Mixtral 8x7B | 47B | 13B | 8 |\n",
    "| GPT-4 (rumored) | 1.8T | ~220B | 16 |\n",
    "| Switch-C | 1.6T | ~1.6B | 2048 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a05cd4",
   "metadata": {},
   "source": [
    "## 8.1 MoE Architecture\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "Replace the FFN in each transformer block with multiple \"expert\" FFNs:\n",
    "\n",
    "```\n",
    "Standard Transformer:          MoE Transformer:\n",
    "                              \n",
    "Input â”€â”€â–º Attention â”€â”€â–º        Input â”€â”€â–º Attention â”€â”€â–º\n",
    "          â”‚                              â”‚\n",
    "          â–¼                              â–¼\n",
    "        FFN                           Router\n",
    "          â”‚                         â•±   â”‚   â•²\n",
    "          â–¼                       E1   E2   E3  (Experts)\n",
    "       Output                       â•²   â”‚   â•±\n",
    "                                      â–¼\n",
    "                                   Output\n",
    "```\n",
    "\n",
    "### The Router (Gating Network)\n",
    "\n",
    "The router decides which expert(s) to use for each token:\n",
    "\n",
    "$G(x) = \\text{Softmax}(W_g \\cdot x)$\n",
    "\n",
    "- Input: Token representation $x$\n",
    "- Output: Probability distribution over experts\n",
    "- Typically select top-k experts (k=1 or k=2)\n",
    "\n",
    "### Sparse Activation\n",
    "\n",
    "Only k experts are activated per token:\n",
    "- **Top-1**: Each token uses 1 expert\n",
    "- **Top-2**: Each token uses 2 experts (weighted average)\n",
    "\n",
    "This keeps compute constant regardless of number of experts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf38380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MIXTURE OF EXPERTS IMPLEMENTATION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"MIXTURE OF EXPERTS (MoE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"Single expert network (just an FFN).\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class Router(nn.Module):\n",
    "    \"\"\"\n",
    "    Router/Gating network that selects experts for each token.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_experts: int):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(d_model, n_experts, bias=False)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            router_logits: (batch, seq_len, n_experts)\n",
    "        \"\"\"\n",
    "        return self.gate(x)\n",
    "\n",
    "class MoELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Experts layer with top-k routing.\n",
    "    \n",
    "    Replaces the FFN in a transformer block.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        n_experts: int,\n",
    "        top_k: int = 2,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_experts = n_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Create experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(d_model, d_ff, dropout) for _ in range(n_experts)\n",
    "        ])\n",
    "        \n",
    "        # Router\n",
    "        self.router = Router(d_model, n_experts)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "            router_logits: For auxiliary loss computation\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Get router logits and probabilities\n",
    "        router_logits = self.router(x)  # (batch, seq_len, n_experts)\n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "        \n",
    "        # Select top-k experts\n",
    "        top_k_probs, top_k_indices = torch.topk(router_probs, self.top_k, dim=-1)\n",
    "        \n",
    "        # Normalize top-k probabilities\n",
    "        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Compute expert outputs\n",
    "        # For simplicity, we process all experts (in practice, use sparse computation)\n",
    "        output = torch.zeros_like(x)\n",
    "        \n",
    "        for k in range(self.top_k):\n",
    "            expert_indices = top_k_indices[:, :, k]  # (batch, seq_len)\n",
    "            expert_weights = top_k_probs[:, :, k:k+1]  # (batch, seq_len, 1)\n",
    "            \n",
    "            for expert_idx in range(self.n_experts):\n",
    "                # Find tokens routed to this expert\n",
    "                mask = (expert_indices == expert_idx)\n",
    "                if mask.any():\n",
    "                    # Get tokens for this expert\n",
    "                    expert_input = x[mask]\n",
    "                    expert_output = self.experts[expert_idx](expert_input)\n",
    "                    \n",
    "                    # Add weighted contribution\n",
    "                    output[mask] += expert_weights[mask].squeeze(-1).unsqueeze(-1) * expert_output\n",
    "        \n",
    "        return output, router_logits\n",
    "\n",
    "# Test MoE layer\n",
    "print(\"\\nðŸ“Œ Testing MoE Layer\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_model = 64\n",
    "d_ff = 256\n",
    "n_experts = 4\n",
    "top_k = 2\n",
    "\n",
    "moe = MoELayer(d_model, d_ff, n_experts, top_k)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, router_logits = moe(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Router logits shape: {router_logits.shape}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Number of experts: {n_experts}\")\n",
    "print(f\"  Top-k: {top_k}\")\n",
    "print(f\"  Active params per token: {top_k}/{n_experts} = {top_k/n_experts:.0%}\")\n",
    "\n",
    "# Show routing decisions\n",
    "probs = F.softmax(router_logits[0, 0], dim=-1)\n",
    "print(f\"\\nRouting for first token:\")\n",
    "print(f\"  Expert probabilities: {probs.tolist()}\")\n",
    "top_experts = torch.topk(probs, top_k)\n",
    "print(f\"  Selected experts: {top_experts.indices.tolist()}\")\n",
    "print(f\"  Weights: {top_experts.values.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbedf1d8",
   "metadata": {},
   "source": [
    "## 8.2 Load Balancing\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Without constraints, the router might:\n",
    "- Send all tokens to one expert (collapse)\n",
    "- Ignore some experts entirely (waste of parameters)\n",
    "\n",
    "### Load Balancing Loss (Fedus et al., 2021)\n",
    "\n",
    "Auxiliary loss to encourage balanced expert usage:\n",
    "\n",
    "$\\mathcal{L}_{aux} = \\alpha \\cdot n_{experts} \\cdot \\sum_{i=1}^{n_{experts}} f_i \\cdot P_i$\n",
    "\n",
    "Where:\n",
    "- $f_i$ = fraction of tokens routed to expert $i$\n",
    "- $P_i$ = average router probability for expert $i$\n",
    "- $\\alpha$ = auxiliary loss weight (typically 0.01)\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "- If expert $i$ gets too many tokens: $f_i$ is high â†’ loss increases\n",
    "- If expert $i$ has high probability but few tokens: $P_i$ is high â†’ loss increases\n",
    "- Minimum when all experts get equal load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f820a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOAD BALANCING LOSS\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"LOAD BALANCING LOSS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def compute_load_balancing_loss(\n",
    "    router_logits: torch.Tensor,\n",
    "    top_k: int,\n",
    "    n_experts: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute auxiliary load balancing loss.\n",
    "    \n",
    "    Encourages balanced expert utilization.\n",
    "    \n",
    "    Args:\n",
    "        router_logits: (batch, seq_len, n_experts)\n",
    "        top_k: Number of experts selected per token\n",
    "        n_experts: Total number of experts\n",
    "    \n",
    "    Returns:\n",
    "        Auxiliary loss scalar\n",
    "    \"\"\"\n",
    "    # Get router probabilities\n",
    "    router_probs = F.softmax(router_logits, dim=-1)  # (batch, seq_len, n_experts)\n",
    "    \n",
    "    # Flatten batch and sequence dimensions\n",
    "    router_probs_flat = router_probs.view(-1, n_experts)  # (batch*seq_len, n_experts)\n",
    "    \n",
    "    # Get top-k expert indices\n",
    "    _, top_k_indices = torch.topk(router_probs_flat, top_k, dim=-1)\n",
    "    \n",
    "    # Compute f_i: fraction of tokens routed to each expert\n",
    "    # Create one-hot for selected experts\n",
    "    expert_mask = torch.zeros_like(router_probs_flat)\n",
    "    expert_mask.scatter_(1, top_k_indices, 1.0)\n",
    "    f = expert_mask.mean(dim=0)  # (n_experts,)\n",
    "    \n",
    "    # Compute P_i: average router probability for each expert\n",
    "    P = router_probs_flat.mean(dim=0)  # (n_experts,)\n",
    "    \n",
    "    # Load balancing loss\n",
    "    aux_loss = n_experts * (f * P).sum()\n",
    "    \n",
    "    return aux_loss\n",
    "\n",
    "# Demonstrate load balancing\n",
    "print(\"\\nðŸ“Œ Load Balancing Loss\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Balanced routing\n",
    "balanced_logits = torch.randn(4, 20, 4)  # Roughly equal probabilities\n",
    "balanced_loss = compute_load_balancing_loss(balanced_logits, top_k=2, n_experts=4)\n",
    "\n",
    "# Imbalanced routing (one expert dominates)\n",
    "imbalanced_logits = torch.randn(4, 20, 4)\n",
    "imbalanced_logits[:, :, 0] += 5.0  # Make expert 0 much more likely\n",
    "imbalanced_loss = compute_load_balancing_loss(imbalanced_logits, top_k=2, n_experts=4)\n",
    "\n",
    "print(f\"Balanced routing loss: {balanced_loss.item():.4f}\")\n",
    "print(f\"Imbalanced routing loss: {imbalanced_loss.item():.4f}\")\n",
    "print(f\"\\nHigher loss for imbalanced routing encourages balance!\")\n",
    "\n",
    "# Visualize expert utilization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for idx, (logits, title) in enumerate([\n",
    "    (balanced_logits, 'Balanced Routing'),\n",
    "    (imbalanced_logits, 'Imbalanced Routing')\n",
    "]):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    _, top_k_idx = torch.topk(probs, 2, dim=-1)\n",
    "    \n",
    "    # Count expert usage\n",
    "    expert_counts = torch.zeros(4)\n",
    "    for i in range(4):\n",
    "        expert_counts[i] = (top_k_idx == i).sum().item()\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.bar(range(4), expert_counts.numpy())\n",
    "    ax.set_xlabel('Expert')\n",
    "    ax.set_ylabel('Token Count')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(4))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a19509",
   "metadata": {},
   "source": [
    "## 8.3 MoE in Modern LLMs\n",
    "\n",
    "### Mixtral 8x7B Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           Mixtral 8x7B                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Layers: 32                              â”‚\n",
    "â”‚ d_model: 4096                           â”‚\n",
    "â”‚ Heads: 32                               â”‚\n",
    "â”‚ Experts per layer: 8                    â”‚\n",
    "â”‚ Top-k: 2                                â”‚\n",
    "â”‚ Total params: 47B                       â”‚\n",
    "â”‚ Active params: 13B                      â”‚\n",
    "â”‚ Context: 32K (sliding window)           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Benefits of MoE\n",
    "\n",
    "1. **More parameters, same compute**: 47B params but only 13B active\n",
    "2. **Specialization**: Experts can specialize in different tasks/domains\n",
    "3. **Efficient scaling**: Add experts without increasing inference cost\n",
    "\n",
    "### Challenges\n",
    "\n",
    "1. **Memory**: All experts must fit in memory\n",
    "2. **Load balancing**: Need auxiliary losses\n",
    "3. **Training stability**: Can be harder to train\n",
    "4. **Communication**: Distributed training requires expert parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6cfa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPLETE MOE TRANSFORMER BLOCK\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"MOE TRANSFORMER BLOCK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class MoETransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block with MoE FFN layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        d_ff: int,\n",
    "        n_experts: int,\n",
    "        top_k: int = 2,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Self-attention\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # MoE FFN\n",
    "        self.moe = MoELayer(d_model, d_ff, n_experts, top_k, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Self-attention\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x, x, x, mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        # MoE FFN\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        moe_output, router_logits = self.moe(x)\n",
    "        x = self.dropout2(moe_output)\n",
    "        x = residual + x\n",
    "        \n",
    "        return x, router_logits\n",
    "\n",
    "# Test MoE block\n",
    "print(\"\\nðŸ“Œ Testing MoE Transformer Block\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "block = MoETransformerBlock(\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    d_ff=512,\n",
    "    n_experts=8,\n",
    "    top_k=2\n",
    ")\n",
    "\n",
    "x = torch.randn(2, 20, 128)\n",
    "output, router_logits = block(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Parameter comparison\n",
    "dense_params = 128 * 512 * 2  # Single FFN\n",
    "moe_params = 128 * 512 * 2 * 8  # 8 experts\n",
    "active_params = 128 * 512 * 2 * 2  # Top-2 active\n",
    "\n",
    "print(f\"\\nParameter comparison:\")\n",
    "print(f\"  Dense FFN: {dense_params:,}\")\n",
    "print(f\"  MoE total: {moe_params:,} ({moe_params/dense_params:.1f}x)\")\n",
    "print(f\"  MoE active: {active_params:,} ({active_params/dense_params:.1f}x)\")\n",
    "print(f\"\\nâœ… 8x more parameters, only 2x compute!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257c4c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 8: Key Takeaways\n",
    "\n",
    "### MoE Architecture\n",
    "\n",
    "- Replace FFN with multiple \"expert\" FFNs\n",
    "- Router selects top-k experts per token\n",
    "- Only k experts activated â†’ constant compute\n",
    "\n",
    "### Key Components\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| **Experts** | Specialized FFN networks |\n",
    "| **Router** | Selects experts for each token |\n",
    "| **Top-k** | Number of active experts |\n",
    "| **Aux Loss** | Encourages balanced routing |\n",
    "\n",
    "### Load Balancing\n",
    "\n",
    "$\\mathcal{L}_{aux} = \\alpha \\cdot n \\cdot \\sum_i f_i \\cdot P_i$\n",
    "\n",
    "- Prevents expert collapse\n",
    "- Ensures all experts are utilized\n",
    "- Typical $\\alpha = 0.01$\n",
    "\n",
    "### Benefits vs Challenges\n",
    "\n",
    "| Benefits | Challenges |\n",
    "|----------|------------|\n",
    "| More params, same compute | Higher memory |\n",
    "| Expert specialization | Training stability |\n",
    "| Efficient scaling | Load balancing |\n",
    "\n",
    "### Modern MoE Models\n",
    "\n",
    "| Model | Experts | Top-k | Total/Active |\n",
    "|-------|---------|-------|--------------|\n",
    "| Mixtral 8x7B | 8 | 2 | 47B/13B |\n",
    "| Switch-C | 2048 | 1 | 1.6T/1.6B |\n",
    "| GPT-4 (rumored) | 16 | 2 | 1.8T/220B |\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [Modern LLM Techniques](#module-9-modern-techniques) - RoPE, RMSNorm, SwiGLU, GQA\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99408d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-9-modern-techniques'></a>\n",
    "# Module 9: Modern LLM Techniques\n",
    "\n",
    "**Prerequisites:** Modules 1-8 (transformer architecture, attention)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Implement Rotary Position Embeddings (RoPE)\n",
    "- Implement RMSNorm (faster than LayerNorm)\n",
    "- Implement SwiGLU activation\n",
    "- Implement Grouped Query Attention (GQA)\n",
    "- Implement KV Cache for efficient inference\n",
    "\n",
    "These techniques are used in modern LLMs like LLaMA, Mistral, and GPT-4.\n",
    "\n",
    "| Technique | Used In | Benefit |\n",
    "|-----------|---------|---------|\n",
    "| RoPE | LLaMA, Mistral, GPT-NeoX | Better position encoding |\n",
    "| RMSNorm | LLaMA, Mistral | Faster than LayerNorm |\n",
    "| SwiGLU | LLaMA, PaLM | Better FFN activation |\n",
    "| GQA | LLaMA 2, Mistral | Reduced KV cache |\n",
    "| KV Cache | All | Faster inference |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c026f3f",
   "metadata": {},
   "source": [
    "## 9.1 Rotary Position Embeddings (RoPE)\n",
    "\n",
    "### The Problem with Absolute Position Embeddings\n",
    "\n",
    "Standard position embeddings add a fixed vector per position:\n",
    "$x_i = \\text{token}_i + \\text{pos}_i$\n",
    "\n",
    "Issues:\n",
    "- Can't extrapolate beyond training length\n",
    "- Position info can be \"washed out\" in deep networks\n",
    "\n",
    "### RoPE: Encoding Position in Attention (Su et al., 2021)\n",
    "\n",
    "Key insight: Encode position by **rotating** query and key vectors!\n",
    "\n",
    "For position $m$, rotate the vector by angle $m\\theta$:\n",
    "\n",
    "$f(x, m) = \\begin{pmatrix} \\cos(m\\theta) & -\\sin(m\\theta) \\\\ \\sin(m\\theta) & \\cos(m\\theta) \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$\n",
    "\n",
    "### Why Rotation Works\n",
    "\n",
    "The dot product of rotated vectors depends on **relative** position:\n",
    "\n",
    "$\\langle f(q, m), f(k, n) \\rangle = g(q, k, m-n)$\n",
    "\n",
    "This naturally captures relative positions without explicit relative position embeddings!\n",
    "\n",
    "### RoPE Formula\n",
    "\n",
    "For dimension pairs $(2i, 2i+1)$:\n",
    "\n",
    "$\\theta_i = 10000^{-2i/d}$\n",
    "\n",
    "$q'_{2i} = q_{2i}\\cos(m\\theta_i) - q_{2i+1}\\sin(m\\theta_i)$\n",
    "$q'_{2i+1} = q_{2i}\\sin(m\\theta_i) + q_{2i+1}\\cos(m\\theta_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad0aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ROTARY POSITION EMBEDDINGS (RoPE)\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"ROTARY POSITION EMBEDDINGS (RoPE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class RotaryPositionEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Position Embedding (RoPE) from Su et al., 2021.\n",
    "    \n",
    "    Encodes position by rotating query and key vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 8192, base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.base = base\n",
    "        \n",
    "        # Compute rotation frequencies\n",
    "        # theta_i = base^(-2i/d) for i in [0, d/2)\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # Precompute cos and sin for all positions\n",
    "        self._precompute_cos_sin(max_len)\n",
    "    \n",
    "    def _precompute_cos_sin(self, max_len: int):\n",
    "        \"\"\"Precompute cos and sin values for efficiency.\"\"\"\n",
    "        positions = torch.arange(max_len).float()\n",
    "        # Outer product: (max_len,) x (d/2,) -> (max_len, d/2)\n",
    "        freqs = torch.outer(positions, self.inv_freq)\n",
    "        # Duplicate for pairs: (max_len, d)\n",
    "        freqs = torch.cat([freqs, freqs], dim=-1)\n",
    "        \n",
    "        self.register_buffer('cos_cached', freqs.cos())\n",
    "        self.register_buffer('sin_cached', freqs.sin())\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get cos and sin for the given sequence length.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (for device)\n",
    "            seq_len: Sequence length\n",
    "        \n",
    "        Returns:\n",
    "            cos, sin: (seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(x.device),\n",
    "            self.sin_cached[:seq_len].to(x.device)\n",
    "        )\n",
    "\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Rotate half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., :x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2:]\n",
    "    return torch.cat([-x2, x1], dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(\n",
    "    q: torch.Tensor,\n",
    "    k: torch.Tensor,\n",
    "    cos: torch.Tensor,\n",
    "    sin: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Apply rotary position embeddings to query and key.\n",
    "    \n",
    "    Args:\n",
    "        q: Query tensor (batch, n_heads, seq_len, d_k)\n",
    "        k: Key tensor (batch, n_heads, seq_len, d_k)\n",
    "        cos: Cosine values (seq_len, d_k)\n",
    "        sin: Sine values (seq_len, d_k)\n",
    "    \n",
    "    Returns:\n",
    "        Rotated q and k\n",
    "    \"\"\"\n",
    "    # Reshape cos/sin for broadcasting\n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, d_k)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Apply rotation\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    \n",
    "    return q_embed, k_embed\n",
    "\n",
    "# Test RoPE\n",
    "print(\"\\nðŸ“Œ Testing RoPE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_model = 64\n",
    "rope = RotaryPositionEmbedding(d_model, max_len=1024)\n",
    "\n",
    "batch_size = 2\n",
    "n_heads = 4\n",
    "seq_len = 10\n",
    "d_k = d_model // n_heads\n",
    "\n",
    "q = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "k = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "\n",
    "cos, sin = rope(q, seq_len)\n",
    "q_rot, k_rot = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "print(f\"Query shape: {q.shape}\")\n",
    "print(f\"Rotated query shape: {q_rot.shape}\")\n",
    "print(f\"Cos/Sin shape: {cos.shape}\")\n",
    "\n",
    "# Verify relative position property\n",
    "print(\"\\nðŸ“Œ Verifying Relative Position Property\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Attention score should depend on relative position\n",
    "# Score(q_m, k_n) should equal Score(q_{m+1}, k_{n+1})\n",
    "score_0_2 = (q_rot[0, 0, 0] * k_rot[0, 0, 2]).sum()\n",
    "score_1_3 = (q_rot[0, 0, 1] * k_rot[0, 0, 3]).sum()\n",
    "print(f\"Score(pos 0, pos 2): {score_0_2.item():.4f}\")\n",
    "print(f\"Score(pos 1, pos 3): {score_1_3.item():.4f}\")\n",
    "print(\"(These should be similar - both have relative distance 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b281a",
   "metadata": {},
   "source": [
    "## 9.2 RMSNorm\n",
    "\n",
    "### LayerNorm Recap\n",
    "\n",
    "$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$\n",
    "\n",
    "Requires computing both mean and variance.\n",
    "\n",
    "### RMSNorm (Zhang & Sennrich, 2019)\n",
    "\n",
    "Simpler: Only normalize by root mean square, no centering:\n",
    "\n",
    "$\\text{RMSNorm}(x) = \\gamma \\cdot \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_{i=1}^{d} x_i^2 + \\epsilon}}$\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **Faster**: No mean computation, no bias term\n",
    "- **Simpler**: Fewer parameters\n",
    "- **Works well**: Used in LLaMA, Mistral, etc.\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| Aspect | LayerNorm | RMSNorm |\n",
    "|--------|-----------|---------|\n",
    "| Mean subtraction | Yes | No |\n",
    "| Variance normalization | Yes | RMS only |\n",
    "| Bias term | Yes | No |\n",
    "| Parameters | 2d | d |\n",
    "| Speed | Baseline | ~10% faster |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RMSNORM\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"RMSNORM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalization.\n",
    "    \n",
    "    RMSNorm(x) = gamma * x / sqrt(mean(x^2) + eps)\n",
    "    \n",
    "    Simpler and faster than LayerNorm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Compute RMS\n",
    "        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        # Normalize and scale\n",
    "        return self.weight * (x / rms)\n",
    "\n",
    "# Compare with LayerNorm\n",
    "print(\"\\nðŸ“Œ Comparing RMSNorm vs LayerNorm\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_model = 768\n",
    "batch_size = 32\n",
    "seq_len = 512\n",
    "\n",
    "rms_norm = RMSNorm(d_model)\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Time comparison\n",
    "import time\n",
    "\n",
    "n_iterations = 100\n",
    "\n",
    "# RMSNorm timing\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    _ = rms_norm(x)\n",
    "rms_time = time.time() - start\n",
    "\n",
    "# LayerNorm timing\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    _ = layer_norm(x)\n",
    "ln_time = time.time() - start\n",
    "\n",
    "print(f\"RMSNorm time: {rms_time*1000:.2f}ms\")\n",
    "print(f\"LayerNorm time: {ln_time*1000:.2f}ms\")\n",
    "print(f\"Speedup: {ln_time/rms_time:.2f}x\")\n",
    "\n",
    "# Parameter comparison\n",
    "print(f\"\\nParameter count:\")\n",
    "print(f\"  RMSNorm: {sum(p.numel() for p in rms_norm.parameters()):,}\")\n",
    "print(f\"  LayerNorm: {sum(p.numel() for p in layer_norm.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48612ed0",
   "metadata": {},
   "source": [
    "## 9.3 SwiGLU Activation\n",
    "\n",
    "### Standard FFN\n",
    "\n",
    "$\\text{FFN}(x) = \\text{ReLU}(xW_1)W_2$\n",
    "\n",
    "### GLU (Gated Linear Unit)\n",
    "\n",
    "$\\text{GLU}(x) = (xW_1) \\otimes \\sigma(xW_2)$\n",
    "\n",
    "The gate $\\sigma(xW_2)$ controls information flow.\n",
    "\n",
    "### SwiGLU (Shazeer, 2020)\n",
    "\n",
    "Combines Swish activation with GLU:\n",
    "\n",
    "$\\text{SwiGLU}(x) = (\\text{Swish}(xW_1)) \\otimes (xW_2)$\n",
    "\n",
    "Where $\\text{Swish}(x) = x \\cdot \\sigma(x)$\n",
    "\n",
    "### Why SwiGLU?\n",
    "\n",
    "- Better performance than ReLU/GELU\n",
    "- Used in PaLM, LLaMA, Mistral\n",
    "- Slightly more parameters (3 weight matrices vs 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079673ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SWIGLU ACTIVATION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"SWIGLU ACTIVATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU activation function.\n",
    "    \n",
    "    SwiGLU(x) = Swish(x @ W1) * (x @ W2)\n",
    "    \n",
    "    Used in LLaMA, PaLM, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int = None, bias: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        if d_ff is None:\n",
    "            # LLaMA uses 2/3 * 4 * d_model for hidden dim\n",
    "            d_ff = int(2 * 4 * d_model / 3)\n",
    "            # Round to multiple of 256 for efficiency\n",
    "            d_ff = 256 * ((d_ff + 255) // 256)\n",
    "        \n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=bias)  # Gate projection\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=bias)  # Down projection\n",
    "        self.w3 = nn.Linear(d_model, d_ff, bias=bias)  # Up projection\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # SwiGLU: Swish(x @ W1) * (x @ W3), then project down\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "# Compare activations\n",
    "print(\"\\nðŸ“Œ Comparing FFN Activations\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_model = 256\n",
    "d_ff = 1024\n",
    "\n",
    "# Standard FFN with ReLU\n",
    "class StandardFFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w2(F.relu(self.w1(x)))\n",
    "\n",
    "standard_ffn = StandardFFN(d_model, d_ff)\n",
    "swiglu_ffn = SwiGLU(d_model, d_ff)\n",
    "\n",
    "x = torch.randn(2, 10, d_model)\n",
    "\n",
    "standard_out = standard_ffn(x)\n",
    "swiglu_out = swiglu_ffn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Standard FFN output: {standard_out.shape}\")\n",
    "print(f\"SwiGLU output: {swiglu_out.shape}\")\n",
    "\n",
    "print(f\"\\nParameter count:\")\n",
    "print(f\"  Standard FFN: {sum(p.numel() for p in standard_ffn.parameters()):,}\")\n",
    "print(f\"  SwiGLU: {sum(p.numel() for p in swiglu_ffn.parameters()):,}\")\n",
    "print(\"  (SwiGLU has ~50% more params due to extra projection)\")\n",
    "\n",
    "# Visualize activation functions\n",
    "x_plot = torch.linspace(-3, 3, 100)\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(x_plot, F.relu(x_plot), label='ReLU')\n",
    "ax.plot(x_plot, F.gelu(x_plot), label='GELU')\n",
    "ax.plot(x_plot, F.silu(x_plot), label='Swish/SiLU')\n",
    "ax.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Activation(x)')\n",
    "ax.set_title('Activation Functions')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001502e1",
   "metadata": {},
   "source": [
    "## 9.4 Grouped Query Attention (GQA)\n",
    "\n",
    "### The KV Cache Problem\n",
    "\n",
    "During inference, we cache K and V to avoid recomputation.\n",
    "For long sequences, this becomes huge:\n",
    "\n",
    "$\\text{KV Cache} = 2 \\times n_{layers} \\times n_{heads} \\times seq_{len} \\times d_k$\n",
    "\n",
    "For LLaMA-70B with 32K context: ~40GB just for KV cache!\n",
    "\n",
    "### Multi-Query Attention (MQA)\n",
    "\n",
    "Use single K and V head, multiple Q heads:\n",
    "- Reduces KV cache by $n_{heads}$Ã—\n",
    "- But can hurt quality\n",
    "\n",
    "### Grouped Query Attention (GQA)\n",
    "\n",
    "Compromise: Groups of Q heads share K/V heads:\n",
    "\n",
    "```\n",
    "MHA:  Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8\n",
    "      K1 K2 K3 K4 K5 K6 K7 K8\n",
    "\n",
    "MQA:  Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8\n",
    "      K1 K1 K1 K1 K1 K1 K1 K1\n",
    "\n",
    "GQA:  Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8\n",
    "      K1 K1 K2 K2 K3 K3 K4 K4\n",
    "```\n",
    "\n",
    "### GQA in Practice\n",
    "\n",
    "| Model | Q Heads | KV Heads | Ratio |\n",
    "|-------|---------|----------|-------|\n",
    "| LLaMA 2 7B | 32 | 32 | 1:1 (MHA) |\n",
    "| LLaMA 2 70B | 64 | 8 | 8:1 (GQA) |\n",
    "| Mistral 7B | 32 | 8 | 4:1 (GQA) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d24b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GROUPED QUERY ATTENTION (GQA)\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"GROUPED QUERY ATTENTION (GQA)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped Query Attention (GQA).\n",
    "    \n",
    "    Multiple query heads share fewer key/value heads.\n",
    "    Reduces KV cache size while maintaining quality.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        n_kv_heads: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert n_heads % n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_groups = n_heads // n_kv_heads  # Q heads per KV head\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Q has n_heads, K/V have n_kv_heads\n",
    "        self.W_q = nn.Linear(d_model, n_heads * self.d_k, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, n_kv_heads * self.d_k, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, n_kv_heads * self.d_k, bias=False)\n",
    "        self.W_o = nn.Linear(n_heads * self.d_k, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project Q, K, V\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n",
    "        \n",
    "        # Transpose for attention: (batch, heads, seq_len, d_k)\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "        \n",
    "        # Repeat K, V for each group\n",
    "        # (batch, n_kv_heads, seq, d_k) -> (batch, n_heads, seq, d_k)\n",
    "        K = K.repeat_interleave(self.n_groups, dim=1)\n",
    "        V = V.repeat_interleave(self.n_groups, dim=1)\n",
    "        \n",
    "        # Standard attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reshape and project\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        return self.W_o(output)\n",
    "\n",
    "# Compare MHA, GQA, MQA\n",
    "print(\"\\nðŸ“Œ Comparing Attention Variants\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "\n",
    "# MHA: 8 Q heads, 8 KV heads\n",
    "mha = GroupedQueryAttention(d_model, n_heads=8, n_kv_heads=8)\n",
    "\n",
    "# GQA: 8 Q heads, 2 KV heads\n",
    "gqa = GroupedQueryAttention(d_model, n_heads=8, n_kv_heads=2)\n",
    "\n",
    "# MQA: 8 Q heads, 1 KV head\n",
    "mqa = GroupedQueryAttention(d_model, n_heads=8, n_kv_heads=1)\n",
    "\n",
    "x = torch.randn(2, 20, d_model)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"\\nKV Cache size comparison (per layer, seq_len=1024):\")\n",
    "\n",
    "seq_len = 1024\n",
    "d_k = d_model // n_heads\n",
    "\n",
    "for name, n_kv in [(\"MHA\", 8), (\"GQA-4\", 2), (\"MQA\", 1)]:\n",
    "    kv_cache_size = 2 * n_kv * seq_len * d_k * 2  # 2 for K+V, 2 bytes for FP16\n",
    "    print(f\"  {name}: {kv_cache_size / 1024:.1f} KB ({n_kv} KV heads)\")\n",
    "\n",
    "print(f\"\\nParameter count:\")\n",
    "print(f\"  MHA: {sum(p.numel() for p in mha.parameters()):,}\")\n",
    "print(f\"  GQA: {sum(p.numel() for p in gqa.parameters()):,}\")\n",
    "print(f\"  MQA: {sum(p.numel() for p in mqa.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b127b80",
   "metadata": {},
   "source": [
    "## 9.5 KV Cache for Efficient Inference\n",
    "\n",
    "### The Problem\n",
    "\n",
    "During autoregressive generation, we compute attention for each new token.\n",
    "Without caching, we recompute K and V for ALL previous tokens every step!\n",
    "\n",
    "### KV Cache Solution\n",
    "\n",
    "Cache K and V from previous steps:\n",
    "\n",
    "```\n",
    "Step 1: Input = [A]\n",
    "        Compute K1, V1, cache them\n",
    "        \n",
    "Step 2: Input = [A, B]\n",
    "        Load K1, V1 from cache\n",
    "        Compute K2, V2, append to cache\n",
    "        \n",
    "Step 3: Input = [A, B, C]\n",
    "        Load K1, K2, V1, V2 from cache\n",
    "        Compute K3, V3, append to cache\n",
    "```\n",
    "\n",
    "### Memory vs Compute Tradeoff\n",
    "\n",
    "| Without Cache | With Cache |\n",
    "|---------------|------------|\n",
    "| Compute: $O(n^2)$ per token | Compute: $O(n)$ per token |\n",
    "| Memory: $O(1)$ | Memory: $O(n)$ |\n",
    "\n",
    "For long sequences, the memory cost is worth the compute savings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b91f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# KV CACHE IMPLEMENTATION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"KV CACHE FOR EFFICIENT INFERENCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class KVCache:\n",
    "    \"\"\"\n",
    "    Key-Value cache for efficient autoregressive generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.k_cache = None\n",
    "        self.v_cache = None\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        k: torch.Tensor,\n",
    "        v: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Update cache with new K, V and return full cached tensors.\n",
    "        \n",
    "        Args:\n",
    "            k: New key tensor (batch, n_heads, new_seq_len, d_k)\n",
    "            v: New value tensor (batch, n_heads, new_seq_len, d_k)\n",
    "        \n",
    "        Returns:\n",
    "            Full K, V including cached values\n",
    "        \"\"\"\n",
    "        if self.k_cache is None:\n",
    "            self.k_cache = k\n",
    "            self.v_cache = v\n",
    "        else:\n",
    "            self.k_cache = torch.cat([self.k_cache, k], dim=2)\n",
    "            self.v_cache = torch.cat([self.v_cache, v], dim=2)\n",
    "        \n",
    "        return self.k_cache, self.v_cache\n",
    "    \n",
    "    def reset(self):\n",
    "        self.k_cache = None\n",
    "        self.v_cache = None\n",
    "\n",
    "class AttentionWithKVCache(nn.Module):\n",
    "    \"\"\"Attention layer with KV caching support.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[KVCache]]:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project Q, K, V\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Update cache if provided\n",
    "        if kv_cache is not None:\n",
    "            K, V = kv_cache.update(K, V)\n",
    "        \n",
    "        # Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Causal mask\n",
    "        kv_len = K.size(2)\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, kv_len, device=x.device))\n",
    "        # Adjust mask for cached positions\n",
    "        if kv_len > seq_len:\n",
    "            causal_mask = torch.ones(seq_len, kv_len, device=x.device)\n",
    "            causal_mask = torch.tril(causal_mask, diagonal=kv_len - seq_len)\n",
    "        \n",
    "        scores = scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        \n",
    "        return self.W_o(output), kv_cache\n",
    "\n",
    "# Demonstrate KV cache\n",
    "print(\"\\nðŸ“Œ KV Cache Demonstration\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "attn = AttentionWithKVCache(d_model, n_heads)\n",
    "\n",
    "# Simulate autoregressive generation\n",
    "cache = KVCache()\n",
    "\n",
    "# Step 1: Process prompt\n",
    "prompt = torch.randn(1, 5, d_model)  # 5 tokens\n",
    "out1, cache = attn(prompt, cache, use_cache=True)\n",
    "print(f\"Step 1 - Prompt (5 tokens):\")\n",
    "print(f\"  Input: {prompt.shape}\")\n",
    "print(f\"  Output: {out1.shape}\")\n",
    "print(f\"  Cache K shape: {cache.k_cache.shape}\")\n",
    "\n",
    "# Step 2: Generate token 1\n",
    "new_token = torch.randn(1, 1, d_model)\n",
    "out2, cache = attn(new_token, cache, use_cache=True)\n",
    "print(f\"\\nStep 2 - Generate token 1:\")\n",
    "print(f\"  Input: {new_token.shape} (only new token!)\")\n",
    "print(f\"  Output: {out2.shape}\")\n",
    "print(f\"  Cache K shape: {cache.k_cache.shape}\")\n",
    "\n",
    "# Step 3: Generate token 2\n",
    "new_token = torch.randn(1, 1, d_model)\n",
    "out3, cache = attn(new_token, cache, use_cache=True)\n",
    "print(f\"\\nStep 3 - Generate token 2:\")\n",
    "print(f\"  Input: {new_token.shape}\")\n",
    "print(f\"  Output: {out3.shape}\")\n",
    "print(f\"  Cache K shape: {cache.k_cache.shape}\")\n",
    "\n",
    "print(\"\\nâœ… With KV cache, we only process NEW tokens, not the entire sequence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d564797",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 9: Key Takeaways\n",
    "\n",
    "### Modern LLM Techniques Summary\n",
    "\n",
    "| Technique | Replaces | Benefit |\n",
    "|-----------|----------|---------|\n",
    "| **RoPE** | Absolute pos emb | Better extrapolation |\n",
    "| **RMSNorm** | LayerNorm | ~10% faster |\n",
    "| **SwiGLU** | ReLU/GELU FFN | Better quality |\n",
    "| **GQA** | MHA | Smaller KV cache |\n",
    "| **KV Cache** | Recomputation | Faster inference |\n",
    "\n",
    "### RoPE\n",
    "\n",
    "$f(x, m) = R_m \\cdot x$ where $R_m$ is rotation matrix\n",
    "\n",
    "- Encodes position via rotation\n",
    "- Captures relative positions naturally\n",
    "- Extrapolates better than absolute embeddings\n",
    "\n",
    "### RMSNorm\n",
    "\n",
    "$\\text{RMSNorm}(x) = \\gamma \\cdot \\frac{x}{\\text{RMS}(x)}$\n",
    "\n",
    "- No mean subtraction\n",
    "- Fewer parameters\n",
    "- Faster computation\n",
    "\n",
    "### SwiGLU\n",
    "\n",
    "$\\text{SwiGLU}(x) = \\text{Swish}(xW_1) \\otimes xW_3$\n",
    "\n",
    "- Gated activation\n",
    "- Better than ReLU/GELU\n",
    "- 50% more parameters\n",
    "\n",
    "### GQA\n",
    "\n",
    "- Multiple Q heads share K/V heads\n",
    "- Reduces KV cache by n_heads/n_kv_heads\n",
    "- Minimal quality loss\n",
    "\n",
    "### KV Cache\n",
    "\n",
    "- Cache K, V from previous tokens\n",
    "- Only compute new token's K, V\n",
    "- Essential for fast inference\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [Training Fundamentals](#module-10-training) - Loss, optimizers, and training loops\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23daa6d1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-10-training'></a>\n",
    "# Module 10: Training Fundamentals\n",
    "\n",
    "**Prerequisites:** Modules 1-9 (transformer architecture, modern techniques)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Implement cross-entropy loss for language modeling\n",
    "- Implement AdamW optimizer with weight decay\n",
    "- Implement learning rate scheduling with warmup\n",
    "- Implement gradient clipping\n",
    "- Build a complete training loop with logging\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24355ae1",
   "metadata": {},
   "source": [
    "## 10.1 Cross-Entropy Loss for Language Modeling\n",
    "\n",
    "### The Language Modeling Objective\n",
    "\n",
    "Predict the next token given previous tokens:\n",
    "\n",
    "$\\mathcal{L} = -\\frac{1}{T}\\sum_{t=1}^{T} \\log P(x_t | x_1, ..., x_{t-1})$\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "For each position, we have:\n",
    "- **Prediction**: Probability distribution over vocabulary (softmax of logits)\n",
    "- **Target**: One-hot encoded true token\n",
    "\n",
    "$\\text{CE}(p, y) = -\\sum_{i} y_i \\log(p_i) = -\\log(p_{\\text{true}})$\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "- **Ignore padding**: Use `ignore_index` to skip padding tokens\n",
    "- **Label smoothing**: Soft targets for regularization\n",
    "- **Shift labels**: Target is input shifted by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c25094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CROSS-ENTROPY LOSS FOR LANGUAGE MODELING\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"CROSS-ENTROPY LOSS FOR LANGUAGE MODELING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def compute_lm_loss(\n",
    "    logits: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    ignore_index: int = -100,\n",
    "    label_smoothing: float = 0.0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute language modeling loss.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model output (batch, seq_len, vocab_size)\n",
    "        labels: Target tokens (batch, seq_len)\n",
    "        ignore_index: Index to ignore in loss (e.g., padding)\n",
    "        label_smoothing: Label smoothing factor\n",
    "    \n",
    "    Returns:\n",
    "        Loss scalar\n",
    "    \"\"\"\n",
    "    # Shift: predict next token\n",
    "    # logits[:, :-1] predicts labels[:, 1:]\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "    \n",
    "    # Flatten for cross-entropy\n",
    "    loss = F.cross_entropy(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1),\n",
    "        ignore_index=ignore_index,\n",
    "        label_smoothing=label_smoothing\n",
    "    )\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Demonstrate\n",
    "print(\"\\nðŸ“Œ Language Modeling Loss\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "vocab_size = 100\n",
    "\n",
    "# Simulated model output\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "labels = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "loss = compute_lm_loss(logits, labels)\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# With label smoothing\n",
    "loss_smooth = compute_lm_loss(logits, labels, label_smoothing=0.1)\n",
    "print(f\"Loss with label smoothing (0.1): {loss_smooth.item():.4f}\")\n",
    "\n",
    "# Perplexity\n",
    "perplexity = torch.exp(loss)\n",
    "print(f\"\\nPerplexity: {perplexity.item():.2f}\")\n",
    "print(\"(Lower is better, random baseline â‰ˆ vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25439aab",
   "metadata": {},
   "source": [
    "## 10.2 AdamW Optimizer\n",
    "\n",
    "### Adam Recap\n",
    "\n",
    "Adam combines momentum and adaptive learning rates:\n",
    "\n",
    "$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$ (momentum)\n",
    "$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$ (adaptive LR)\n",
    "$\\theta_t = \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$\n",
    "\n",
    "### AdamW: Decoupled Weight Decay (Loshchilov & Hutter, 2017)\n",
    "\n",
    "Standard L2 regularization in Adam doesn't work well because it's scaled by the adaptive learning rate.\n",
    "\n",
    "**AdamW** decouples weight decay:\n",
    "\n",
    "$\\theta_t = \\theta_{t-1} - \\alpha \\left(\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda \\theta_{t-1}\\right)$\n",
    "\n",
    "### Typical Hyperparameters for LLMs\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Learning rate | 1e-4 to 6e-4 |\n",
    "| $\\beta_1$ | 0.9 |\n",
    "| $\\beta_2$ | 0.95 or 0.999 |\n",
    "| Weight decay | 0.1 |\n",
    "| $\\epsilon$ | 1e-8 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aee512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ADAMW OPTIMIZER\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"ADAMW OPTIMIZER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class AdamW:\n",
    "    \"\"\"\n",
    "    AdamW optimizer with decoupled weight decay.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-3,\n",
    "        betas: Tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-8,\n",
    "        weight_decay: float = 0.01\n",
    "    ):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # State\n",
    "        self.m = [torch.zeros_like(p) for p in self.params]  # First moment\n",
    "        self.v = [torch.zeros_like(p) for p in self.params]  # Second moment\n",
    "        self.t = 0  # Timestep\n",
    "    \n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        \n",
    "        for i, p in enumerate(self.params):\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            \n",
    "            g = p.grad\n",
    "            \n",
    "            # Update moments\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * g\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * g ** 2\n",
    "            \n",
    "            # Bias correction\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # Update parameters\n",
    "            p.data -= self.lr * (m_hat / (torch.sqrt(v_hat) + self.eps) + self.weight_decay * p.data)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "\n",
    "# Compare with PyTorch AdamW\n",
    "print(\"\\nðŸ“Œ Testing AdamW Implementation\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create identical models\n",
    "torch.manual_seed(42)\n",
    "model1 = nn.Linear(10, 10)\n",
    "torch.manual_seed(42)\n",
    "model2 = nn.Linear(10, 10)\n",
    "\n",
    "our_optim = AdamW(model1.parameters(), lr=0.01, weight_decay=0.1)\n",
    "torch_optim = torch.optim.AdamW(model2.parameters(), lr=0.01, weight_decay=0.1)\n",
    "\n",
    "# Training step\n",
    "x = torch.randn(5, 10)\n",
    "target = torch.randn(5, 10)\n",
    "\n",
    "for step in range(5):\n",
    "    # Our optimizer\n",
    "    our_optim.zero_grad()\n",
    "    loss1 = F.mse_loss(model1(x), target)\n",
    "    loss1.backward()\n",
    "    our_optim.step()\n",
    "    \n",
    "    # PyTorch optimizer\n",
    "    torch_optim.zero_grad()\n",
    "    loss2 = F.mse_loss(model2(x), target)\n",
    "    loss2.backward()\n",
    "    torch_optim.step()\n",
    "\n",
    "# Compare weights\n",
    "diff = (model1.weight - model2.weight).abs().max().item()\n",
    "print(f\"Max weight difference after 5 steps: {diff:.2e}\")\n",
    "print(f\"Implementations match: {diff < 1e-5} âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3a969",
   "metadata": {},
   "source": [
    "## 10.3 Learning Rate Scheduling\n",
    "\n",
    "### Why Scheduling?\n",
    "\n",
    "- **Warmup**: Start with small LR to stabilize early training\n",
    "- **Decay**: Reduce LR as training progresses for fine-grained optimization\n",
    "\n",
    "### Common Schedules\n",
    "\n",
    "**Linear Warmup + Cosine Decay** (most common for LLMs):\n",
    "\n",
    "$\\text{LR}(t) = \\begin{cases} \n",
    "\\text{lr}_{max} \\cdot \\frac{t}{t_{warmup}} & t < t_{warmup} \\\\\n",
    "\\text{lr}_{min} + \\frac{1}{2}(\\text{lr}_{max} - \\text{lr}_{min})(1 + \\cos(\\pi \\frac{t - t_{warmup}}{t_{total} - t_{warmup}})) & t \\geq t_{warmup}\n",
    "\\end{cases}$\n",
    "\n",
    "### Typical Values\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Warmup steps | 1-10% of total |\n",
    "| Max LR | 1e-4 to 6e-4 |\n",
    "| Min LR | 0.1 Ã— Max LR |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c731bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LEARNING RATE SCHEDULING\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"LEARNING RATE SCHEDULING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def get_lr_scheduler(\n",
    "    optimizer,\n",
    "    warmup_steps: int,\n",
    "    total_steps: int,\n",
    "    min_lr_ratio: float = 0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Create linear warmup + cosine decay scheduler.\n",
    "    \"\"\"\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            # Linear warmup\n",
    "            return step / warmup_steps\n",
    "        else:\n",
    "            # Cosine decay\n",
    "            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "            return min_lr_ratio + 0.5 * (1 - min_lr_ratio) * (1 + math.cos(math.pi * progress))\n",
    "    \n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Visualize schedule\n",
    "total_steps = 1000\n",
    "warmup_steps = 100\n",
    "\n",
    "model = nn.Linear(10, 10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = get_lr_scheduler(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "lrs = []\n",
    "for step in range(total_steps):\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(lrs)\n",
    "plt.axvline(x=warmup_steps, color='r', linestyle='--', label='End of warmup')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Linear Warmup + Cosine Decay Schedule')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Max LR: {max(lrs):.2e}\")\n",
    "print(f\"Min LR: {min(lrs):.2e}\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3112b73",
   "metadata": {},
   "source": [
    "## 10.4 Gradient Clipping\n",
    "\n",
    "### Why Clip Gradients?\n",
    "\n",
    "Transformers can have **exploding gradients**, especially:\n",
    "- Early in training\n",
    "- With long sequences\n",
    "- With certain attention patterns\n",
    "\n",
    "### Gradient Norm Clipping\n",
    "\n",
    "Clip gradients if their norm exceeds a threshold:\n",
    "\n",
    "$\\text{if } ||g|| > \\text{max\\_norm}: g \\leftarrow g \\cdot \\frac{\\text{max\\_norm}}{||g||}$\n",
    "\n",
    "### Typical Values\n",
    "\n",
    "- **max_norm = 1.0**: Most common for LLMs\n",
    "- **max_norm = 0.5**: More aggressive clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff58be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GRADIENT CLIPPING\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"GRADIENT CLIPPING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def clip_grad_norm_(parameters, max_norm: float) -> float:\n",
    "    \"\"\"\n",
    "    Clip gradient norm of parameters.\n",
    "    \n",
    "    Returns:\n",
    "        Total gradient norm before clipping\n",
    "    \"\"\"\n",
    "    parameters = list(parameters)\n",
    "    \n",
    "    # Compute total norm\n",
    "    total_norm = 0.0\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            total_norm += p.grad.data.norm(2).item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    \n",
    "    # Clip if necessary\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    if clip_coef < 1:\n",
    "        for p in parameters:\n",
    "            if p.grad is not None:\n",
    "                p.grad.data.mul_(clip_coef)\n",
    "    \n",
    "    return total_norm\n",
    "\n",
    "# Demonstrate gradient clipping\n",
    "print(\"\\nðŸ“Œ Gradient Clipping Demo\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "model = nn.Linear(100, 100)\n",
    "x = torch.randn(10, 100)\n",
    "target = torch.randn(10, 100) * 100  # Large target to create large gradients\n",
    "\n",
    "# Forward and backward\n",
    "loss = F.mse_loss(model(x), target)\n",
    "loss.backward()\n",
    "\n",
    "# Check gradient norm before clipping\n",
    "grad_norm_before = sum(p.grad.norm(2).item()**2 for p in model.parameters())**0.5\n",
    "print(f\"Gradient norm before clipping: {grad_norm_before:.2f}\")\n",
    "\n",
    "# Clip\n",
    "max_norm = 1.0\n",
    "grad_norm = clip_grad_norm_(model.parameters(), max_norm)\n",
    "grad_norm_after = sum(p.grad.norm(2).item()**2 for p in model.parameters())**0.5\n",
    "\n",
    "print(f\"Max norm: {max_norm}\")\n",
    "print(f\"Gradient norm after clipping: {grad_norm_after:.2f}\")\n",
    "print(f\"Clipping applied: {grad_norm_before > max_norm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eee74d",
   "metadata": {},
   "source": [
    "## 10.5 Complete Training Loop\n",
    "\n",
    "### Training Loop Components\n",
    "\n",
    "1. **Data loading**: Batching, shuffling, padding\n",
    "2. **Forward pass**: Model prediction\n",
    "3. **Loss computation**: Cross-entropy\n",
    "4. **Backward pass**: Gradient computation\n",
    "5. **Gradient clipping**: Prevent explosions\n",
    "6. **Optimizer step**: Update weights\n",
    "7. **Scheduler step**: Update learning rate\n",
    "8. **Logging**: Track metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598bf4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPLETE TRAINING LOOP\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE TRAINING LOOP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    device: str = 'cpu'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for batch_idx, (input_ids, labels) in enumerate(dataloader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(input_ids, labels=labels)\n",
    "        loss = output['loss']\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "        total_tokens += input_ids.numel()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'perplexity': perplexity,\n",
    "        'lr': optimizer.param_groups[0]['lr']\n",
    "    }\n",
    "\n",
    "# Create a simple dataset\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.seq_len]\n",
    "        y = self.data[idx+1:idx+self.seq_len+1]\n",
    "        return x, y\n",
    "\n",
    "# Training demo\n",
    "print(\"\\nðŸ“Œ Training Demo\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create data\n",
    "vocab_size = 100\n",
    "data = torch.randint(0, vocab_size, (1000,))\n",
    "dataset = SimpleDataset(data, seq_len=32)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Create model\n",
    "model = GPT(vocab_size=vocab_size, d_model=64, n_heads=4, n_layers=2, max_len=64)\n",
    "\n",
    "# Setup training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.1)\n",
    "total_steps = len(dataloader) * 5  # 5 epochs\n",
    "scheduler = get_lr_scheduler(optimizer, warmup_steps=10, total_steps=total_steps)\n",
    "\n",
    "# Train\n",
    "print(f\"Training for 5 epochs...\")\n",
    "for epoch in range(5):\n",
    "    metrics = train_epoch(model, dataloader, optimizer, scheduler)\n",
    "    print(f\"Epoch {epoch+1}: Loss={metrics['loss']:.4f}, PPL={metrics['perplexity']:.2f}, LR={metrics['lr']:.2e}\")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19a803",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 10: Key Takeaways\n",
    "\n",
    "### Language Modeling Loss\n",
    "\n",
    "$\\mathcal{L} = -\\frac{1}{T}\\sum_{t=1}^{T} \\log P(x_t | x_{<t})$\n",
    "\n",
    "- Cross-entropy between predictions and targets\n",
    "- Shift labels by 1 for next-token prediction\n",
    "- Use `ignore_index` for padding\n",
    "\n",
    "### AdamW\n",
    "\n",
    "$\\theta_t = \\theta_{t-1} - \\alpha \\left(\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda \\theta_{t-1}\\right)$\n",
    "\n",
    "- Decoupled weight decay\n",
    "- Typical: lr=1e-4, Î²=(0.9, 0.95), wd=0.1\n",
    "\n",
    "### Learning Rate Schedule\n",
    "\n",
    "- **Warmup**: Linear increase for first 1-10% of training\n",
    "- **Decay**: Cosine decay to 10% of max LR\n",
    "- Stabilizes training, improves final performance\n",
    "\n",
    "### Gradient Clipping\n",
    "\n",
    "- Clip gradient norm to max_norm (typically 1.0)\n",
    "- Prevents exploding gradients\n",
    "- Essential for stable transformer training\n",
    "\n",
    "### Training Loop Checklist\n",
    "\n",
    "1. âœ“ Forward pass\n",
    "2. âœ“ Compute loss\n",
    "3. âœ“ Backward pass\n",
    "4. âœ“ Clip gradients\n",
    "5. âœ“ Optimizer step\n",
    "6. âœ“ Scheduler step\n",
    "7. âœ“ Log metrics\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [Fine-Tuning Techniques](#module-11-finetuning) - LoRA, adapters, and more\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3cdab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-11-finetuning'></a>\n",
    "# Module 11: Fine-Tuning Techniques\n",
    "\n",
    "**Prerequisites:** Modules 1-10 (transformer architecture, training)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand full fine-tuning vs parameter-efficient methods\n",
    "- Implement LoRA (Low-Rank Adaptation) from scratch\n",
    "- Understand QLoRA and quantization concepts\n",
    "- Implement adapter layers\n",
    "- Compare different fine-tuning approaches\n",
    "\n",
    "**The Fine-Tuning Challenge**\n",
    "\n",
    "Full fine-tuning of large models is expensive:\n",
    "- LLaMA-7B: 7 billion parameters Ã— 4 bytes = 28GB just for weights\n",
    "- Need optimizer states: 2-3Ã— more memory\n",
    "- Total: 80-100GB for training\n",
    "\n",
    "**Parameter-Efficient Fine-Tuning (PEFT)** methods train only a small subset of parameters!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7afe6d",
   "metadata": {},
   "source": [
    "## 11.1 Full Fine-Tuning\n",
    "\n",
    "### The Standard Approach\n",
    "\n",
    "Update ALL model parameters on task-specific data:\n",
    "\n",
    "$\\theta_{new} = \\theta_{pretrained} - \\alpha \\nabla_\\theta \\mathcal{L}_{task}$\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Maximum flexibility | High memory cost |\n",
    "| Best potential performance | Risk of catastrophic forgetting |\n",
    "| Simple to implement | Need to store full model per task |\n",
    "\n",
    "### When to Use\n",
    "\n",
    "- Small models (<1B parameters)\n",
    "- Abundant compute resources\n",
    "- Task very different from pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad78ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FULL FINE-TUNING\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"FULL FINE-TUNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable and total parameters.\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "\n",
    "# Create a model\n",
    "model = GPT(vocab_size=1000, d_model=256, n_heads=4, n_layers=4, max_len=128)\n",
    "\n",
    "trainable, total = count_parameters(model)\n",
    "print(f\"\\nðŸ“Œ Full Fine-Tuning\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total parameters: {total:,}\")\n",
    "print(f\"Trainable parameters: {trainable:,}\")\n",
    "print(f\"Trainable ratio: {trainable/total:.1%}\")\n",
    "\n",
    "# Memory estimate (FP32)\n",
    "memory_weights = total * 4 / 1e9  # 4 bytes per param\n",
    "memory_optimizer = total * 8 / 1e9  # Adam: 2 states Ã— 4 bytes\n",
    "memory_gradients = total * 4 / 1e9\n",
    "print(f\"\\nMemory estimate (FP32):\")\n",
    "print(f\"  Weights: {memory_weights:.2f} GB\")\n",
    "print(f\"  Optimizer states: {memory_optimizer:.2f} GB\")\n",
    "print(f\"  Gradients: {memory_gradients:.2f} GB\")\n",
    "print(f\"  Total: {memory_weights + memory_optimizer + memory_gradients:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15594882",
   "metadata": {},
   "source": [
    "## 11.2 LoRA: Low-Rank Adaptation\n",
    "\n",
    "### The Key Insight (Hu et al., 2021)\n",
    "\n",
    "Weight updates during fine-tuning have **low intrinsic rank**.\n",
    "\n",
    "Instead of updating $W \\in \\mathbb{R}^{d \\times k}$ directly, decompose the update:\n",
    "\n",
    "$W' = W + \\Delta W = W + BA$\n",
    "\n",
    "Where:\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$\n",
    "- $A \\in \\mathbb{R}^{r \\times k}$\n",
    "- $r \\ll \\min(d, k)$ (rank is much smaller)\n",
    "\n",
    "### Parameter Savings\n",
    "\n",
    "Original: $d \\times k$ parameters\n",
    "LoRA: $d \\times r + r \\times k = r(d + k)$ parameters\n",
    "\n",
    "For $d = k = 4096$ and $r = 8$:\n",
    "- Original: 16.7M parameters\n",
    "- LoRA: 65K parameters (256Ã— reduction!)\n",
    "\n",
    "### LoRA Formula\n",
    "\n",
    "$h = Wx + \\frac{\\alpha}{r}BAx$\n",
    "\n",
    "Where $\\alpha$ is a scaling factor (typically $\\alpha = r$ or $\\alpha = 2r$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08022922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LORA IMPLEMENTATION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"LORA: LOW-RANK ADAPTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with LoRA adaptation.\n",
    "    \n",
    "    W' = W + (alpha/r) * B @ A\n",
    "    \n",
    "    Only A and B are trained; W is frozen.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        rank: int = 8,\n",
    "        alpha: float = 16,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Original weight (frozen)\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.02)\n",
    "        self.weight.requires_grad = False\n",
    "        \n",
    "        # LoRA matrices (trainable)\n",
    "        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.02)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Original forward\n",
    "        result = F.linear(x, self.weight)\n",
    "        \n",
    "        # LoRA forward\n",
    "        lora_out = F.linear(F.linear(self.dropout(x), self.lora_A), self.lora_B)\n",
    "        \n",
    "        return result + self.scaling * lora_out\n",
    "    \n",
    "    def merge_weights(self):\n",
    "        \"\"\"Merge LoRA weights into main weight for inference.\"\"\"\n",
    "        self.weight.data += self.scaling * (self.lora_B @ self.lora_A)\n",
    "        self.lora_A.data.zero_()\n",
    "        self.lora_B.data.zero_()\n",
    "\n",
    "# Test LoRA\n",
    "print(\"\\nðŸ“Œ Testing LoRA Layer\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "in_features = 768\n",
    "out_features = 768\n",
    "rank = 8\n",
    "\n",
    "lora_layer = LoRALinear(in_features, out_features, rank=rank, alpha=16)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in lora_layer.parameters())\n",
    "trainable_params = sum(p.numel() for p in lora_layer.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Input/Output features: {in_features}\")\n",
    "print(f\"  LoRA rank: {rank}\")\n",
    "print(f\"\\nParameter count:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Frozen (W): {frozen_params:,}\")\n",
    "print(f\"  Trainable (A, B): {trainable_params:,}\")\n",
    "print(f\"  Reduction: {frozen_params / trainable_params:.1f}x\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(2, 10, in_features)\n",
    "output = lora_layer(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996a7553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# APPLY LORA TO A MODEL\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"APPLYING LORA TO GPT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def apply_lora_to_model(model, rank=8, alpha=16, target_modules=['W_q', 'W_v']):\n",
    "    \"\"\"\n",
    "    Replace linear layers with LoRA versions.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to modify\n",
    "        rank: LoRA rank\n",
    "        alpha: LoRA alpha\n",
    "        target_modules: Names of modules to replace\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        for target in target_modules:\n",
    "            if target in name and isinstance(module, nn.Linear):\n",
    "                # Get parent module\n",
    "                parent_name = '.'.join(name.split('.')[:-1])\n",
    "                parent = model.get_submodule(parent_name) if parent_name else model\n",
    "                attr_name = name.split('.')[-1]\n",
    "                \n",
    "                # Create LoRA layer\n",
    "                lora_layer = LoRALinear(\n",
    "                    module.in_features,\n",
    "                    module.out_features,\n",
    "                    rank=rank,\n",
    "                    alpha=alpha\n",
    "                )\n",
    "                # Copy original weights\n",
    "                lora_layer.weight.data = module.weight.data.clone()\n",
    "                \n",
    "                # Replace\n",
    "                setattr(parent, attr_name, lora_layer)\n",
    "    \n",
    "    # Freeze all non-LoRA parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora_' not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Create and modify model\n",
    "model = GPT(vocab_size=1000, d_model=256, n_heads=4, n_layers=4, max_len=128)\n",
    "\n",
    "print(\"Before LoRA:\")\n",
    "trainable_before, total_before = count_parameters(model)\n",
    "print(f\"  Trainable: {trainable_before:,} / {total_before:,}\")\n",
    "\n",
    "# Note: In practice, you'd apply LoRA to attention projections\n",
    "# This is a simplified demonstration\n",
    "print(\"\\nðŸ“Œ LoRA Efficiency Comparison\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Simulate LoRA on attention layers\n",
    "d_model = 256\n",
    "n_layers = 4\n",
    "n_attention_matrices = 4  # Q, K, V, O per layer\n",
    "rank = 8\n",
    "\n",
    "full_params = n_layers * n_attention_matrices * d_model * d_model\n",
    "lora_params = n_layers * n_attention_matrices * 2 * rank * d_model  # A and B\n",
    "\n",
    "print(f\"Attention parameters (full): {full_params:,}\")\n",
    "print(f\"LoRA parameters (rank={rank}): {lora_params:,}\")\n",
    "print(f\"Reduction: {full_params / lora_params:.1f}x\")\n",
    "print(f\"\\nâœ… LoRA enables fine-tuning with ~{100*lora_params/full_params:.1f}% of parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43a973d",
   "metadata": {},
   "source": [
    "## 11.3 QLoRA: Quantized LoRA\n",
    "\n",
    "### The Idea (Dettmers et al., 2023)\n",
    "\n",
    "Combine LoRA with quantization for even more efficiency:\n",
    "\n",
    "1. **Quantize base model** to 4-bit (NF4 format)\n",
    "2. **Add LoRA adapters** in full precision\n",
    "3. **Train only LoRA** parameters\n",
    "\n",
    "### Memory Savings\n",
    "\n",
    "| Method | 7B Model Memory |\n",
    "|--------|-----------------|\n",
    "| Full FP32 | ~28 GB |\n",
    "| Full FP16 | ~14 GB |\n",
    "| LoRA FP16 | ~14 GB + small |\n",
    "| QLoRA 4-bit | ~4 GB + small |\n",
    "\n",
    "### Key Techniques\n",
    "\n",
    "- **NF4 (4-bit NormalFloat)**: Quantization format optimized for normally distributed weights\n",
    "- **Double Quantization**: Quantize the quantization constants\n",
    "- **Paged Optimizers**: Handle memory spikes with CPU offloading\n",
    "\n",
    "### When to Use QLoRA\n",
    "\n",
    "- Limited GPU memory (consumer GPUs)\n",
    "- Fine-tuning large models (7B+)\n",
    "- Acceptable small quality trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2de3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# QLORA CONCEPTS\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"QLORA: QUANTIZED LORA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Note: Full QLoRA requires bitsandbytes library\n",
    "# This demonstrates the concepts\n",
    "\n",
    "def quantize_to_nf4(tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Simplified NF4-like quantization (conceptual).\n",
    "    \n",
    "    Real NF4 uses a specific set of 16 values optimized for\n",
    "    normally distributed weights.\n",
    "    \"\"\"\n",
    "    # Compute scale (absmax quantization)\n",
    "    scale = tensor.abs().max() / 7  # 4-bit: -8 to 7\n",
    "    \n",
    "    # Quantize\n",
    "    quantized = torch.round(tensor / scale).clamp(-8, 7).to(torch.int8)\n",
    "    \n",
    "    return quantized, scale\n",
    "\n",
    "def dequantize_nf4(quantized: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Dequantize NF4 values.\"\"\"\n",
    "    return quantized.float() * scale\n",
    "\n",
    "# Demonstrate quantization\n",
    "print(\"\\nðŸ“Œ Quantization Demo\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create sample weights\n",
    "weights = torch.randn(256, 256) * 0.02\n",
    "\n",
    "# Quantize\n",
    "quantized, scale = quantize_to_nf4(weights)\n",
    "\n",
    "# Dequantize\n",
    "reconstructed = dequantize_nf4(quantized, scale)\n",
    "\n",
    "# Measure error\n",
    "error = (weights - reconstructed).abs().mean()\n",
    "print(f\"Original weights shape: {weights.shape}\")\n",
    "print(f\"Original dtype: {weights.dtype}\")\n",
    "print(f\"Quantized dtype: {quantized.dtype}\")\n",
    "print(f\"\\nMemory comparison:\")\n",
    "print(f\"  FP32: {weights.numel() * 4 / 1024:.1f} KB\")\n",
    "print(f\"  INT8: {quantized.numel() * 1 / 1024:.1f} KB\")\n",
    "print(f\"  4-bit: {quantized.numel() * 0.5 / 1024:.1f} KB (theoretical)\")\n",
    "print(f\"\\nReconstruction error: {error:.6f}\")\n",
    "\n",
    "# QLoRA memory estimate\n",
    "print(\"\\nðŸ“Œ QLoRA Memory Estimate (7B model)\")\n",
    "print(\"-\" * 50)\n",
    "params_7b = 7e9\n",
    "\n",
    "fp32_memory = params_7b * 4 / 1e9\n",
    "fp16_memory = params_7b * 2 / 1e9\n",
    "int4_memory = params_7b * 0.5 / 1e9\n",
    "\n",
    "print(f\"Base model memory:\")\n",
    "print(f\"  FP32: {fp32_memory:.1f} GB\")\n",
    "print(f\"  FP16: {fp16_memory:.1f} GB\")\n",
    "print(f\"  4-bit: {int4_memory:.1f} GB\")\n",
    "print(f\"\\nQLoRA total (4-bit base + FP16 LoRA):\")\n",
    "print(f\"  ~{int4_memory + 0.1:.1f} GB (vs {fp16_memory:.1f} GB for LoRA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0078b7df",
   "metadata": {},
   "source": [
    "## 11.4 Adapter Layers\n",
    "\n",
    "### The Idea (Houlsby et al., 2019)\n",
    "\n",
    "Insert small trainable modules between frozen layers:\n",
    "\n",
    "```\n",
    "Frozen Layer\n",
    "     â”‚\n",
    "     â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Adapter â”‚ â† Trainable\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     â”‚\n",
    "     â–¼\n",
    "Frozen Layer\n",
    "```\n",
    "\n",
    "### Adapter Architecture\n",
    "\n",
    "```\n",
    "Input (d_model)\n",
    "     â”‚\n",
    "     â–¼\n",
    "Down-project (d_model â†’ d_adapter)\n",
    "     â”‚\n",
    "     â–¼\n",
    "Non-linearity (ReLU/GELU)\n",
    "     â”‚\n",
    "     â–¼\n",
    "Up-project (d_adapter â†’ d_model)\n",
    "     â”‚\n",
    "     â–¼\n",
    "Residual connection (+)\n",
    "     â”‚\n",
    "     â–¼\n",
    "Output (d_model)\n",
    "```\n",
    "\n",
    "### Comparison with LoRA\n",
    "\n",
    "| Aspect | LoRA | Adapters |\n",
    "|--------|------|----------|\n",
    "| Where | Modifies existing weights | Adds new layers |\n",
    "| Inference | Can merge (no overhead) | Adds latency |\n",
    "| Parameters | Very few | Few |\n",
    "| Flexibility | Limited to linear | Any architecture |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0996f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ADAPTER LAYERS\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"ADAPTER LAYERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapter module for parameter-efficient fine-tuning.\n",
    "    \n",
    "    Architecture: Down-project â†’ Activation â†’ Up-project â†’ Residual\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        adapter_dim: int = 64,\n",
    "        activation: str = 'relu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.down_proj = nn.Linear(d_model, adapter_dim)\n",
    "        self.up_proj = nn.Linear(adapter_dim, d_model)\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        \n",
    "        # Initialize up_proj to near-zero for stable training\n",
    "        nn.init.zeros_(self.up_proj.weight)\n",
    "        nn.init.zeros_(self.up_proj.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Adapter forward with residual\n",
    "        residual = x\n",
    "        x = self.down_proj(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.up_proj(x)\n",
    "        return residual + x\n",
    "\n",
    "class TransformerBlockWithAdapter(nn.Module):\n",
    "    \"\"\"Transformer block with adapters after attention and FFN.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, adapter_dim: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Original components (frozen)\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PositionwiseFFN(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Adapters (trainable)\n",
    "        self.adapter1 = Adapter(d_model, adapter_dim)\n",
    "        self.adapter2 = Adapter(d_model, adapter_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        # Attention + Adapter\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x, x, x, mask)\n",
    "        x = self.adapter1(x)  # Adapter after attention\n",
    "        x = residual + x\n",
    "        \n",
    "        # FFN + Adapter\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.adapter2(x)  # Adapter after FFN\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test adapter\n",
    "print(\"\\nðŸ“Œ Testing Adapter Layer\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "d_model = 768\n",
    "adapter_dim = 64\n",
    "\n",
    "adapter = Adapter(d_model, adapter_dim)\n",
    "\n",
    "total_params = sum(p.numel() for p in adapter.parameters())\n",
    "print(f\"Adapter configuration:\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  adapter_dim: {adapter_dim}\")\n",
    "print(f\"  Parameters: {total_params:,}\")\n",
    "print(f\"  Bottleneck ratio: {adapter_dim/d_model:.1%}\")\n",
    "\n",
    "# Compare with full layer\n",
    "full_layer_params = d_model * d_model\n",
    "print(f\"\\nComparison with full linear layer:\")\n",
    "print(f\"  Full layer: {full_layer_params:,}\")\n",
    "print(f\"  Adapter: {total_params:,}\")\n",
    "print(f\"  Reduction: {full_layer_params/total_params:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bca1b2",
   "metadata": {},
   "source": [
    "## 11.5 Fine-Tuning Method Comparison\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Method | Trainable % | Memory | Inference | Quality |\n",
    "|--------|-------------|--------|-----------|---------|\n",
    "| Full FT | 100% | High | Same | Best |\n",
    "| LoRA | 0.1-1% | Medium | Same* | Very Good |\n",
    "| QLoRA | 0.1-1% | Low | Slower | Good |\n",
    "| Adapters | 1-5% | Medium | Slower | Good |\n",
    "| Prefix Tuning | <0.1% | Low | Same | Moderate |\n",
    "\n",
    "*LoRA can be merged into weights for zero inference overhead\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "- **Full Fine-tuning**: Small models, lots of compute, maximum quality\n",
    "- **LoRA**: Large models, limited memory, need inference speed\n",
    "- **QLoRA**: Very large models, consumer GPUs\n",
    "- **Adapters**: Need flexibility, multi-task scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a8826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FINE-TUNING COMPARISON\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"FINE-TUNING METHOD COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def compare_methods(model_params: int):\n",
    "    \"\"\"Compare different fine-tuning methods.\"\"\"\n",
    "    \n",
    "    methods = {\n",
    "        'Full Fine-tuning': {\n",
    "            'trainable_ratio': 1.0,\n",
    "            'memory_factor': 4.0,  # weights + optimizer + gradients\n",
    "            'inference_overhead': 0\n",
    "        },\n",
    "        'LoRA (r=8)': {\n",
    "            'trainable_ratio': 0.005,  # ~0.5%\n",
    "            'memory_factor': 2.1,  # frozen weights + small trainable\n",
    "            'inference_overhead': 0  # can merge\n",
    "        },\n",
    "        'QLoRA (4-bit)': {\n",
    "            'trainable_ratio': 0.005,\n",
    "            'memory_factor': 0.6,  # 4-bit weights + small trainable\n",
    "            'inference_overhead': 0.1  # dequantization\n",
    "        },\n",
    "        'Adapters': {\n",
    "            'trainable_ratio': 0.02,  # ~2%\n",
    "            'memory_factor': 2.2,\n",
    "            'inference_overhead': 0.05  # extra layers\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nModel size: {model_params/1e9:.1f}B parameters\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Method':<20} {'Trainable':<15} {'Memory (GB)':<15} {'Inference':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for name, config in methods.items():\n",
    "        trainable = model_params * config['trainable_ratio']\n",
    "        memory = model_params * 4 * config['memory_factor'] / 1e9  # FP32 baseline\n",
    "        overhead = f\"+{config['inference_overhead']*100:.0f}%\" if config['inference_overhead'] > 0 else \"Same\"\n",
    "        \n",
    "        print(f\"{name:<20} {trainable/1e6:>10.1f}M    {memory:>10.1f}      {overhead:<15}\")\n",
    "\n",
    "# Compare for different model sizes\n",
    "for size in [7e9, 13e9, 70e9]:\n",
    "    compare_methods(size)\n",
    "    print()\n",
    "\n",
    "print(\"âœ… Choose based on your memory constraints and quality requirements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe10cdfb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 11: Key Takeaways\n",
    "\n",
    "### LoRA\n",
    "\n",
    "$W' = W + \\frac{\\alpha}{r}BA$\n",
    "\n",
    "- Low-rank decomposition of weight updates\n",
    "- Typical rank: 8-64\n",
    "- Can merge for zero inference overhead\n",
    "- ~0.1-1% trainable parameters\n",
    "\n",
    "### QLoRA\n",
    "\n",
    "- 4-bit quantized base model\n",
    "- LoRA adapters in FP16\n",
    "- Enables 7B+ models on consumer GPUs\n",
    "- Small quality trade-off\n",
    "\n",
    "### Adapters\n",
    "\n",
    "- Bottleneck modules between layers\n",
    "- Down-project â†’ Activation â†’ Up-project\n",
    "- ~1-5% trainable parameters\n",
    "- Adds inference latency\n",
    "\n",
    "### Choosing a Method\n",
    "\n",
    "| Constraint | Recommendation |\n",
    "|------------|----------------|\n",
    "| Maximum quality | Full fine-tuning |\n",
    "| Limited memory | LoRA |\n",
    "| Consumer GPU | QLoRA |\n",
    "| Multi-task | Adapters |\n",
    "| Minimal changes | Prefix/Prompt tuning |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. Start with LoRA (good balance)\n",
    "2. Use rank 8-16 for most tasks\n",
    "3. Apply to attention projections (Q, V)\n",
    "4. Learning rate: 1e-4 to 3e-4\n",
    "5. Train for 1-3 epochs\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [Tokenizer Deep Dive](#module-12-tokenizers) - BPE and vocabulary building\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb7843",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-12-tokenizers'></a>\n",
    "# Module 12: Tokenizer Deep Dive\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Implement BPE (Byte Pair Encoding) from scratch\n",
    "- Understand vocabulary building\n",
    "- Compare different tokenization approaches\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61a5ee4",
   "metadata": {},
   "source": [
    "## 12.1 Byte Pair Encoding (BPE)\n",
    "\n",
    "### The Algorithm (Sennrich et al., 2016)\n",
    "\n",
    "1. Start with character-level vocabulary\n",
    "2. Count all adjacent pairs\n",
    "3. Merge most frequent pair into new token\n",
    "4. Repeat until desired vocabulary size\n",
    "\n",
    "### Why BPE?\n",
    "\n",
    "- Handles unknown words (falls back to subwords)\n",
    "- Balances vocabulary size vs sequence length\n",
    "- Language-agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BPE IMPLEMENTATION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"BYTE PAIR ENCODING (BPE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def get_pair_counts(vocab):\n",
    "    \"\"\"Count frequency of adjacent pairs in vocabulary.\"\"\"\n",
    "    pairs = {}\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i + 1])\n",
    "            pairs[pair] = pairs.get(pair, 0) + freq\n",
    "    return pairs\n",
    "\n",
    "def merge_pair(vocab, pair):\n",
    "    \"\"\"Merge a pair of symbols in the vocabulary.\"\"\"\n",
    "    new_vocab = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    \n",
    "    for word, freq in vocab.items():\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = freq\n",
    "    \n",
    "    return new_vocab\n",
    "\n",
    "def learn_bpe(text, num_merges):\n",
    "    \"\"\"Learn BPE merges from text.\"\"\"\n",
    "    # Initialize vocabulary with character-level tokens\n",
    "    words = text.split()\n",
    "    vocab = {}\n",
    "    for word in words:\n",
    "        # Add end-of-word marker\n",
    "        word_chars = ' '.join(list(word)) + ' </w>'\n",
    "        vocab[word_chars] = vocab.get(word_chars, 0) + 1\n",
    "    \n",
    "    merges = []\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_pair_counts(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        \n",
    "        # Find most frequent pair\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        merges.append(best_pair)\n",
    "        \n",
    "        # Merge in vocabulary\n",
    "        vocab = merge_pair(vocab, best_pair)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Merge {i+1}: {best_pair} (freq: {pairs[best_pair]})\")\n",
    "    \n",
    "    return vocab, merges\n",
    "\n",
    "# Learn BPE on sample text\n",
    "text = \"low lower lowest low lower lowest newer newest wide wider widest\"\n",
    "print(f\"\\nText: {text}\")\n",
    "print(\"\\nLearning BPE merges...\")\n",
    "\n",
    "vocab, merges = learn_bpe(text, num_merges=20)\n",
    "\n",
    "print(f\"\\nFinal vocabulary ({len(vocab)} words):\")\n",
    "for word, freq in sorted(vocab.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(f\"  '{word}': {freq}\")\n",
    "\n",
    "print(f\"\\nLearned {len(merges)} merges\")\n",
    "print(\"First 5 merges:\", merges[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae03a6a",
   "metadata": {},
   "source": [
    "## 12.2 Tokenization Comparison\n",
    "\n",
    "| Tokenizer | Used By | Vocab Size | Method |\n",
    "|-----------|---------|------------|--------|\n",
    "| BPE | GPT-2, GPT-3 | 50K | Byte Pair Encoding |\n",
    "| WordPiece | BERT | 30K | Similar to BPE |\n",
    "| SentencePiece | T5, LLaMA | 32K | Unigram/BPE |\n",
    "| Tiktoken | GPT-4 | 100K | BPE with bytes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a90583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TOKENIZER COMPARISON\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"TOKENIZATION EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple tokenizer for demonstration\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, merges):\n",
    "        self.merges = {' '.join(pair): ''.join(pair) for pair in merges}\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        # Character-level start\n",
    "        tokens = list(text) + ['</w>']\n",
    "        tokens = ' '.join(tokens)\n",
    "        \n",
    "        # Apply merges\n",
    "        for pair, merged in self.merges.items():\n",
    "            tokens = tokens.replace(pair, merged)\n",
    "        \n",
    "        return tokens.split()\n",
    "\n",
    "tokenizer = SimpleTokenizer(merges)\n",
    "\n",
    "test_words = [\"lower\", \"lowest\", \"newer\", \"unknown\"]\n",
    "print(\"\\nTokenization results:\")\n",
    "for word in test_words:\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    print(f\"  '{word}' -> {tokens}\")\n",
    "\n",
    "print(\"\\nâœ… BPE handles unknown words by breaking into known subwords!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d469db",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 12: Key Takeaways\n",
    "\n",
    "### BPE Algorithm\n",
    "1. Start with characters\n",
    "2. Iteratively merge most frequent pairs\n",
    "3. Build vocabulary of subwords\n",
    "\n",
    "### Benefits\n",
    "- No unknown tokens (falls back to characters)\n",
    "- Efficient vocabulary size\n",
    "- Language-agnostic\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [Scaling Laws](#module-13-scaling) - Understanding model scaling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25677e51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-13-scaling'></a>\n",
    "# Module 13: Scaling Laws\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand power-law scaling relationships\n",
    "- Learn Chinchilla optimal training\n",
    "- Predict model performance from scale\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7933238",
   "metadata": {},
   "source": [
    "## 13.1 Scaling Laws (Kaplan et al., 2020)\n",
    "\n",
    "### The Power Law\n",
    "\n",
    "Model performance scales predictably with:\n",
    "- **N**: Number of parameters\n",
    "- **D**: Dataset size (tokens)\n",
    "- **C**: Compute (FLOPs)\n",
    "\n",
    "$L(N) = \\left(\\frac{N_c}{N}\\right)^{\\alpha_N}$\n",
    "\n",
    "$L(D) = \\left(\\frac{D_c}{D}\\right)^{\\alpha_D}$\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- Loss decreases as power law of scale\n",
    "- Parameters, data, and compute are somewhat interchangeable\n",
    "- Larger models are more sample-efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9084d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SCALING LAW VISUALIZATION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"SCALING LAWS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulated scaling law data\n",
    "params = np.array([1e6, 1e7, 1e8, 1e9, 1e10, 1e11])\n",
    "loss = 10 * (params / 1e6) ** (-0.076)  # Approximate power law\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.loglog(params, loss, 'b-o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Parameters')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Scaling Law: Loss vs Parameters')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "for p, l in zip(params[::2], loss[::2]):\n",
    "    plt.annotate(f'{p/1e9:.0f}B' if p >= 1e9 else f'{p/1e6:.0f}M', \n",
    "                 (p, l), textcoords=\"offset points\", xytext=(10,5))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Œ Key Insight: Loss decreases as a power law of model size!\")\n",
    "print(\"   Doubling parameters gives consistent improvement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ec499",
   "metadata": {},
   "source": [
    "## 13.2 Chinchilla Scaling (Hoffmann et al., 2022)\n",
    "\n",
    "### Compute-Optimal Training\n",
    "\n",
    "For a fixed compute budget, optimal allocation:\n",
    "\n",
    "$N_{opt} \\propto C^{0.5}$\n",
    "$D_{opt} \\propto C^{0.5}$\n",
    "\n",
    "**Rule of thumb**: Train on ~20 tokens per parameter\n",
    "\n",
    "### Implications\n",
    "\n",
    "| Model | Params | Chinchilla Optimal Tokens |\n",
    "|-------|--------|---------------------------|\n",
    "| 1B | 1B | 20B |\n",
    "| 7B | 7B | 140B |\n",
    "| 70B | 70B | 1.4T |\n",
    "\n",
    "Many models are undertrained by this standard!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1295d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 13: Key Takeaways\n",
    "\n",
    "### Scaling Laws\n",
    "- Loss follows power law with scale\n",
    "- $L \\propto N^{-0.076}$ approximately\n",
    "- Predictable performance from scale\n",
    "\n",
    "### Chinchilla Optimal\n",
    "- ~20 tokens per parameter\n",
    "- Balance model size and data\n",
    "- Many models are undertrained\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [Inference Optimization](#module-14-inference) - Quantization and speedups\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed745dda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-14-inference'></a>\n",
    "# Module 14: Inference Optimization\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Implement basic quantization\n",
    "- Understand pruning techniques\n",
    "- Learn about speculative decoding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83a3f7",
   "metadata": {},
   "source": [
    "## 14.1 Quantization\n",
    "\n",
    "### The Idea\n",
    "\n",
    "Reduce precision of weights/activations:\n",
    "- FP32 â†’ FP16: 2Ã— memory reduction\n",
    "- FP16 â†’ INT8: 2Ã— more reduction\n",
    "- INT8 â†’ INT4: 2Ã— more reduction\n",
    "\n",
    "### Quantization Formula\n",
    "\n",
    "$q = \\text{round}\\left(\\frac{x - z}{s}\\right)$\n",
    "\n",
    "Where:\n",
    "- $s$ = scale factor\n",
    "- $z$ = zero point\n",
    "- $q$ = quantized value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0255c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# QUANTIZATION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"QUANTIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def quantize_tensor(tensor, bits=8):\n",
    "    \"\"\"Quantize tensor to specified bits.\"\"\"\n",
    "    qmin = -(2 ** (bits - 1))\n",
    "    qmax = 2 ** (bits - 1) - 1\n",
    "    \n",
    "    # Compute scale and zero point\n",
    "    min_val, max_val = tensor.min(), tensor.max()\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    zero_point = qmin - min_val / scale\n",
    "    \n",
    "    # Quantize\n",
    "    quantized = torch.round(tensor / scale + zero_point).clamp(qmin, qmax)\n",
    "    \n",
    "    return quantized.to(torch.int8), scale, zero_point\n",
    "\n",
    "def dequantize_tensor(quantized, scale, zero_point):\n",
    "    \"\"\"Dequantize tensor.\"\"\"\n",
    "    return (quantized.float() - zero_point) * scale\n",
    "\n",
    "# Test quantization\n",
    "weights = torch.randn(256, 256)\n",
    "quantized, scale, zp = quantize_tensor(weights, bits=8)\n",
    "reconstructed = dequantize_tensor(quantized, scale, zp)\n",
    "\n",
    "error = (weights - reconstructed).abs().mean()\n",
    "print(f\"Original dtype: {weights.dtype}\")\n",
    "print(f\"Quantized dtype: {quantized.dtype}\")\n",
    "print(f\"Mean absolute error: {error:.6f}\")\n",
    "print(f\"Memory reduction: {weights.numel() * 4 / quantized.numel():.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0c913",
   "metadata": {},
   "source": [
    "## 14.2 Speculative Decoding\n",
    "\n",
    "### The Idea\n",
    "\n",
    "Use a small \"draft\" model to generate candidates, verify with large model:\n",
    "\n",
    "1. Draft model generates K tokens quickly\n",
    "2. Large model verifies all K in parallel\n",
    "3. Accept correct prefix, reject rest\n",
    "4. Repeat\n",
    "\n",
    "### Speedup\n",
    "\n",
    "If draft model has ~70% acceptance rate:\n",
    "- Generate K=5 tokens per large model call\n",
    "- ~3-4Ã— speedup possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c0cc3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 14: Key Takeaways\n",
    "\n",
    "### Quantization\n",
    "- Reduce precision for memory/speed\n",
    "- INT8: 4Ã— memory reduction\n",
    "- INT4: 8Ã— memory reduction\n",
    "\n",
    "### Speculative Decoding\n",
    "- Small model drafts, large model verifies\n",
    "- 2-4Ã— speedup possible\n",
    "- No quality loss\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [Vision Transformers](#module-15-vit) - Transformers for images\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e0e815",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-15-vit'></a>\n",
    "# Module 15: Vision Transformers\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Implement patch embeddings\n",
    "- Build a Vision Transformer (ViT)\n",
    "- Understand vision-language connections\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720b16a0",
   "metadata": {},
   "source": [
    "## 15.1 Patch Embeddings\n",
    "\n",
    "### From Images to Sequences\n",
    "\n",
    "ViT treats images as sequences of patches:\n",
    "\n",
    "```\n",
    "Image (224Ã—224Ã—3)\n",
    "       â”‚\n",
    "       â–¼\n",
    "Split into patches (14Ã—14 patches of 16Ã—16)\n",
    "       â”‚\n",
    "       â–¼\n",
    "Flatten each patch (16Ã—16Ã—3 = 768)\n",
    "       â”‚\n",
    "       â–¼\n",
    "Linear projection to d_model\n",
    "       â”‚\n",
    "       â–¼\n",
    "Add [CLS] token + position embeddings\n",
    "       â”‚\n",
    "       â–¼\n",
    "Transformer Encoder\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ad53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISION TRANSFORMER\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"VISION TRANSFORMER (ViT)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Convert image to sequence of patch embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, d_model=768):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Linear projection of flattened patches\n",
    "        self.proj = nn.Conv2d(in_channels, d_model, \n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels, height, width)\n",
    "        x = self.proj(x)  # (batch, d_model, n_patches_h, n_patches_w)\n",
    "        x = x.flatten(2)  # (batch, d_model, n_patches)\n",
    "        x = x.transpose(1, 2)  # (batch, n_patches, d_model)\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"Vision Transformer.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3,\n",
    "                 n_classes=1000, d_model=768, n_heads=12, n_layers=12):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, d_model)\n",
    "        n_patches = self.patch_embed.n_patches\n",
    "        \n",
    "        # CLS token and position embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, d_model))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, n_heads, d_model * 4)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        \n",
    "        # Add position embedding\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Classification head\n",
    "        x = self.norm(x[:, 0])  # CLS token\n",
    "        return self.head(x)\n",
    "\n",
    "# Test ViT\n",
    "vit = ViT(img_size=224, patch_size=16, n_classes=10, \n",
    "          d_model=192, n_heads=3, n_layers=4)\n",
    "\n",
    "img = torch.randn(2, 3, 224, 224)\n",
    "output = vit(img)\n",
    "\n",
    "print(f\"Input shape: {img.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in vit.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2964fd43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 15: Key Takeaways\n",
    "\n",
    "### ViT Architecture\n",
    "- Split image into patches\n",
    "- Treat patches as tokens\n",
    "- Standard transformer encoder\n",
    "- CLS token for classification\n",
    "\n",
    "### Key Insight\n",
    "Same transformer architecture works for both text and images!\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [VLA and World Models](#module-16-vla) - Robotics and embodied AI\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384d78a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-16-vla'></a>\n",
    "# Module 16: VLA and World Models\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand Vision-Language-Action models\n",
    "- Learn about world models for robotics\n",
    "- See how LLMs extend to embodied AI\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef1e3c1",
   "metadata": {},
   "source": [
    "## 16.1 Vision-Language-Action Models\n",
    "\n",
    "### The Architecture\n",
    "\n",
    "VLAs combine:\n",
    "- **Vision encoder**: Process images/video\n",
    "- **Language model**: Understand instructions\n",
    "- **Action decoder**: Output robot actions\n",
    "\n",
    "```\n",
    "Camera Image â†’ Vision Encoder â”€â”\n",
    "                               â”œâ”€â†’ LLM â†’ Action Tokens\n",
    "Instruction â†’ Text Tokenizer â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### RT-2 (Brodie et al., 2023)\n",
    "\n",
    "Key insight: Represent robot actions as text tokens!\n",
    "\n",
    "```\n",
    "\"Pick up the red block\" + Image\n",
    "       â†“\n",
    "    VLM\n",
    "       â†“\n",
    "\"move(x=0.3, y=0.1, z=0.5, gripper=close)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c2b8a8",
   "metadata": {},
   "source": [
    "## 16.2 World Models\n",
    "\n",
    "### The Idea\n",
    "\n",
    "Learn to predict future states of the world:\n",
    "\n",
    "$s_{t+1} = f(s_t, a_t)$\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **Planning**: Simulate actions before executing\n",
    "- **Imagination**: Generate training data\n",
    "- **Safety**: Predict consequences\n",
    "\n",
    "### Modern Approaches\n",
    "\n",
    "- **Video prediction**: Predict future frames\n",
    "- **Latent world models**: Predict in learned latent space\n",
    "- **Diffusion models**: Generate possible futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aec416",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Module 16: Key Takeaways\n",
    "\n",
    "### VLA Models\n",
    "- Combine vision, language, and action\n",
    "- Actions as tokens (RT-2 style)\n",
    "- Enable instruction-following robots\n",
    "\n",
    "### World Models\n",
    "- Predict future states\n",
    "- Enable planning and simulation\n",
    "- Key for embodied AI\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [Practical Exercises](#module-17-exercises) - Hands-on projects\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac0d1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='module-17-exercises'></a>\n",
    "# Module 17: Practical Exercises\n",
    "\n",
    "**Projects to solidify your understanding:**\n",
    "\n",
    "1. Train a character-level language model\n",
    "2. Implement attention visualization\n",
    "3. Fine-tune with LoRA\n",
    "4. Build a simple chatbot\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13747e27",
   "metadata": {},
   "source": [
    "## Exercise 1: Shakespeare Language Model\n",
    "\n",
    "Train a character-level GPT on Shakespeare text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723195db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SHAKESPEARE LANGUAGE MODEL\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"SHAKESPEARE LANGUAGE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample Shakespeare text\n",
    "shakespeare = '''\n",
    "To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles\n",
    "And by opposing end them. To die: to sleep;\n",
    "No more; and by a sleep to say we end\n",
    "The heart-ache and the thousand natural shocks\n",
    "That flesh is heir to, 'tis a consummation\n",
    "Devoutly to be wish'd. To die, to sleep;\n",
    "To sleep: perchance to dream: ay, there's the rub;\n",
    "''' * 20  # Repeat for more data\n",
    "\n",
    "# Character-level tokenization\n",
    "chars = sorted(list(set(shakespeare)))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Sample characters: {chars[:20]}\")\n",
    "\n",
    "# Encode text\n",
    "data = torch.tensor([char_to_idx[c] for c in shakespeare])\n",
    "\n",
    "# Create model\n",
    "model = GPT(vocab_size=vocab_size, d_model=64, n_heads=4, n_layers=4, max_len=128)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training\n",
    "print(\"\\nTraining...\")\n",
    "model.train()\n",
    "seq_len = 64\n",
    "batch_size = 32\n",
    "\n",
    "for step in range(200):\n",
    "    # Get batch\n",
    "    idx = torch.randint(len(data) - seq_len - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in idx])\n",
    "    \n",
    "    # Forward\n",
    "    output = model(x, labels=y)\n",
    "    loss = output['loss']\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (step + 1) % 50 == 0:\n",
    "        print(f\"Step {step+1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# Generate\n",
    "print(\"\\nGenerating text...\")\n",
    "model.eval()\n",
    "prompt = \"To be\"\n",
    "prompt_ids = torch.tensor([[char_to_idx[c] for c in prompt]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(200):\n",
    "        logits = model(prompt_ids)['logits']\n",
    "        probs = F.softmax(logits[0, -1] / 0.8, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1)\n",
    "        prompt_ids = torch.cat([prompt_ids, next_id.unsqueeze(0)], dim=1)\n",
    "\n",
    "generated = ''.join([idx_to_char[i.item()] for i in prompt_ids[0]])\n",
    "print(f\"\\nGenerated text:\\n{generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec451e2",
   "metadata": {},
   "source": [
    "## Self-Assessment Questions\n",
    "\n",
    "1. **Attention**: Why do we scale by $\\sqrt{d_k}$?\n",
    "2. **Transformers**: What's the difference between encoder and decoder?\n",
    "3. **BERT vs GPT**: When would you use each?\n",
    "4. **LoRA**: How does it reduce memory requirements?\n",
    "5. **Scaling**: What does Chinchilla-optimal mean?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f34797",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Congratulations!\n",
    "\n",
    "You've completed the LLM Learning Notebook!\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. âœ… PyTorch fundamentals\n",
    "2. âœ… Text representation and embeddings\n",
    "3. âœ… Attention mechanisms\n",
    "4. âœ… Transformer architecture\n",
    "5. âœ… BERT and GPT\n",
    "6. âœ… Efficient attention\n",
    "7. âœ… Mixture of Experts\n",
    "8. âœ… Modern techniques (RoPE, GQA, etc.)\n",
    "9. âœ… Training and fine-tuning\n",
    "10. âœ… Tokenizers and scaling laws\n",
    "11. âœ… Vision transformers\n",
    "12. âœ… VLA and world models\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Train larger models on real datasets\n",
    "- Explore Hugging Face Transformers library\n",
    "- Read recent papers (LLaMA, Mistral, etc.)\n",
    "- Build your own applications!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
